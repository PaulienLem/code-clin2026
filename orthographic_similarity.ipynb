{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7252686-05bd-4c43-8175-a32a47a66cf8",
   "metadata": {},
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51922e6-8776-4973-98ca-79e383e52cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "DATA_FILE = 'paper_verses.csv'\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"GPU detected - using CuPy acceleration\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"No GPU - using NumPy (CPU mode)\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, lowercase=True, remove_punctuation=True, remove_diacritics=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_diacritics = remove_diacritics\n",
    "        if remove_punctuation:\n",
    "            self.punct_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "            self.remove_chars_pattern = re.compile(r'[\\(\\)\\{\\}]')\n",
    "\n",
    "    def _remove_diacritics(self, text: str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) if pd.notna(text) else ''\n",
    "\n",
    "        if self.remove_diacritics:\n",
    "            text = self._remove_diacritics(text)\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = self.remove_chars_pattern.sub('', text)\n",
    "            text = self.punct_pattern.sub(' ', text)\n",
    "\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.preprocess(t) for t in texts]\n",
    "\n",
    "\n",
    "class ShingleGenerator:\n",
    "    def __init__(self, shingle_size: int = 4, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.shingle_size = shingle_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def generate_shingles(self, text: str) -> np.ndarray:\n",
    "        if len(text) < self.shingle_size:\n",
    "            return np.array([hash(text) % (2**31)], dtype=np.int32)\n",
    "\n",
    "        chars = self.xp.array([ord(c) for c in text], dtype=np.int32)\n",
    "        n_shingles = len(text) - self.shingle_size + 1\n",
    "\n",
    "        shingles = self.xp.zeros(n_shingles, dtype=np.int32)\n",
    "        for i in range(self.shingle_size):\n",
    "            shingles += chars[i:i+n_shingles] * (31 ** i)\n",
    "\n",
    "        unique_shingles = self.xp.unique(shingles)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            unique_shingles = cp.asnumpy(unique_shingles)\n",
    "\n",
    "        return unique_shingles\n",
    "\n",
    "    def generate_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        return [self.generate_shingles(t) for t in texts]\n",
    "\n",
    "\n",
    "class MinHashProcessor:\n",
    "    def __init__(self, num_perm: int = 128, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.num_perm = num_perm\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "        rng = self.xp.random.RandomState(42)\n",
    "        self.hash_a = rng.randint(1, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.hash_b = rng.randint(0, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.prime = np.int64(2**31-1)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            print(f\"Using GPU for MinHash ({num_perm} permutations)\")\n",
    "\n",
    "    def compute_signature(self, shingles: np.ndarray) -> np.ndarray:\n",
    "        if len(shingles) == 0:\n",
    "            return np.full(self.num_perm, self.prime, dtype=np.int64)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            shingles_gpu = self.xp.array(shingles, dtype=np.int64)\n",
    "        else:\n",
    "            shingles_gpu = shingles.astype(np.int64)\n",
    "\n",
    "        shingles_expanded = shingles_gpu[:, self.xp.newaxis]\n",
    "        hashes = (self.hash_a * shingles_expanded + self.hash_b) % self.prime\n",
    "        signature = self.xp.min(hashes, axis=0)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            signature = cp.asnumpy(signature)\n",
    "\n",
    "        return signature\n",
    "\n",
    "    def compute_batch(self, shingles_batch: List[np.ndarray]) -> np.ndarray:\n",
    "        signatures = np.zeros((len(shingles_batch), self.num_perm), dtype=np.int64)\n",
    "        for i, shingles in enumerate(shingles_batch):\n",
    "            signatures[i] = self.compute_signature(shingles)\n",
    "        return signatures\n",
    "\n",
    "\n",
    "class LSHIndex:\n",
    "    def __init__(self, threshold: float = 0.3, num_perm: int = 128):\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.bands = 16\n",
    "        self.rows = num_perm // self.bands\n",
    "        self.signatures = []\n",
    "        self.num_docs = 0\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.bands)]\n",
    "\n",
    "    def _hash_band(self, band: np.ndarray) -> int:\n",
    "        return int(hash(tuple(band)) % (2**31))\n",
    "\n",
    "    def insert_batch(self, signatures: np.ndarray, start_idx: int):\n",
    "        batch_size = signatures.shape[0]\n",
    "        self.signatures.append(signatures)\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                global_doc_id = start_idx + doc_idx\n",
    "                self.hash_tables[band_idx][band_hash].append(global_doc_id)\n",
    "\n",
    "        self.num_docs += batch_size\n",
    "\n",
    "    def query_batch(self, signatures: np.ndarray, start_idx: int) -> List[set]:\n",
    "        batch_size = signatures.shape[0]\n",
    "        candidates = [set() for _ in range(batch_size)]\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                query_doc_id = start_idx + doc_idx\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                bucket = self.hash_tables[band_idx].get(band_hash, [])\n",
    "                candidates[doc_idx].update(c for c in bucket if c < query_doc_id)\n",
    "\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SimilarityComputer:\n",
    "    def __init__(self, threshold: float = 0.3, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.threshold = threshold\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def compute_batch_similarities(self, query_sig: np.ndarray,\n",
    "                                   candidate_sigs: np.ndarray) -> np.ndarray:\n",
    "        if self.use_gpu:\n",
    "            query_gpu = self.xp.array(query_sig)\n",
    "            cands_gpu = self.xp.array(candidate_sigs)\n",
    "            query_expanded = self.xp.tile(query_gpu, (len(candidate_sigs), 1))\n",
    "            matches = self.xp.sum(query_expanded == cands_gpu, axis=1)\n",
    "            sims = matches.astype(np.float32) / query_sig.shape[0]\n",
    "            return cp.asnumpy(sims)\n",
    "        else:\n",
    "            query_expanded = np.tile(query_sig, (len(candidate_sigs), 1))\n",
    "            matches = np.sum(query_expanded == candidate_sigs, axis=1)\n",
    "            return matches.astype(np.float32) / query_sig.shape[0]\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n: int):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x: int, y: int):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "\n",
    "    def get_clusters(self) -> Dict[int, int]:\n",
    "        return {i: self.find(i) for i in range(len(self.parent))}\n",
    "\n",
    "\n",
    "class FastMinHashClustering:\n",
    "    def __init__(self, threshold: float = 0.3, shingle_size: int = 4,\n",
    "                 num_perm: int = 128, chunk_size: int = 50000,\n",
    "                 use_gpu: Optional[bool] = None):\n",
    "\n",
    "        if use_gpu is None:\n",
    "            use_gpu = GPU_AVAILABLE\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "\n",
    "        self.preprocessor = TextPreprocessor(\n",
    "            lowercase=True,\n",
    "            remove_punctuation=True,\n",
    "            remove_diacritics=True\n",
    "        )\n",
    "        self.shingler = ShingleGenerator(shingle_size, use_gpu)\n",
    "        self.minhash = MinHashProcessor(num_perm, use_gpu)\n",
    "        self.lsh_index = LSHIndex(threshold, num_perm)\n",
    "        self.similarity_computer = SimilarityComputer(threshold, use_gpu)\n",
    "        self.all_similarities = []\n",
    "\n",
    "        mode = \"GPU (CuPy)\" if self.use_gpu else \"CPU (NumPy)\"\n",
    "        print(f\"Initialized in {mode} mode\")\n",
    "\n",
    "    def cluster(self, texts: List[str]) -> Tuple[Dict[int, int], List[Tuple[int, int, float]]]:\n",
    "        n_docs = len(texts)\n",
    "        n_chunks = (n_docs + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        print(f\"\\nClustering {n_docs:,} documents in {n_chunks} chunks\")\n",
    "        print(f\"threshold={self.threshold}, chunk_size={self.chunk_size:,}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for chunk_idx in tqdm(range(n_chunks), desc=\"Processing\"):\n",
    "            chunk_start = chunk_idx * self.chunk_size\n",
    "            chunk_end = min(chunk_start + self.chunk_size, n_docs)\n",
    "            chunk_texts = texts[chunk_start:chunk_end]\n",
    "\n",
    "            processed = self.preprocessor.preprocess_batch(chunk_texts)\n",
    "            shingles = self.shingler.generate_batch(processed)\n",
    "            signatures = self.minhash.compute_batch(shingles)\n",
    "            self.lsh_index.insert_batch(signatures, chunk_start)\n",
    "\n",
    "            if chunk_start > 0:\n",
    "                candidates = self.lsh_index.query_batch(signatures, chunk_start)\n",
    "\n",
    "                for doc_idx, cand_set in enumerate(candidates):\n",
    "                    if not cand_set:\n",
    "                        continue\n",
    "\n",
    "                    query_doc_id = chunk_start + doc_idx\n",
    "                    query_sig = signatures[doc_idx]\n",
    "\n",
    "                    cand_list = sorted(cand_set)\n",
    "                    cand_sigs = []\n",
    "                    for cand_id in cand_list:\n",
    "                        batch_idx = cand_id // self.chunk_size\n",
    "                        local_idx = cand_id % self.chunk_size\n",
    "                        if batch_idx < len(self.lsh_index.signatures):\n",
    "                            cand_sigs.append(self.lsh_index.signatures[batch_idx][local_idx])\n",
    "\n",
    "                    if cand_sigs:\n",
    "                        cand_sigs = np.array(cand_sigs)\n",
    "                        sims = self.similarity_computer.compute_batch_similarities(\n",
    "                            query_sig, cand_sigs\n",
    "                        )\n",
    "\n",
    "                        for cand_id, sim in zip(cand_list[:len(sims)], sims):\n",
    "                            if sim >= self.threshold:\n",
    "                                self.all_similarities.append((cand_id, query_doc_id, float(sim)))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nFound {len(self.all_similarities):,} similarities in {elapsed:.2f}s\")\n",
    "        print(f\"Throughput: {n_docs/elapsed:,.0f} docs/sec\")\n",
    "\n",
    "        print(\"Building clusters...\")\n",
    "        uf = UnionFind(n_docs)\n",
    "        for doc1, doc2, _ in tqdm(self.all_similarities, desc=\"Clustering\"):\n",
    "            uf.union(doc1, doc2)\n",
    "\n",
    "        clusters = uf.get_clusters()\n",
    "        n_clusters = len(set(clusters.values()))\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nCreated {n_clusters:,} clusters in {total_time:.2f}s total\")\n",
    "\n",
    "        return clusters, self.all_similarities\n",
    "\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_id']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "\n",
    "def calculate_poem_cluster_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def cluster_poems(poem_to_clusters: Dict, similarity_threshold: float = 0.60):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            poem_a = poem_ids[i]\n",
    "            poem_b = poem_ids[j]\n",
    "            similarity = calculate_poem_cluster_similarity(\n",
    "                poem_to_clusters[poem_a],\n",
    "                poem_to_clusters[poem_b]\n",
    "            )\n",
    "            if similarity >= similarity_threshold:\n",
    "                edges.append((poem_a, poem_b, similarity))\n",
    "\n",
    "    class PoemUnionFind:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return\n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "    for poem_a, poem_b, _ in edges:\n",
    "        uf.union(poem_a, poem_b)\n",
    "\n",
    "    poem_clusters = {poem_id: uf.find(poem_id) for poem_id in poem_ids}\n",
    "    n_clusters = len(set(poem_clusters.values()))\n",
    "\n",
    "    return poem_clusters, edges, n_clusters\n",
    "\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    return ari, v_measure\n",
    "\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=18, fontweight='bold')\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.heatmap(ari_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax1,\n",
    "                cbar_kws={'label': 'ARI'}, xticklabels=col_labels)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('Adjusted Rand Index (ARI)', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.heatmap(vmeasure_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax2,\n",
    "                cbar_kws={'label': 'V-measure'}, xticklabels=col_labels)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('V-measure', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.heatmap(clusters_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax3,\n",
    "                cbar_kws={'label': 'Clusters'}, xticklabels=col_labels)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax3.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax3.set_title('Number of Clusters', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.heatmap(similarities_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax4,\n",
    "                cbar_kws={'label': 'Similarities'}, xticklabels=col_labels)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax4.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax4.set_title('Number of Similarities Found', fontweight='bold', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_poem_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'poem_grid_search_results.png'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Poem-Level Clustering Grid Search Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "    thresholds = results_df['threshold'].values\n",
    "    thresholds_pct = [f\"{t:.0%}\" for t in thresholds]\n",
    "\n",
    "    def normalize(vals):\n",
    "        return (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    norm_vals = normalize(results_df['ari'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax1.plot(thresholds_pct, results_df['ari'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['ari'].values)):\n",
    "        ax1.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax1.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "    ax1.set_title('ARI vs Threshold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    norm_vals = normalize(results_df['v_measure'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax2.plot(thresholds_pct, results_df['v_measure'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['v_measure'].values)):\n",
    "        ax2.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax2.set_ylabel('V-measure', fontweight='bold')\n",
    "    ax2.set_title('V-measure vs Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    prr_vals = results_df['perfect_reconstruction_rate'].values * 100\n",
    "    norm_vals = normalize(prr_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax3.plot(thresholds_pct, prr_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, prr_vals)):\n",
    "        ax3.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax3.text(i, y, f'{y:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax3.set_ylabel('Perfect Reconstruction Rate (%)', fontweight='bold')\n",
    "    ax3.set_title('Perfect Reconstruction Rate vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    n_clusters_vals = results_df['n_clusters'].values\n",
    "    norm_vals = normalize(n_clusters_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax4.plot(thresholds_pct, n_clusters_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, n_clusters_vals)):\n",
    "        ax4.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax4.text(i, y, f'{y}', ha='center', va='bottom', fontsize=9)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Poem Clusters', fontweight='bold')\n",
    "    ax4.set_title('Number of Clusters vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def verse_level_grid_search(texts, df, thresholds, shingle_sizes, num_perm=128):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_shingle_size = None\n",
    "    best_clusters = None\n",
    "    best_similarities = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL 2D GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\")\n",
    "    print(f\"Testing shingle sizes: {shingle_sizes}\\n\")\n",
    "\n",
    "    total_combinations = len(thresholds) * len(shingle_sizes)\n",
    "    print(f\"Total combinations: {total_combinations}\\n\")\n",
    "\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            print(f\"\\nTesting shingle_size={shingle_size}, threshold={threshold:.0%}...\")\n",
    "\n",
    "            clusterer = FastMinHashClustering(\n",
    "                threshold=threshold,\n",
    "                shingle_size=shingle_size,\n",
    "                num_perm=num_perm,\n",
    "                chunk_size=1\n",
    "            )\n",
    "\n",
    "            clusters, similarities = clusterer.cluster(texts)\n",
    "\n",
    "            if 'idgroup' in df.columns:\n",
    "                temp_df = df.copy()\n",
    "                temp_df['cluster_id'] = temp_df.index.map(clusters)\n",
    "\n",
    "                mask = temp_df['idgroup'].notna() & temp_df['cluster_id'].notna()\n",
    "                y_true = temp_df.loc[mask, 'idgroup'].tolist()\n",
    "                y_pred = temp_df.loc[mask, 'cluster_id'].tolist()\n",
    "\n",
    "                ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "                n_gt_clusters = len(set(y_true))\n",
    "            else:\n",
    "                ari, v_measure = 0, 0\n",
    "                n_gt_clusters = 0\n",
    "\n",
    "            n_clusters = len(set(clusters.values()))\n",
    "\n",
    "            results.append({\n",
    "                'shingle_size': shingle_size,\n",
    "                'threshold': threshold,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_similarities': len(similarities),\n",
    "                'ari': ari,\n",
    "                'v_measure': v_measure,\n",
    "                'n_gt_clusters': n_gt_clusters\n",
    "            })\n",
    "\n",
    "            if ari > best_ari:\n",
    "                best_ari = ari\n",
    "                best_threshold = threshold\n",
    "                best_shingle_size = shingle_size\n",
    "                best_clusters = clusters\n",
    "                best_similarities = similarities\n",
    "\n",
    "            print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Shingle':<10} {'Threshold':<12} {'Clusters':<10} {'Similarities':<15} {'ARI':<8} {'V-measure':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['shingle_size']:<10} \"\n",
    "              f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_similarities']:<15} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST VERSE-LEVEL PARAMETERS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest parameters by ARI:\")\n",
    "    print(f\"  Shingle size: {best_shingle_size}\")\n",
    "    print(f\"  Threshold: {best_threshold:.0%}\")\n",
    "    best_result = results_df[(results_df['threshold'] == best_threshold) &\n",
    "                              (results_df['shingle_size'] == best_shingle_size)].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Number of clusters: {best_result['n_clusters']}\")\n",
    "    print(f\"  Number of similarities found: {best_result['n_similarities']}\")\n",
    "\n",
    "    visualize_verse_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'verse_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nVerse grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_clusters, best_similarities, best_threshold, best_shingle_size, results_df\n",
    "\n",
    "\n",
    "def poem_level_grid_search(df, poem_to_clusters, thresholds):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_poem_clusters = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\\n\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nTesting threshold {threshold:.0%}...\")\n",
    "\n",
    "        poem_clusters, poem_edges, n_clusters = cluster_poems(poem_to_clusters, threshold)\n",
    "\n",
    "        if 'type_id' in df.columns:\n",
    "            poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for poem_id, predicted_cluster in poem_clusters.items():\n",
    "                if poem_id in poem_to_type:\n",
    "                    y_true.append(poem_to_type[poem_id])\n",
    "                    y_pred.append(predicted_cluster)\n",
    "\n",
    "            ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "            reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "        else:\n",
    "            ari, v_measure = 0, 0\n",
    "            reconstruction_rate, n_perfect, n_total_gt = 0, 0, 0\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_edges': len(poem_edges),\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'perfect_reconstruction_rate': reconstruction_rate,\n",
    "            'n_perfect_clusters': n_perfect,\n",
    "            'n_total_gt_clusters': n_total_gt\n",
    "        })\n",
    "\n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_threshold = threshold\n",
    "            best_poem_clusters = poem_clusters\n",
    "\n",
    "        print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Threshold':<12} {'Clusters':<10} {'Edges':<10} {'ARI':<8} {'V-measure':<12} {'Perfect Recon.':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_edges']:<10} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f} \"\n",
    "              f\"{result['perfect_reconstruction_rate']:<15.1%}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST POEM-LEVEL THRESHOLD\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest threshold by ARI: {best_threshold:.0%}\")\n",
    "    best_result = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Perfect reconstruction rate: {best_result['perfect_reconstruction_rate']:.1%}\")\n",
    "    print(f\"    ({best_result['n_perfect_clusters']:.0f}/{best_result['n_total_gt_clusters']:.0f} GT clusters perfectly reconstructed)\")\n",
    "\n",
    "    visualize_poem_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'poem_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nPoem grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_poem_clusters, best_threshold, results_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    if 'verse' in df.columns:\n",
    "        df['text'] = df['verse']\n",
    "    elif 'text' not in df.columns:\n",
    "        raise ValueError(\"Dataset must have either 'verse' or 'text' column\")\n",
    "\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    print(f\"\\nLoaded {df.shape[0]:,} verses\")\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    verse_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    shingle_sizes = [2, 3, 4, 5]\n",
    "\n",
    "    best_clusters, best_similarities, best_verse_threshold, best_shingle_size, verse_results = verse_level_grid_search(\n",
    "        texts, df, verse_thresholds, shingle_sizes, num_perm=128\n",
    "    )\n",
    "\n",
    "    df['cluster_id'] = df.index.map(best_clusters)\n",
    "\n",
    "    sim_dict = defaultdict(list)\n",
    "    for doc1, doc2, sim in best_similarities:\n",
    "        sim_dict[doc1].append(sim)\n",
    "        sim_dict[doc2].append(sim)\n",
    "\n",
    "    df['certainty'] = df.index.map(\n",
    "        lambda i: np.mean(sim_dict[i]) if i in sim_dict else 1.0\n",
    "    )\n",
    "\n",
    "    preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "    df['text_preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "    verse_output = RESULTS_DIR / \"dbbe_verse_clustered_results.csv\"\n",
    "    df.to_csv(verse_output, index=False)\n",
    "    print(f\"\\n{verse_output} saved with best parameters (shingle_size={best_shingle_size}, threshold={best_verse_threshold:.0%})\")\n",
    "\n",
    "    if 'idoriginal_poem' in df.columns and 'type_id' in df.columns:\n",
    "        poem_to_clusters, poem_verse_counts = reconstruct_poems(df)\n",
    "\n",
    "        poem_thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "\n",
    "        best_poem_clusters, best_poem_threshold, poem_results = poem_level_grid_search(\n",
    "            df, poem_to_clusters, poem_thresholds\n",
    "        )\n",
    "\n",
    "        df['poem_cluster_id'] = df['idoriginal_poem'].map(best_poem_clusters)\n",
    "\n",
    "        poem_output = RESULTS_DIR / \"dbbe_poem_level_clusters.csv\"\n",
    "        df.to_csv(poem_output, index=False)\n",
    "        print(f\"\\n{poem_output} saved with best threshold ({best_poem_threshold:.0%})\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SKIPPING POEM-LEVEL CLUSTERING\")\n",
    "        print(\"=\"*100)\n",
    "        print(\"\\nRequired columns 'idoriginal_poem' and/or 'type_id' not found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015214-3208-4830-8b30-3a29155f56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "results_df = pd.read_csv(RESULTS_DIR / 'verse_grid_search_results.csv')\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search_larger_font(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results_large_font.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=22, fontweight='bold')  # larger\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    for ax, pivot, title, cbar_label in zip(\n",
    "        axes.flat,\n",
    "        [ari_pivot, vmeasure_pivot, clusters_pivot, similarities_pivot],\n",
    "        ['Adjusted Rand Index (ARI)', 'V-measure', 'Number of Clusters', 'Number of Similarities Found'],\n",
    "        ['ARI', 'V-measure', 'Clusters', 'Similarities']\n",
    "    ):\n",
    "        sns.heatmap(pivot, annot=True, fmt='.4f' if pivot is not clusters_pivot and pivot is not similarities_pivot else '.0f',\n",
    "                    cmap='viridis', ax=ax, cbar_kws={'label': cbar_label}, annot_kws={'fontsize':16})\n",
    "        ax.set_xlabel('Similarity Threshold', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Shingle Size', fontsize=14, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "visualize_verse_grid_search_larger_font(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a46f20-71a8-4a83-90ad-5fb2b40425ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dbbe_orthographic_results/dbbe_poem_level_clusters.csv')\n",
    "output_file = Path(\"dbbe_orthographic_results/five_largest_clusters.txt\")\n",
    "\n",
    "poem_to_verses = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    poem_id = row['idoriginal_poem']\n",
    "    order = row['order'] if 'order' in df.columns else 0\n",
    "    text = str(row['text']) if pd.notna(row['text']) else ''\n",
    "    poem_to_verses[poem_id].append((order, text))\n",
    "\n",
    "cluster_to_poems = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    cid = row['poem_cluster_id']\n",
    "    poem_id = row['idoriginal_poem']\n",
    "    if poem_id not in cluster_to_poems[cid]:\n",
    "        cluster_to_poems[cid].append(poem_id)\n",
    "\n",
    "cluster_sizes = {cid: len(pids) for cid, pids in cluster_to_poems.items()}\n",
    "\n",
    "top_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for cid, size in top_clusters:\n",
    "        f.write(f\"CLUSTER ID={cid}, poems={size}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for poem_id in cluster_to_poems[cid]:\n",
    "            verses_sorted = [v for _, v in sorted(poem_to_verses[poem_id], key=lambda x: x[0])]\n",
    "            f.write(f\"Poem ID: {poem_id}\\n\")\n",
    "            for verse_text in verses_sorted:\n",
    "                rows = df[(df['idoriginal_poem'] == poem_id) & (df['text'] == verse_text)]\n",
    "                prefix = \"* \" if (rows['poem_cluster_id'] == cid).any() else \"  \"\n",
    "                f.write(f\"{prefix}{verse_text}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Five largest clusters saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427e076-d1ec-4420-9e45-cf76a4467eb9",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a9a27-ac3f-45d4-a932-916bb8c4ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "import multiprocessing as mp\n",
    "import hashlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "from numba import njit, cuda\n",
    "import psutil\n",
    "import platform\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "class SystemResourceAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.cpu_count_physical = psutil.cpu_count(logical=False) or mp.cpu_count()\n",
    "        self.cpu_count_logical = psutil.cpu_count(logical=True) or mp.cpu_count()\n",
    "        self.total_ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        self.available_ram_gb = psutil.virtual_memory().available / (1024**3)\n",
    "        self.has_gpu = self._check_gpu()\n",
    "        self.gpu_count = self._get_gpu_count() if self.has_gpu else 0\n",
    "        self.gpu_memory_gb = self._get_gpu_memory() if self.has_gpu else 0\n",
    "        \n",
    "    def _check_gpu(self):\n",
    "        try:\n",
    "            cuda.detect()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _get_gpu_count(self):\n",
    "        try:\n",
    "            return len(cuda.gpus)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _get_gpu_memory(self):\n",
    "        try:\n",
    "            if cuda.gpus:\n",
    "                return cuda.current_context().get_memory_info()[1] / (1024**3)\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def get_optimal_workers(self, task_type='cpu_intensive'):\n",
    "        if task_type == 'cpu_intensive':\n",
    "            return min(self.cpu_count_logical, 64)\n",
    "        elif task_type == 'io_intensive':\n",
    "            return min(self.cpu_count_logical * 2, 128)\n",
    "        elif task_type == 'memory_intensive':\n",
    "            workers = int(self.available_ram_gb / 2)\n",
    "            return max(min(workers, self.cpu_count_logical), 4)\n",
    "        else:\n",
    "            return self.cpu_count_logical\n",
    "    \n",
    "    def get_optimal_chunk_size(self, total_items, workers):\n",
    "        base_chunk = max(50, total_items // (workers * 8))\n",
    "        available_ram_per_worker_gb = self.available_ram_gb / workers * 0.7\n",
    "        max_chunk_by_memory = int(available_ram_per_worker_gb * 100000)\n",
    "        return min(base_chunk, max_chunk_by_memory, 50000)\n",
    "    \n",
    "    def should_use_gpu(self, data_size):\n",
    "        if not self.has_gpu:\n",
    "            return False\n",
    "        return data_size > 10000 and self.gpu_memory_gb > 2\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"=\"*80)\n",
    "        print(\"SYSTEM RESOURCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"CPU Cores (Physical): {self.cpu_count_physical}\")\n",
    "        print(f\"CPU Cores (Logical):  {self.cpu_count_logical}\")\n",
    "        print(f\"Total RAM:            {self.total_ram_gb:.2f} GB\")\n",
    "        print(f\"Available RAM:        {self.available_ram_gb:.2f} GB\")\n",
    "        print(f\"GPU Available:        {'Yes' if self.has_gpu else 'No'}\")\n",
    "        if self.has_gpu:\n",
    "            print(f\"GPU Count:            {self.gpu_count}\")\n",
    "            print(f\"GPU Memory:           {self.gpu_memory_gb:.2f} GB per GPU\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Optimal Workers (CPU Intensive):    {self.get_optimal_workers('cpu_intensive')}\")\n",
    "        print(f\"Optimal Workers (Memory Intensive): {self.get_optimal_workers('memory_intensive')}\")\n",
    "        print(f\"Optimal Workers (I/O Intensive):    {self.get_optimal_workers('io_intensive')}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self):\n",
    "        self.monitoring = False\n",
    "        self.thread = None\n",
    "        self.peak_ram_gb = 0\n",
    "        self.peak_cpu_percent = 0\n",
    "        self.ram_samples = []\n",
    "        self.cpu_samples = []\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def _monitor_loop(self):\n",
    "        while self.monitoring:\n",
    "            ram_gb = self.process.memory_info().rss / (1024**3)\n",
    "            self.ram_samples.append(ram_gb)\n",
    "            self.peak_ram_gb = max(self.peak_ram_gb, ram_gb)\n",
    "            \n",
    "            try:\n",
    "                cpu_percent = self.process.cpu_percent(interval=0.1)\n",
    "                self.cpu_samples.append(cpu_percent)\n",
    "                self.peak_cpu_percent = max(self.peak_cpu_percent, cpu_percent)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    def start(self):\n",
    "        self.monitoring = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.monitoring = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'peak_ram_gb': self.peak_ram_gb,\n",
    "            'avg_ram_gb': np.mean(self.ram_samples) if self.ram_samples else 0,\n",
    "            'peak_cpu_percent': self.peak_cpu_percent,\n",
    "            'avg_cpu_percent': np.mean(self.cpu_samples) if self.cpu_samples else 0\n",
    "        }\n",
    "\n",
    "class TimingLogger:\n",
    "    def __init__(self):\n",
    "        self.stages = {}\n",
    "        self.current_stage = None\n",
    "        self.stage_start = None\n",
    "    \n",
    "    def start_stage(self, name):\n",
    "        self.current_stage = name\n",
    "        self.stage_start = time.time()\n",
    "    \n",
    "    def end_stage(self):\n",
    "        if self.current_stage and self.stage_start:\n",
    "            duration = time.time() - self.stage_start\n",
    "            self.stages[self.current_stage] = duration\n",
    "            self.current_stage = None\n",
    "            self.stage_start = None\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return self.stages.copy()\n",
    "\n",
    "system_analyzer = SystemResourceAnalyzer()\n",
    "resource_monitor = ResourceMonitor()\n",
    "timing_logger = TimingLogger()\n",
    "\n",
    "def get_system_info():\n",
    "    info = {\n",
    "        'hostname': socket.gethostname(),\n",
    "        'platform': platform.platform(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'processor': platform.processor(),\n",
    "        'cpu_count_physical': system_analyzer.cpu_count_physical,\n",
    "        'cpu_count_logical': system_analyzer.cpu_count_logical,\n",
    "        'total_ram_gb': system_analyzer.total_ram_gb,\n",
    "        'available_ram_gb': system_analyzer.available_ram_gb,\n",
    "        'has_gpu': system_analyzer.has_gpu,\n",
    "        'gpu_count': system_analyzer.gpu_count,\n",
    "        'gpu_memory_gb': system_analyzer.gpu_memory_gb,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    return info\n",
    "\n",
    "system_analyzer.print_summary()\n",
    "resource_monitor.start()\n",
    "script_start_time = time.time()\n",
    "\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class UnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            self.parent[px] = py\n",
    "        elif self.rank[px] > self.rank[py]:\n",
    "            self.parent[py] = px\n",
    "        else:\n",
    "            self.parent[py] = px\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        return np.array([self.find(i) for i in range(len(self.parent))], dtype=np.int32)\n",
    "\n",
    "def get_ngrams_vectorized(text, n=4):\n",
    "    if not text or len(text) < n:\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "def compute_minhash_chunk(args):\n",
    "    texts, start_idx, n_gram_size, num_perm, seed = args\n",
    "    np.random.seed(seed)\n",
    "    minhashes = []\n",
    "    for text in texts:\n",
    "        ngrams = get_ngrams_vectorized(text, n_gram_size)\n",
    "        m = MinHash(num_perm=num_perm, seed=seed)\n",
    "        if ngrams:\n",
    "            for ngram in ngrams:\n",
    "                m.update(ngram.encode('utf8'))\n",
    "        minhashes.append(m)\n",
    "    return minhashes\n",
    "\n",
    "def compute_minhash_parallel(texts, n_gram_size=3, num_perm=128, n_cores=None):\n",
    "    if n_cores is None:\n",
    "        n_cores = system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "    \n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(len(texts), n_cores)\n",
    "    chunks = [(texts[i:i+chunk_size], i, n_gram_size, num_perm, 42) \n",
    "              for i in range(0, len(texts), chunk_size)]\n",
    "    \n",
    "    print(f\"  Using {n_cores} workers with chunk size {chunk_size}\")\n",
    "    \n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(compute_minhash_chunk, chunks),\n",
    "                          total=len(chunks), desc=f\"MinHash (n={n_gram_size})\", leave=False))\n",
    "    \n",
    "    minhashes = [mh for chunk_mhs in results for mh in chunk_mhs]\n",
    "    return minhashes\n",
    "\n",
    "def fast_hash(data):\n",
    "    return int(hashlib.md5(data).hexdigest()[:16], 16)\n",
    "\n",
    "def find_exact_duplicates_fast(texts):\n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(len(texts), n_workers)\n",
    "    \n",
    "    print(f\"  Using {n_workers} workers for hashing\")\n",
    "    \n",
    "    def hash_chunk(chunk_data):\n",
    "        chunk_texts, start_idx = chunk_data\n",
    "        local_hashes = {}\n",
    "        for i, text in enumerate(chunk_texts):\n",
    "            normalized = str(text).strip().lower()\n",
    "            if not normalized:\n",
    "                continue\n",
    "            text_hash = fast_hash(normalized.encode('utf-8'))\n",
    "            local_hashes.setdefault(text_hash, []).append(start_idx + i)\n",
    "        return local_hashes\n",
    "    \n",
    "    chunks = [(texts[i:i+chunk_size], i) for i in range(0, len(texts), chunk_size)]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_results = list(tqdm(executor.map(hash_chunk, chunks),\n",
    "                                 total=len(chunks), desc=\"Hashing\", leave=False))\n",
    "    \n",
    "    text_hashes = {}\n",
    "    for chunk_hash_dict in chunk_results:\n",
    "        for hash_val, indices in chunk_hash_dict.items():\n",
    "            text_hashes.setdefault(hash_val, []).extend(indices)\n",
    "    \n",
    "    duplicate_groups = [indices for indices in text_hashes.values() if len(indices) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        if n_from_dataset > 0:\n",
    "            sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "            sample_indices.extend(sampled)\n",
    "    \n",
    "    return sorted(sample_indices)\n",
    "\n",
    "def compute_cluster_cohesion(minhashes, cluster_labels):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cohesions = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        if len(cluster_indices) > 50:\n",
    "            sampled_indices = np.random.choice(cluster_indices, 50, replace=False)\n",
    "        else:\n",
    "            sampled_indices = cluster_indices\n",
    "        \n",
    "        sims = []\n",
    "        for i in range(len(sampled_indices)):\n",
    "            for j in range(i+1, len(sampled_indices)):\n",
    "                sim = minhashes[sampled_indices[i]].jaccard(minhashes[sampled_indices[j]])\n",
    "                sims.append(sim)\n",
    "        \n",
    "        if sims:\n",
    "            cohesions.append(np.mean(sims))\n",
    "    \n",
    "    return np.mean(cohesions) if cohesions else 0.0\n",
    "\n",
    "def compute_cluster_separation(minhashes, cluster_labels, n_samples=500):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 1.0\n",
    "    \n",
    "    separations = []\n",
    "    for _ in range(n_samples):\n",
    "        c1, c2 = np.random.choice(unique_clusters, 2, replace=False)\n",
    "        idx1 = np.random.choice(np.where(cluster_labels == c1)[0])\n",
    "        idx2 = np.random.choice(np.where(cluster_labels == c2)[0])\n",
    "        sim = minhashes[idx1].jaccard(minhashes[idx2])\n",
    "        separations.append(1 - sim)\n",
    "    \n",
    "    return np.mean(separations) if separations else 0.0\n",
    "\n",
    "def compute_silhouette_approximation(minhashes, cluster_labels, n_samples=1000):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    n_total = len(cluster_labels)\n",
    "    if n_total > n_samples:\n",
    "        sample_indices = np.random.choice(n_total, n_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = np.arange(n_total)\n",
    "    \n",
    "    silhouettes = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        cluster_id = cluster_labels[idx]\n",
    "        same_cluster = np.where(cluster_labels == cluster_id)[0]\n",
    "        same_cluster = same_cluster[same_cluster != idx]\n",
    "        \n",
    "        if len(same_cluster) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(same_cluster) > 20:\n",
    "            same_cluster = np.random.choice(same_cluster, 20, replace=False)\n",
    "        \n",
    "        a = np.mean([1 - minhashes[idx].jaccard(minhashes[j]) \n",
    "                    for j in same_cluster])\n",
    "        \n",
    "        other_clusters = unique_clusters[unique_clusters != cluster_id]\n",
    "        if len(other_clusters) == 0:\n",
    "            continue\n",
    "        \n",
    "        min_b = float('inf')\n",
    "        for other_id in other_clusters:\n",
    "            other_cluster = np.where(cluster_labels == other_id)[0]\n",
    "            \n",
    "            if len(other_cluster) > 20:\n",
    "                other_cluster = np.random.choice(other_cluster, 20, replace=False)\n",
    "            \n",
    "            b = np.mean([1 - minhashes[idx].jaccard(minhashes[j]) \n",
    "                        for j in other_cluster])\n",
    "            min_b = min(min_b, b)\n",
    "        \n",
    "        s = (min_b - a) / max(a, min_b) if max(a, min_b) > 0 else 0\n",
    "        silhouettes.append(s)\n",
    "    \n",
    "    return np.mean(silhouettes) if silhouettes else 0.0\n",
    "\n",
    "def evaluate_single_config(args):\n",
    "    shingle_size, threshold, texts, sample_indices, duplicate_groups = args\n",
    "    \n",
    "    try:\n",
    "        sample_texts = [texts[i] for i in sample_indices]\n",
    "        minhashes_sample = compute_minhash_parallel(\n",
    "            sample_texts, \n",
    "            n_gram_size=shingle_size, \n",
    "            num_perm=128\n",
    "        )\n",
    "        \n",
    "        forest = MinHashLSHForest(num_perm=128)\n",
    "        for idx, mh in enumerate(minhashes_sample):\n",
    "            forest.add(str(idx), mh)\n",
    "        forest.index()\n",
    "        \n",
    "        n_sample = len(sample_indices)\n",
    "        uf = UnionFind(n_sample)\n",
    "        \n",
    "        sample_set = set(sample_indices)\n",
    "        for group in duplicate_groups:\n",
    "            sample_group = [sample_indices.index(g) for g in group if g in sample_set]\n",
    "            if len(sample_group) > 1:\n",
    "                for i in range(1, len(sample_group)):\n",
    "                    uf.union(sample_group[0], sample_group[i])\n",
    "        \n",
    "        top_k = 50\n",
    "        merges = 0\n",
    "        for idx in range(n_sample):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes_sample[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                sim = minhashes_sample[idx].jaccard(minhashes_sample[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        merges += 1\n",
    "        \n",
    "        cluster_labels = uf.get_clusters()\n",
    "        unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "        \n",
    "        n_clusters = len(unique_clusters)\n",
    "        n_multi = np.sum(cluster_sizes > 1)\n",
    "        n_singleton = np.sum(cluster_sizes == 1)\n",
    "        avg_size = float(cluster_sizes.mean())\n",
    "        max_size = int(cluster_sizes.max())\n",
    "        \n",
    "        cohesion = compute_cluster_cohesion(minhashes_sample, cluster_labels)\n",
    "        separation = compute_cluster_separation(minhashes_sample, cluster_labels)\n",
    "        silhouette = compute_silhouette_approximation(minhashes_sample, cluster_labels)\n",
    "        \n",
    "        return {\n",
    "            'shingle_size': shingle_size,\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_multi_clusters': n_multi,\n",
    "            'n_singletons': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'separation': separation,\n",
    "            'silhouette': silhouette,\n",
    "            'merges': merges\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at shingle={shingle_size}, threshold={threshold:.2f}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_parameters(texts, df, duplicate_groups, \n",
    "                          shingle_sizes=[2, 3,4,5],\n",
    "                          threshold_range=(0.3, 0.9, 7),\n",
    "                          n_sample=15000,\n",
    "                          results_folder=\"full_orthographic_results\",\n",
    "                          max_workers=None):\n",
    "    timing_logger.start_stage(\"01_verse_parameter_search\")\n",
    "    \n",
    "    if max_workers is None:\n",
    "        max_workers = system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2D GRID SEARCH: SHINGLE SIZE  THRESHOLD\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_indices = stratified_sample(df, n_sample)\n",
    "    print(f\"Sample size: {len(sample_indices):,}\")\n",
    "    \n",
    "    thresholds = np.linspace(threshold_range[0], threshold_range[1], threshold_range[2])\n",
    "    \n",
    "    print(f\"\\nParameter grid:\")\n",
    "    print(f\"  Shingle sizes: {shingle_sizes}\")\n",
    "    print(f\"  Thresholds: {len(thresholds)} values from {thresholds[0]:.2f} to {thresholds[-1]:.2f}\")\n",
    "    print(f\"  Total combinations: {len(shingle_sizes) * len(thresholds)}\")\n",
    "    \n",
    "    args_list = []\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            args_list.append((shingle_size, threshold, texts, sample_indices, duplicate_groups))\n",
    "    \n",
    "    print(f\"\\nRunning grid search with {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(evaluate_single_config, args): args for args in args_list}\n",
    "        \n",
    "        with tqdm(total=len(futures), desc=\"Grid search\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"Grid search complete in {time.time()-start_time:.1f}s\")\n",
    "    print(f\"  Valid results: {len(results)} / {len(args_list)}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\nComputing quality scores...\")\n",
    "    \n",
    "    def normalize(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val < 1e-10:\n",
    "            return pd.Series(0.5, index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    \n",
    "    silhouette_score = normalize(results_df['silhouette'])\n",
    "    cohesion_score = normalize(results_df['cohesion'])\n",
    "    separation_score = normalize(results_df['separation'])\n",
    "    \n",
    "    singleton_ratio = results_df['n_singletons'] / len(sample_indices)\n",
    "    balance_score = 1 - singleton_ratio\n",
    "    balance_score = np.clip(balance_score, 0, 1)\n",
    "    \n",
    "    results_df['quality_score'] = (\n",
    "        silhouette_score * 0.25 +\n",
    "        cohesion_score * 0.25 +\n",
    "        separation_score * 0.25 +\n",
    "        balance_score * 0.25\n",
    "    )\n",
    "    \n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    results_csv = os.path.join(results_folder, 'parameter_grid_search_results.csv')\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"Results saved: {results_csv}\")\n",
    "    \n",
    "    create_verse_grid_search_heatmap(results_df, results_folder)\n",
    "    \n",
    "    best_config = results_df.iloc[0]\n",
    "    best_shingle = int(best_config['shingle_size'])\n",
    "    best_threshold = float(best_config['threshold'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 5 CONFIGURATIONS (BY QUALITY SCORE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, (i, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "        print(f\"\\n#{idx}. Shingle size: {int(row['shingle_size'])}, Threshold: {row['threshold']:.3f}\")\n",
    "        print(f\"     Quality score: {row['quality_score']:.3f}\")\n",
    "        print(f\"     Silhouette: {row['silhouette']:.3f}, Cohesion: {row['cohesion']:.3f}, \"\n",
    "              f\"Separation: {row['separation']:.3f}\")\n",
    "        print(f\"     Clusters: {int(row['n_multi_clusters']):,}, Singletons: {int(row['n_singletons']):,}, \"\n",
    "              f\"Avg size: {row['avg_cluster_size']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SELECTED CONFIGURATION (HIGHEST QUALITY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shingle size: {best_shingle}\")\n",
    "    print(f\"Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Quality score: {best_config['quality_score']:.3f}\")\n",
    "    print(f\"  - Silhouette: {best_config['silhouette']:.3f}\")\n",
    "    print(f\"  - Cohesion: {best_config['cohesion']:.3f}\")\n",
    "    print(f\"  - Separation: {best_config['separation']:.3f}\")\n",
    "    print(f\"Multi-member clusters: {int(best_config['n_multi_clusters']):,}\")\n",
    "    print(f\"Singletons: {int(best_config['n_singletons']):,}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timing_logger.end_stage()\n",
    "    return best_shingle, best_threshold, results_df\n",
    "\n",
    "def create_verse_grid_search_heatmap(results_df, results_folder):\n",
    "    print(\"\\nCreating verse-level heatmap...\")\n",
    "    \n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    pivot_quality = results_df.pivot_table(\n",
    "        values='quality_score',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(pivot_quality, annot=True, fmt='.3f', cmap='viridis', ax=ax,\n",
    "               cbar_kws={'label': 'Quality Score'})\n",
    "    ax.set_ylabel('Threshold', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Verse-Level Quality Score Heatmap', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_folder, 'verse_grid_search_heatmap.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Verse heatmap saved: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def cluster_with_lsh_forest(minhashes, duplicate_groups, threshold, top_k=100):\n",
    "    n_docs = len(minhashes)\n",
    "    uf = UnionFind(n_docs)\n",
    "    \n",
    "    exact_merges = 0\n",
    "    for group in duplicate_groups:\n",
    "        for i in range(1, len(group)):\n",
    "            if uf.union(group[0], group[i]):\n",
    "                exact_merges += 1\n",
    "    \n",
    "    forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "    \n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(n_docs, n_workers)\n",
    "    \n",
    "    print(f\"  Indexing with {n_workers} threads, chunk size {chunk_size}\")\n",
    "    \n",
    "    def index_chunk(chunk_data):\n",
    "        chunk_minhashes, start_idx = chunk_data\n",
    "        local_forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "        for i, mh in enumerate(chunk_minhashes):\n",
    "            local_forest.add(str(start_idx + i), mh)\n",
    "        return local_forest\n",
    "    \n",
    "    for idx, mh in enumerate(tqdm(minhashes, desc=\"Indexing\", leave=False)):\n",
    "        forest.add(str(idx), mh)\n",
    "    forest.index()\n",
    "    \n",
    "    lsh_merges = 0\n",
    "    verified_pairs = 0\n",
    "    \n",
    "    optimal_chunk = system_analyzer.get_optimal_chunk_size(n_docs, system_analyzer.get_optimal_workers('memory_intensive'))\n",
    "    \n",
    "    for start_idx in tqdm(range(0, n_docs, optimal_chunk), desc=\"Clustering\"):\n",
    "        end_idx = min(start_idx + optimal_chunk, n_docs)\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                verified_pairs += 1\n",
    "                sim = minhashes[idx].jaccard(minhashes[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        lsh_merges += 1\n",
    "    \n",
    "    cluster_labels = uf.get_clusters()\n",
    "    unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "    \n",
    "    return cluster_labels, {\n",
    "        'n_clusters': len(unique_clusters),\n",
    "        'n_multi_clusters': np.sum(cluster_sizes > 1),\n",
    "        'n_singletons': np.sum(cluster_sizes == 1),\n",
    "        'avg_cluster_size': float(cluster_sizes.mean()),\n",
    "        'max_cluster_size': int(cluster_sizes.max()),\n",
    "        'exact_merges': exact_merges,\n",
    "        'lsh_merges': lsh_merges,\n",
    "        'threshold': threshold,\n",
    "        'verified_pairs': verified_pairs\n",
    "    }\n",
    "\n",
    "@njit\n",
    "def jaccard_numba(a_arr, b_arr):\n",
    "    intersection = 0\n",
    "    a_set = set(a_arr)\n",
    "    b_set = set(b_arr)\n",
    "    \n",
    "    for item in a_set:\n",
    "        if item in b_set:\n",
    "            intersection += 1\n",
    "    \n",
    "    union = len(a_set) + len(b_set) - intersection\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "@njit\n",
    "def count_shared_verses(a_arr, b_arr):\n",
    "    shared = 0\n",
    "    a_set = set(a_arr)\n",
    "    b_set = set(b_arr)\n",
    "    \n",
    "    for item in a_set:\n",
    "        if item in b_set:\n",
    "            shared += 1\n",
    "    \n",
    "    return shared\n",
    "\n",
    "class PoemUnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "    \n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 0 for e in elements}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        clusters = defaultdict(set)\n",
    "        for elem in self.parent.keys():\n",
    "            clusters[self.find(elem)].add(elem)\n",
    "        return dict(clusters)\n",
    "\n",
    "def compute_similarity_batch(args):\n",
    "    pairs_batch, poem_to_array_dict, min_shared = args\n",
    "    \n",
    "    results = []\n",
    "    for p1, p2 in pairs_batch:\n",
    "        arr1 = poem_to_array_dict[p1]\n",
    "        arr2 = poem_to_array_dict[p2]\n",
    "        \n",
    "        shared = count_shared_verses(arr1, arr2)\n",
    "        \n",
    "        if shared >= min_shared:\n",
    "            sim = jaccard_numba(arr1, arr2)\n",
    "            results.append({\n",
    "                'poem1': p1,\n",
    "                'poem2': p2,\n",
    "                'similarity': sim,\n",
    "                'shared_verses': shared\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "class PoemThresholdSelector:\n",
    "    def __init__(self, sample_size: int = 15000, random_seed: int = 42, min_shared_verses: int = 1):\n",
    "        self.sample_size = sample_size\n",
    "        self.random_seed = random_seed\n",
    "        self.min_shared_verses = min_shared_verses\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct_poems_vectorized(df):\n",
    "        valid_mask = df['cluster_id'] != -1\n",
    "        df_valid = df[valid_mask].copy()\n",
    "        \n",
    "        df_valid['idoriginal_poem'] = df_valid['idoriginal_poem'].astype(str)\n",
    "        \n",
    "        grouped = df_valid.groupby('idoriginal_poem')['cluster_id'].apply(\n",
    "            lambda x: np.array(sorted(set(x)), dtype=np.int32)\n",
    "        )\n",
    "        \n",
    "        return grouped.to_dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_inverted_index_fast(poem_to_clusters):\n",
    "        cluster_to_poems = defaultdict(set)\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            for c in clusters:\n",
    "                cluster_to_poems[c].add(poem_id)\n",
    "        return dict(cluster_to_poems)\n",
    "    \n",
    "    def find_candidate_pairs_for_sample(self, sample_poems, poem_to_clusters, cluster_to_poems):\n",
    "        sample_set = set(sample_poems)\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        poem_potential_matches = defaultdict(set)\n",
    "        \n",
    "        print(\"  Building potential matches using inverted index...\")\n",
    "        \n",
    "        for cluster_id, poems_in_cluster in cluster_to_poems.items():\n",
    "            sample_poems_in_cluster = list(poems_in_cluster & sample_set)\n",
    "            \n",
    "            if len(sample_poems_in_cluster) < 2:\n",
    "                continue\n",
    "            \n",
    "            for poem in sample_poems_in_cluster:\n",
    "                poem_potential_matches[poem].update(sample_poems_in_cluster)\n",
    "        \n",
    "        n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "        \n",
    "        def process_poem_batch(poems_batch):\n",
    "            local_pairs = set()\n",
    "            for poem_id in poems_batch:\n",
    "                poem_id_str = str(poem_id)\n",
    "                for other_poem in poem_potential_matches.get(poem_id, set()):\n",
    "                    other_poem_str = str(other_poem)\n",
    "                    if other_poem_str > poem_id_str:\n",
    "                        local_pairs.add((poem_id_str, other_poem_str))\n",
    "            return local_pairs\n",
    "        \n",
    "        poem_chunks = np.array_split(sample_poems, n_workers * 4)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "            chunk_results = list(tqdm(executor.map(process_poem_batch, poem_chunks), \n",
    "                                     total=len(poem_chunks), desc=\"Building sample pairs\"))\n",
    "        \n",
    "        for chunk_pairs in chunk_results:\n",
    "            candidate_pairs.update(chunk_pairs)\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def compute_sample_similarities(self, candidate_pairs, poem_to_array):\n",
    "        pairs_list = list(candidate_pairs)\n",
    "        \n",
    "        if len(pairs_list) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        n_cores = system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "        chunk_size = system_analyzer.get_optimal_chunk_size(len(pairs_list), n_cores)\n",
    "        chunks = [pairs_list[i:i+chunk_size] for i in range(0, len(pairs_list), chunk_size)]\n",
    "        \n",
    "        print(f\"  Computing {len(pairs_list):,} pairs using {n_cores} cores with chunk size {chunk_size}...\")\n",
    "        \n",
    "        poem_to_array_dict = {k: v for k, v in poem_to_array.items()}\n",
    "        \n",
    "        all_results = []\n",
    "        with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "            args_list = [(chunk, poem_to_array_dict, self.min_shared_verses) for chunk in chunks]\n",
    "            futures = [executor.submit(compute_similarity_batch, args) for args in args_list]\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing similarities\"):\n",
    "                all_results.extend(future.result())\n",
    "        \n",
    "        return pd.DataFrame(all_results)\n",
    "    \n",
    "    def stratified_sample_poems(self, df, poem_to_clusters):\n",
    "        has_source = 'source_dataset' in df.columns\n",
    "        \n",
    "        if has_source:\n",
    "            poem_to_source = df.groupby('idoriginal_poem')['source_dataset'].first().to_dict()\n",
    "        \n",
    "        poem_metadata = []\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            metadata = {\n",
    "                'poem_id': poem_id,\n",
    "                'n_clusters': len(clusters)\n",
    "            }\n",
    "            \n",
    "            if has_source:\n",
    "                metadata['source'] = poem_to_source.get(poem_id, 'unknown')\n",
    "            \n",
    "            poem_metadata.append(metadata)\n",
    "        \n",
    "        poem_df = pd.DataFrame(poem_metadata)\n",
    "        \n",
    "        poem_df['size_bin'] = pd.cut(poem_df['n_clusters'], \n",
    "                                      bins=[0, 5, 10, 20, 50, np.inf],\n",
    "                                      labels=['tiny', 'small', 'medium', 'large', 'huge'])\n",
    "        \n",
    "        sample_indices = []\n",
    "        \n",
    "        if has_source:\n",
    "            print(\"  Stratifying by source dataset and poem size...\")\n",
    "            for (source, size_bin), group in poem_df.groupby(['source', 'size_bin']):\n",
    "                n_in_group = len(group)\n",
    "                proportion = n_in_group / len(poem_df)\n",
    "                n_sample = max(1, int(self.sample_size * proportion))\n",
    "                n_sample = min(n_sample, n_in_group)\n",
    "                \n",
    "                sampled = group.sample(n=n_sample, random_state=self.random_seed)\n",
    "                sample_indices.extend(sampled['poem_id'].tolist())\n",
    "        else:\n",
    "            print(\"  Stratifying by poem size only...\")\n",
    "            for size_bin, group in poem_df.groupby('size_bin'):\n",
    "                n_in_group = len(group)\n",
    "                proportion = n_in_group / len(poem_df)\n",
    "                n_sample = max(1, int(self.sample_size * proportion))\n",
    "                n_sample = min(n_sample, n_in_group)\n",
    "                \n",
    "                sampled = group.sample(n=n_sample, random_state=self.random_seed)\n",
    "                sample_indices.extend(sampled['poem_id'].tolist())\n",
    "        \n",
    "        if len(sample_indices) < self.sample_size:\n",
    "            remaining = self.sample_size - len(sample_indices)\n",
    "            available = set(poem_df['poem_id']) - set(sample_indices)\n",
    "            if available:\n",
    "                additional = np.random.choice(list(available), \n",
    "                                             size=min(remaining, len(available)), \n",
    "                                             replace=False)\n",
    "                sample_indices.extend(additional)\n",
    "        \n",
    "        return sample_indices[:self.sample_size]\n",
    "    \n",
    "    def cluster_at_threshold(self, similarities_df, threshold, sample_poems, poem_to_array):\n",
    "        valid_pairs = similarities_df[similarities_df['similarity'] >= threshold]\n",
    "        \n",
    "        uf = PoemUnionFind(sample_poems)\n",
    "        \n",
    "        for _, row in valid_pairs.iterrows():\n",
    "            uf.union(row['poem1'], row['poem2'])\n",
    "        \n",
    "        poem_clusters = uf.get_clusters()\n",
    "        cluster_assignments = {}\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            for poem in poems:\n",
    "                cluster_assignments[poem] = cluster_id\n",
    "        \n",
    "        return cluster_assignments\n",
    "    \n",
    "    def compute_cluster_cohesion(self, poem_to_array, cluster_assignments, max_comparisons=10):\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        cohesions = []\n",
    "        \n",
    "        clusters = defaultdict(list)\n",
    "        for poem_id in poem_ids:\n",
    "            cluster_id = cluster_assignments.get(poem_id)\n",
    "            if cluster_id is not None:\n",
    "                clusters[cluster_id].append(poem_id)\n",
    "        \n",
    "        for cluster_id, cluster_poems in clusters.items():\n",
    "            if len(cluster_poems) < 2:\n",
    "                continue\n",
    "            \n",
    "            if len(cluster_poems) > 20:\n",
    "                sampled = np.random.choice(cluster_poems, 20, replace=False)\n",
    "            else:\n",
    "                sampled = cluster_poems\n",
    "            \n",
    "            sims = []\n",
    "            for i in range(len(sampled)):\n",
    "                for j in range(i+1, min(i+1+max_comparisons, len(sampled))):\n",
    "                    sim = jaccard_numba(poem_to_array[sampled[i]], poem_to_array[sampled[j]])\n",
    "                    sims.append(sim)\n",
    "            \n",
    "            if sims:\n",
    "                cohesions.append(np.mean(sims))\n",
    "        \n",
    "        return np.mean(cohesions) if cohesions else 0.0\n",
    "    \n",
    "    def compute_cluster_separation(self, poem_to_array, cluster_assignments, n_samples=200):\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        unique_clusters = set(cluster_assignments.values())\n",
    "        \n",
    "        if len(unique_clusters) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        separations = []\n",
    "        cluster_to_poems = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            cluster_to_poems[cluster_id].append(poem_id)\n",
    "        \n",
    "        unique_clusters = list(unique_clusters)\n",
    "        for _ in range(n_samples):\n",
    "            c1, c2 = np.random.choice(unique_clusters, 2, replace=False)\n",
    "            \n",
    "            p1 = np.random.choice(cluster_to_poems[c1])\n",
    "            p2 = np.random.choice(cluster_to_poems[c2])\n",
    "            \n",
    "            sim = jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "            separations.append(1 - sim)\n",
    "        \n",
    "        return np.mean(separations) if separations else 0.0\n",
    "    \n",
    "    def compute_silhouette_approximation(self, poem_to_array, cluster_assignments, n_samples=300):\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        unique_clusters = set(cluster_assignments.values())\n",
    "        \n",
    "        if len(unique_clusters) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        if len(poem_ids) > n_samples:\n",
    "            sampled_poems = np.random.choice(poem_ids, n_samples, replace=False)\n",
    "        else:\n",
    "            sampled_poems = poem_ids\n",
    "        \n",
    "        silhouettes = []\n",
    "        cluster_to_poems = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            cluster_to_poems[cluster_id].append(poem_id)\n",
    "        \n",
    "        convergence_window = 30\n",
    "        convergence_threshold = 0.01\n",
    "        \n",
    "        for i, poem_id in enumerate(sampled_poems):\n",
    "            cluster_id = cluster_assignments[poem_id]\n",
    "            same_cluster = [p for p in cluster_to_poems[cluster_id] if p != poem_id]\n",
    "            \n",
    "            if len(same_cluster) == 0:\n",
    "                continue\n",
    "            \n",
    "            if len(same_cluster) > 10:\n",
    "                same_cluster = np.random.choice(same_cluster, 10, replace=False)\n",
    "            \n",
    "            a = np.mean([1 - jaccard_numba(poem_to_array[poem_id], poem_to_array[p]) \n",
    "                        for p in same_cluster])\n",
    "            \n",
    "            other_clusters = [c for c in unique_clusters if c != cluster_id]\n",
    "            if len(other_clusters) == 0:\n",
    "                continue\n",
    "            \n",
    "            min_b = float('inf')\n",
    "            for other_cluster in other_clusters:\n",
    "                other_poems = cluster_to_poems[other_cluster]\n",
    "                \n",
    "                if len(other_poems) > 10:\n",
    "                    other_poems = np.random.choice(other_poems, 10, replace=False)\n",
    "                \n",
    "                b = np.mean([1 - jaccard_numba(poem_to_array[poem_id], poem_to_array[p]) \n",
    "                            for p in other_poems])\n",
    "                min_b = min(min_b, b)\n",
    "            \n",
    "            s = (min_b - a) / max(a, min_b) if max(a, min_b) > 0 else 0\n",
    "            silhouettes.append(s)\n",
    "            \n",
    "            if i > convergence_window and i % 30 == 0:\n",
    "                recent_mean = np.mean(silhouettes[-convergence_window:])\n",
    "                prev_mean = np.mean(silhouettes[-2*convergence_window:-convergence_window])\n",
    "                \n",
    "                if abs(recent_mean - prev_mean) < convergence_threshold:\n",
    "                    break\n",
    "        \n",
    "        return np.mean(silhouettes) if silhouettes else 0.0\n",
    "    \n",
    "    def evaluate_threshold(self, threshold, similarities_df, sample_poems, poem_to_array):\n",
    "        cluster_assignments = self.cluster_at_threshold(\n",
    "            similarities_df, threshold, sample_poems, poem_to_array\n",
    "        )\n",
    "        \n",
    "        clusters = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            clusters[cluster_id].append(poem_id)\n",
    "        \n",
    "        n_clusters = len(clusters)\n",
    "        cluster_sizes = [len(poems) for poems in clusters.values()]\n",
    "        n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "        avg_size = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "        max_size = max(cluster_sizes) if cluster_sizes else 0\n",
    "        \n",
    "        cohesion = self.compute_cluster_cohesion(poem_to_array, cluster_assignments)\n",
    "        separation = self.compute_cluster_separation(poem_to_array, cluster_assignments)\n",
    "        silhouette = self.compute_silhouette_approximation(poem_to_array, cluster_assignments)\n",
    "        \n",
    "        n_pairs_above = len(similarities_df[similarities_df['similarity'] >= threshold])\n",
    "        pct_pairs_above = (n_pairs_above / len(similarities_df) * 100) if len(similarities_df) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singletons': n_singletons,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'separation': separation,\n",
    "            'silhouette': silhouette,\n",
    "            'n_pairs_above': n_pairs_above,\n",
    "            'pct_pairs_above': pct_pairs_above\n",
    "        }\n",
    "    \n",
    "    def grid_search_thresholds(self, similarities_df, sample_poems, poem_to_array):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ADAPTIVE GRID SEARCH: TWO-STAGE APPROACH\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        coarse_thresholds = np.linspace(0.01, 0.1, 7)\n",
    "        print(f\"Stage 1: Testing {len(coarse_thresholds)} coarse thresholds...\")\n",
    "        \n",
    "        coarse_results = []\n",
    "        for threshold in tqdm(coarse_thresholds, desc=\"Coarse search\"):\n",
    "            result = self.evaluate_threshold(threshold, similarities_df, sample_poems, poem_to_array)\n",
    "            coarse_results.append(result)\n",
    "        \n",
    "        coarse_df = pd.DataFrame(coarse_results)\n",
    "        \n",
    "        def normalize(series):\n",
    "            min_val = series.min()\n",
    "            max_val = series.max()\n",
    "            if max_val - min_val < 1e-10:\n",
    "                return pd.Series(0.5, index=series.index)\n",
    "            return (series - min_val) / (max_val - min_val)\n",
    "        \n",
    "        silhouette_score = normalize(coarse_df['silhouette'])\n",
    "        cohesion_score = normalize(coarse_df['cohesion'])\n",
    "        separation_score = normalize(coarse_df['separation'])\n",
    "        singleton_ratio = coarse_df['n_singletons'] / len(sample_poems)\n",
    "        balance_score = np.clip(1 - singleton_ratio, 0, 1)\n",
    "        \n",
    "        coarse_df['quality_score'] = (\n",
    "            silhouette_score * 0.40 +\n",
    "            cohesion_score * 0.30 +\n",
    "            separation_score * 0.20 +\n",
    "            balance_score * 0.10\n",
    "        )\n",
    "        \n",
    "        best_idx = coarse_df['quality_score'].idxmax()\n",
    "        best_coarse = coarse_df.loc[best_idx]\n",
    "        best_thresh = best_coarse['threshold']\n",
    "        \n",
    "        print(f\"  Best coarse threshold: {best_thresh:.3f} (quality: {best_coarse['quality_score']:.3f})\")\n",
    "        \n",
    "        fine_range = 0.15\n",
    "        fine_thresholds = np.linspace(\n",
    "            max(0.3, best_thresh - fine_range),\n",
    "            min(0.9, best_thresh + fine_range),\n",
    "            9\n",
    "        )\n",
    "        \n",
    "        print(f\"Stage 2: Refining around {best_thresh:.3f}  {fine_range}...\")\n",
    "        \n",
    "        fine_results = []\n",
    "        for threshold in tqdm(fine_thresholds, desc=\"Fine search\"):\n",
    "            if any(abs(r['threshold'] - threshold) < 0.01 for r in coarse_results):\n",
    "                continue\n",
    "            result = self.evaluate_threshold(threshold, similarities_df, sample_poems, poem_to_array)\n",
    "            fine_results.append(result)\n",
    "        \n",
    "        all_results = coarse_results + fine_results\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        silhouette_score = normalize(results_df['silhouette'])\n",
    "        cohesion_score = normalize(results_df['cohesion'])\n",
    "        separation_score = normalize(results_df['separation'])\n",
    "        singleton_ratio = results_df['n_singletons'] / len(sample_poems)\n",
    "        balance_score = np.clip(1 - singleton_ratio, 0, 1)\n",
    "        \n",
    "        results_df['quality_score'] = (\n",
    "            silhouette_score * 0.40 +\n",
    "            cohesion_score * 0.30 +\n",
    "            separation_score * 0.20 +\n",
    "            balance_score * 0.10\n",
    "        )\n",
    "        \n",
    "        results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "        results_df.to_csv('full_orthographic_results/poem_threshold_grid_search.csv', index=False)\n",
    "        print(f\"\\nGrid search results saved\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_poem_grid_search_line(self, results_df, selected_threshold, results_folder):\n",
    "        print(\"\\nCreating poem-level line graph...\")\n",
    "        \n",
    "        sns.set_palette(\"colorblind\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        thresholds = results_df['threshold'].values\n",
    "        \n",
    "        ax.plot(thresholds, results_df['quality_score'], 'o-', \n",
    "                linewidth=2, markersize=8, color='#0173B2', label='Quality Score')\n",
    "        ax.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=2,\n",
    "                   label=f'Selected: {selected_threshold:.3f}')\n",
    "        ax.set_xlabel('Jaccard Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Quality Score', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Poem-Level Quality Score vs Threshold', fontweight='bold', fontsize=14)\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        best_idx = results_df['quality_score'].idxmax()\n",
    "        ax.scatter(results_df.loc[best_idx, 'threshold'], \n",
    "                   results_df.loc[best_idx, 'quality_score'],\n",
    "                   color='red', s=200, marker='*', edgecolors='black', linewidth=2, zorder=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(results_folder, 'poem_grid_search_line.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Poem line graph saved: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def run_threshold_analysis(self, df):\n",
    "        timing_logger.start_stage(\"02_poem_threshold_analysis\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"OPTIMIZED POEM-LEVEL THRESHOLD SELECTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Minimum shared verses: {self.min_shared_verses}\")\n",
    "        \n",
    "        print(\"\\nStep 1: Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"  Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        print(\"\\nStep 2: Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"  Found {len(cluster_to_poems):,} verse clusters\")\n",
    "        \n",
    "        print(f\"\\nStep 3: Sampling {self.sample_size:,} poems...\")\n",
    "        sample_poems = self.stratified_sample_poems(df, poem_to_clusters)\n",
    "        print(f\"  Sampled {len(sample_poems):,} poems\")\n",
    "        \n",
    "        print(\"\\nStep 4: Finding candidate pairs in sample...\")\n",
    "        start_time = time.time()\n",
    "        candidate_pairs = self.find_candidate_pairs_for_sample(\n",
    "            sample_poems, poem_to_clusters, cluster_to_poems\n",
    "        )\n",
    "        print(f\"  Found {len(candidate_pairs):,} candidate pairs in {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        print(\"\\nStep 5: Converting to arrays...\")\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(poem_to_clusters[p]), dtype=np.int32)\n",
    "            for p in sample_poems\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nStep 6: Computing similarities...\")\n",
    "        start_time = time.time()\n",
    "        similarities_df = self.compute_sample_similarities(candidate_pairs, poem_to_array)\n",
    "        print(f\"  Computed {len(similarities_df):,} similarities in {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        similarities_df.to_csv('full_orthographic_results/poem_similarities_sample.csv', index=False)\n",
    "        \n",
    "        print(\"\\nStep 7: Adaptive grid search over thresholds...\")\n",
    "        grid_results = self.grid_search_thresholds(similarities_df, sample_poems, poem_to_array)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"THRESHOLD SELECTION BASED ON QUALITY METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        best_result = grid_results.iloc[0]\n",
    "        threshold = float(best_result['threshold'])\n",
    "        \n",
    "        print(f\"Selected Threshold:   {threshold:.4f}\")\n",
    "        print(f\"Quality Score:        {best_result['quality_score']:.4f}\")\n",
    "        print(f\"Silhouette:           {best_result['silhouette']:.4f}\")\n",
    "        print(f\"Cohesion:             {best_result['cohesion']:.4f}\")\n",
    "        print(f\"Separation:           {best_result['separation']:.4f}\")\n",
    "        print(f\"Clusters:             {int(best_result['n_clusters']):,}\")\n",
    "        print(f\"Singletons:           {int(best_result['n_singletons']):,}\")\n",
    "        print(f\"Avg Cluster Size:     {best_result['avg_cluster_size']:.2f}\")\n",
    "        \n",
    "        print(\"\\nStep 8: Creating visualizations...\")\n",
    "        self.plot_poem_grid_search_line(grid_results, threshold, 'full_orthographic_results')\n",
    "        \n",
    "        summary = {\n",
    "            'selected_threshold': threshold,\n",
    "            'quality_score': best_result['quality_score'],\n",
    "            'silhouette': best_result['silhouette'],\n",
    "            'cohesion': best_result['cohesion'],\n",
    "            'separation': best_result['separation'],\n",
    "            'n_clusters': int(best_result['n_clusters']),\n",
    "            'n_singletons': int(best_result['n_singletons']),\n",
    "            'avg_cluster_size': best_result['avg_cluster_size'],\n",
    "            'min_shared_verses': self.min_shared_verses,\n",
    "            'sample_size': len(sample_poems),\n",
    "            'total_poems': len(poem_to_clusters)\n",
    "        }\n",
    "        \n",
    "        pd.DataFrame([summary]).to_csv('full_orthographic_results/poem_enhanced_threshold_summary.csv', index=False)\n",
    "        print(f\"Summary saved\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"POEM THRESHOLD ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        return threshold, grid_results, similarities_df, poem_to_clusters\n",
    "\n",
    "def cluster_all_poems_at_threshold(df, poem_threshold, poem_to_clusters, results_folder=\"full_orthographic_results\"):\n",
    "    timing_logger.start_stage(\"03_full_poem_clustering\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLUSTERING ALL POEMS WITH OPTIMAL THRESHOLD\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Threshold: {poem_threshold:.3f}\")\n",
    "    \n",
    "    poem_to_dataset = df.groupby('idoriginal_poem')['source_dataset'].first().to_dict()\n",
    "    \n",
    "    cluster_to_poems = defaultdict(set)\n",
    "    for poem_id, clusters in poem_to_clusters.items():\n",
    "        for c in clusters:\n",
    "            cluster_to_poems[c].add(poem_id)\n",
    "    \n",
    "    print(\"\\nFinding cross-dataset candidate pairs...\")\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    \n",
    "    poems_by_dataset = defaultdict(list)\n",
    "    for poem_id, dataset in poem_to_dataset.items():\n",
    "        poems_by_dataset[dataset].append(poem_id)\n",
    "    \n",
    "    all_pairs = set()\n",
    "    \n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "    \n",
    "    def process_dataset_pair(dataset_pair):\n",
    "        dataset1, dataset2 = dataset_pair\n",
    "        poems1 = poems_by_dataset[dataset1]\n",
    "        poems2 = poems_by_dataset[dataset2]\n",
    "        poems2_set = set(poems2)\n",
    "        \n",
    "        local_pairs = set()\n",
    "        for poem_id in poems1:\n",
    "            clusters = poem_to_clusters.get(poem_id, [])\n",
    "            \n",
    "            candidates = set()\n",
    "            for cluster_id in clusters:\n",
    "                if int(cluster_id) in cluster_to_poems:\n",
    "                    candidates.update(cluster_to_poems[int(cluster_id)])\n",
    "            \n",
    "            candidates = candidates & poems2_set\n",
    "            \n",
    "            for other_poem in candidates:\n",
    "                pair = tuple(sorted([poem_id, other_poem]))\n",
    "                local_pairs.add(pair)\n",
    "        \n",
    "        return local_pairs\n",
    "    \n",
    "    dataset_pairs = []\n",
    "    for i, dataset1 in enumerate(datasets):\n",
    "        for dataset2 in datasets[i+1:]:\n",
    "            dataset_pairs.append((dataset1, dataset2))\n",
    "    \n",
    "    print(f\"  Processing {len(dataset_pairs)} dataset pairs with {n_workers} workers...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        pair_results = list(tqdm(executor.map(process_dataset_pair, dataset_pairs),\n",
    "                                total=len(dataset_pairs), desc=\"Dataset pairs\"))\n",
    "    \n",
    "    for pair_set in pair_results:\n",
    "        all_pairs.update(pair_set)\n",
    "    \n",
    "    print(f\"  Total candidate pairs: {len(all_pairs):,}\")\n",
    "    \n",
    "    print(\"\\nClustering poems...\")\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "    \n",
    "    merges = 0\n",
    "    for p1, p2 in tqdm(all_pairs, desc=\"Processing pairs\"):\n",
    "        clusters1 = poem_to_clusters[p1]\n",
    "        clusters2 = poem_to_clusters[p2]\n",
    "        \n",
    "        intersection = len(set(clusters1) & set(clusters2))\n",
    "        union = len(set(clusters1) | set(clusters2))\n",
    "        \n",
    "        if union > 0:\n",
    "            jaccard = intersection / union\n",
    "            if jaccard >= poem_threshold:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "    \n",
    "    print(f\"  Performed {merges:,} merges\")\n",
    "    \n",
    "    poem_clusters = uf.get_clusters()\n",
    "    cluster_assignments = {}\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        for poem in poems:\n",
    "            cluster_assignments[poem] = cluster_id\n",
    "    \n",
    "    n_clusters = len(poem_clusters)\n",
    "    cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "    n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "    \n",
    "    n_cross_dataset_clusters = 0\n",
    "    cross_dataset_cluster_ids = set()\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "        if len(datasets) > 1:\n",
    "            n_cross_dataset_clusters += 1\n",
    "            cross_dataset_cluster_ids.add(cluster_id)\n",
    "    \n",
    "    print(f\"\\n  Total poem clusters: {n_clusters:,}\")\n",
    "    print(f\"  Cross-dataset clusters: {n_cross_dataset_clusters:,}\")\n",
    "    print(f\"  Singleton poems: {n_singletons:,}\")\n",
    "    print(f\"  Avg cluster size: {np.mean(cluster_sizes):.2f}\")\n",
    "    print(f\"  Max cluster size: {max(cluster_sizes)}\")\n",
    "    \n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].astype(str).map(cluster_assignments)\n",
    "    df['is_cross_dataset_poem_cluster'] = df['poem_cluster_id'].isin(cross_dataset_cluster_ids)\n",
    "    \n",
    "    output_csv = os.path.join(results_folder, \"poems_clustered_full.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nFull results saved: {output_csv}\")\n",
    "    \n",
    "    poem_summary = {\n",
    "        'n_verses': len(df),\n",
    "        'n_poems': len(poem_to_clusters),\n",
    "        'n_datasets': len(set(poem_to_dataset.values())),\n",
    "        'best_jaccard_threshold': poem_threshold,\n",
    "        'n_poem_clusters': len(set(cluster_assignments.values())),\n",
    "        'n_cross_dataset_clusters': n_cross_dataset_clusters,\n",
    "        'n_poems_in_cross_dataset_clusters': sum(df['is_cross_dataset_poem_cluster'])\n",
    "    }\n",
    "    \n",
    "    summary_csv = os.path.join(results_folder, 'poem_clustering_full_summary.csv')\n",
    "    pd.DataFrame([poem_summary]).to_csv(summary_csv, index=False)\n",
    "    print(f\"Summary saved: {summary_csv}\")\n",
    "    \n",
    "    timing_logger.end_stage()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL POEM CLUSTERING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return df, poem_clusters, cluster_assignments, poem_summary\n",
    "\n",
    "def print_example_clusters(df, results_folder=\"full_orthographic_results\"):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE CLUSTERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"5 EXAMPLE VERSE-LEVEL CLUSTERS (multi-member)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    cluster_info = df[df['cluster_id'] != -1].groupby('cluster_id').agg({\n",
    "        'verse': 'count',\n",
    "        'source_dataset': lambda x: list(x.unique())\n",
    "    }).rename(columns={'verse': 'size'})\n",
    "    \n",
    "    multi_clusters = cluster_info[cluster_info['size'] > 1].sort_values('size', ascending=False)\n",
    "    \n",
    "    for idx, (cluster_id, row) in enumerate(multi_clusters.head(5).iterrows(), 1):\n",
    "        print(f\"\\nVerse Cluster {idx} (ID: {cluster_id})\")\n",
    "        print(f\"  Size: {row['size']} verses\")\n",
    "        print(f\"  Datasets: {', '.join(row['source_dataset'])}\")\n",
    "        \n",
    "        cluster_verses = df[df['cluster_id'] == cluster_id]\n",
    "        print(f\"  Example verses:\")\n",
    "        for i, (_, verse_row) in enumerate(cluster_verses.head(3).iterrows(), 1):\n",
    "            verse_text = str(verse_row['verse'])[:80]\n",
    "            print(f\"    {i}. [{verse_row['source_dataset']}] {verse_text}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"5 EXAMPLE POEM-LEVEL CLUSTERS (multi-member)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if 'poem_cluster_id' in df.columns:\n",
    "        poem_cluster_info = df[df['poem_cluster_id'].notna()].groupby('poem_cluster_id').agg({\n",
    "            'idoriginal_poem': lambda x: len(set(x)),\n",
    "            'source_dataset': lambda x: list(set(x))\n",
    "        }).rename(columns={'idoriginal_poem': 'n_poems'})\n",
    "        \n",
    "        multi_poem_clusters = poem_cluster_info[poem_cluster_info['n_poems'] > 1].sort_values('n_poems', ascending=False)\n",
    "        \n",
    "        for idx, (cluster_id, row) in enumerate(multi_poem_clusters.head(5).iterrows(), 1):\n",
    "            print(f\"\\nPoem Cluster {idx} (ID: {cluster_id})\")\n",
    "            print(f\"  Size: {row['n_poems']} poems\")\n",
    "            print(f\"  Datasets: {', '.join(row['source_dataset'])}\")\n",
    "            \n",
    "            cluster_poems = df[df['poem_cluster_id'] == cluster_id]['idoriginal_poem'].unique()\n",
    "            print(f\"  Poems in cluster:\")\n",
    "            for i, poem_id in enumerate(cluster_poems[:5], 1):\n",
    "                poem_data = df[df['idoriginal_poem'] == poem_id]\n",
    "                dataset = poem_data['source_dataset'].iloc[0]\n",
    "                n_verses = len(poem_data)\n",
    "                print(f\"    {i}. Poem {poem_id} [{dataset}] - {n_verses} verses\")\n",
    "                \n",
    "                first_verse = poem_data.iloc[0]['verse'][:80]\n",
    "                print(f\"       First verse: {first_verse}...\")\n",
    "            \n",
    "            if len(cluster_poems) > 5:\n",
    "                print(f\"    ... and {len(cluster_poems) - 5} more poems\")\n",
    "    else:\n",
    "        print(\"  Poem clustering not yet completed\")\n",
    "\n",
    "def main():\n",
    "    results_folder = \"full_orthographic_results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {results_folder}/\\n\")\n",
    "    \n",
    "    clustered_file = os.path.join(results_folder, \"clustered_optimized.csv\")\n",
    "    metrics_file = os.path.join(results_folder, \"clustering_metrics.csv\")\n",
    "    \n",
    "    if os.path.exists(clustered_file) and os.path.exists(metrics_file):\n",
    "        print(\"=\"*80)\n",
    "        print(\"FOUND EXISTING VERSE CLUSTERING RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Loading from: {clustered_file}\")\n",
    "        \n",
    "        timing_logger.start_stage(\"00_load_existing_results\")\n",
    "        \n",
    "        df = pd.read_csv(clustered_file)\n",
    "        metrics = pd.read_csv(metrics_file).iloc[0].to_dict()\n",
    "        \n",
    "        print(f\"\\nLoaded {len(df):,} verses\")\n",
    "        print(f\"Verse clusters: {metrics['n_clusters']:,}\")\n",
    "        print(f\"Multi-member clusters: {metrics['n_multi_clusters']:,}\")\n",
    "        print(f\"Singletons: {metrics['n_singletons']:,}\")\n",
    "        \n",
    "        verse_summary = {\n",
    "            'n_verses': len(df),\n",
    "            'best_shingle_size': int(metrics['best_shingle_size']),\n",
    "            'best_threshold': float(metrics['best_threshold']),\n",
    "            'n_clusters': int(metrics['n_clusters']),\n",
    "            'n_multi_clusters': int(metrics['n_multi_clusters']),\n",
    "            'n_singletons': int(metrics['n_singletons']),\n",
    "            'max_cluster_size': int(metrics['max_cluster_size'])\n",
    "        }\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        print(\"\\nSkipping verse clustering - jumping to poem-level analysis\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    else:\n",
    "        print(\"=\"*80)\n",
    "        print(\"NO EXISTING RESULTS FOUND - RUNNING FULL PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        timing_logger.start_stage(\"00_data_loading\")\n",
    "        \n",
    "        df = pd.read_csv(\"concatenated.csv\")\n",
    "        df = df[df['source_dataset'].isin(['rhoby', 'dbbe', 'phi', 'papyri'])]\n",
    "        df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "        df['verse'] = df['verse'].apply(preprocess_text)\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df[df['verse'].str.strip().str.lower() != 'nan']\n",
    "        texts = df['verse'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        print(f\"Verses: {len(texts):,}\")\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        timing_logger.start_stage(\"01_exact_duplicates\")\n",
    "        \n",
    "        duplicate_groups = find_exact_duplicates_fast(texts)\n",
    "        print(f\"Found {len(duplicate_groups):,} exact duplicate groups\")\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        best_shingle, best_threshold, grid_results = grid_search_parameters(\n",
    "            texts, df, duplicate_groups,\n",
    "            shingle_sizes=[2, 3, 4, 5],\n",
    "            threshold_range=(0.3, 0.85, 7),\n",
    "            n_sample=15000,\n",
    "            results_folder=results_folder,\n",
    "            max_workers=system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "        )\n",
    "        \n",
    "        timing_logger.start_stage(\"02_minhash_computation\")\n",
    "        \n",
    "        print(f\"\\nComputing MinHashes with optimal shingle size={best_shingle}...\")\n",
    "        minhashes = compute_minhash_parallel(texts, n_gram_size=best_shingle, num_perm=128)\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        timing_logger.start_stage(\"03_verse_clustering\")\n",
    "        \n",
    "        print(f\"\\nClustering with threshold={best_threshold:.3f}...\")\n",
    "        cluster_labels, metrics = cluster_with_lsh_forest(\n",
    "            minhashes, duplicate_groups, best_threshold, top_k=100\n",
    "        )\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        timing_logger.start_stage(\"04_save_verse_results\")\n",
    "        \n",
    "        df['cluster_id'] = cluster_labels\n",
    "        output_csv = os.path.join(results_folder, \"clustered_optimized.csv\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Clustered data saved: {output_csv}\")\n",
    "        \n",
    "        timing_logger.end_stage()\n",
    "        \n",
    "        verse_summary = {\n",
    "            'n_verses': len(df),\n",
    "            'best_shingle_size': best_shingle,\n",
    "            'best_threshold': best_threshold,\n",
    "            'n_clusters': metrics['n_clusters'],\n",
    "            'n_multi_clusters': metrics['n_multi_clusters'],\n",
    "            'n_singletons': metrics['n_singletons'],\n",
    "            'max_cluster_size': metrics['max_cluster_size']\n",
    "        }\n",
    "        \n",
    "        metrics.update({\n",
    "            'total_time_minutes': (time.time() - script_start_time) / 60,\n",
    "            'best_shingle_size': best_shingle,\n",
    "            'best_threshold': best_threshold\n",
    "        })\n",
    "        metrics_csv = os.path.join(results_folder, \"clustering_metrics.csv\")\n",
    "        pd.DataFrame([metrics]).to_csv(metrics_csv, index=False)\n",
    "        print(f\"Metrics saved: {metrics_csv}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"VERSE-LEVEL CLUSTERING COMPLETE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Optimal shingle size: {best_shingle}\")\n",
    "        print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "        print(f\"Multi-member clusters: {metrics['n_multi_clusters']:,}\")\n",
    "        print(f\"Singletons: {metrics['n_singletons']:,}\")\n",
    "        print(f\"Max cluster size: {metrics['max_cluster_size']}\")\n",
    "        print(f\"All results saved to: {results_folder}/\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"POEM-LEVEL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    selector = PoemThresholdSelector(\n",
    "        sample_size=15000, \n",
    "        random_seed=42, \n",
    "        min_shared_verses=1\n",
    "    )\n",
    "    \n",
    "    poem_threshold, poem_grid_results, poem_similarities_df, poem_to_clusters = selector.run_threshold_analysis(df)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    df_final, poem_clusters, cluster_assignments, poem_summary = cluster_all_poems_at_threshold(\n",
    "        df, \n",
    "        poem_threshold,\n",
    "        poem_to_clusters,\n",
    "        results_folder\n",
    "    )\n",
    "    \n",
    "    print_example_clusters(df_final, results_folder)\n",
    "    \n",
    "    return verse_summary, poem_threshold\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verse_summary, poem_threshold = main()\n",
    "    \n",
    "    resource_monitor.stop()\n",
    "    total_time = time.time() - script_start_time\n",
    "    \n",
    "    system_info = get_system_info()\n",
    "    resource_stats = resource_monitor.get_stats()\n",
    "    timing_summary = timing_logger.get_summary()\n",
    "    \n",
    "    report_lines = []\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"COMPREHENSIVE ORTHOGRAPHIC CLUSTERING PERFORMANCE REPORT\")\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"SYSTEM INFORMATION\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    report_lines.append(f\"Hostname:            {system_info['hostname']}\")\n",
    "    report_lines.append(f\"Platform:            {system_info['platform']}\")\n",
    "    report_lines.append(f\"Python Version:      {system_info['python_version']}\")\n",
    "    report_lines.append(f\"Processor:           {system_info['processor']}\")\n",
    "    report_lines.append(f\"CPU Cores (Physical):{system_info['cpu_count_physical']}\")\n",
    "    report_lines.append(f\"CPU Cores (Logical): {system_info['cpu_count_logical']}\")\n",
    "    report_lines.append(f\"Total RAM:           {system_info['total_ram_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"Available RAM:       {system_info['available_ram_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"GPU Available:       {'Yes' if system_info['has_gpu'] else 'No'}\")\n",
    "    if system_info['has_gpu']:\n",
    "        report_lines.append(f\"GPU Count:           {system_info['gpu_count']}\")\n",
    "        report_lines.append(f\"GPU Memory:          {system_info['gpu_memory_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"Timestamp:           {system_info['timestamp']}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"PEAK RESOURCE USAGE\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    report_lines.append(f\"Peak RAM Usage:      {resource_stats['peak_ram_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"Average RAM Usage:   {resource_stats['avg_ram_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"Peak CPU Percent:    {resource_stats['peak_cpu_percent']:.1f}%\")\n",
    "    report_lines.append(f\"Average CPU Percent: {resource_stats['avg_cpu_percent']:.1f}%\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"TIMING BREAKDOWN (BY STAGE)\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    \n",
    "    total_measured = sum(timing_summary.values())\n",
    "    for stage_name, duration in timing_summary.items():\n",
    "        pct = (duration / total_measured * 100) if total_measured > 0 else 0\n",
    "        report_lines.append(f\"{stage_name:.<50} {duration:>8.1f}s ({pct:>5.1f}%)\")\n",
    "    \n",
    "    report_lines.append(f\"{'TOTAL MEASURED TIME':.<50} {total_measured:>8.1f}s\")\n",
    "    report_lines.append(f\"{'TOTAL WALL CLOCK TIME':.<50} {total_time:>8.1f}s ({total_time/60:>6.1f} min)\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"DETAILED TIMING ANALYSIS\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    \n",
    "    verse_stages = [k for k in timing_summary.keys() if k.startswith(('00_', '01_', '02_', '03_', '04_'))]\n",
    "    verse_time = sum(timing_summary.get(k, 0) for k in verse_stages)\n",
    "    \n",
    "    poem_stages = [k for k in timing_summary.keys() if k.startswith(('05_',)) or 'poem' in k.lower()]\n",
    "    poem_time = sum(timing_summary.get(k, 0) for k in poem_stages)\n",
    "    \n",
    "    report_lines.append(f\"Verse-Level Clustering:  {verse_time:>8.1f}s ({verse_time/60:>6.1f} min)\")\n",
    "    report_lines.append(f\"Poem-Level Clustering:   {poem_time:>8.1f}s ({poem_time/60:>6.1f} min)\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"CLUSTERING RESULTS SUMMARY\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    report_lines.append(\"Verse-Level:\")\n",
    "    report_lines.append(f\"  Total verses:             {verse_summary['n_verses']:,}\")\n",
    "    report_lines.append(f\"  Total clusters:           {verse_summary['n_clusters']:,}\")\n",
    "    report_lines.append(f\"  Multi-member clusters:    {verse_summary['n_multi_clusters']:,}\")\n",
    "    report_lines.append(f\"  Singletons:               {verse_summary['n_singletons']:,}\")\n",
    "    report_lines.append(f\"  Best shingle size:        {verse_summary['best_shingle_size']}\")\n",
    "    report_lines.append(f\"  Best threshold:           {verse_summary['best_threshold']:.3f}\")\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"Poem-Level:\")\n",
    "    report_lines.append(f\"  Selected threshold:       {poem_threshold:.3f}\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"PERFORMANCE METRICS\")\n",
    "    report_lines.append(\"-\" * 80)\n",
    "    if verse_time > 0:\n",
    "        report_lines.append(f\"Verse clustering throughput:  {verse_summary['n_verses'] / verse_time:.1f} verses/sec\")\n",
    "    report_lines.append(f\"Overall processing rate:      {verse_summary['n_verses'] / total_time:.1f} verses/sec\")\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"END OF REPORT\")\n",
    "    report_lines.append(\"=\"*80)\n",
    "    \n",
    "    report_path = Path('full_orthographic_results') / 'clustering_performance_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    \n",
    "    print(f\"\\nPerformance report saved to: {report_path}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Completed.\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c10f0e-7b22-4ab6-ac79-ef7c84612928",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints_tmp2/timing_metadata.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      4\u001B[0m CHECKPOINT_DIR \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints_tmp2\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# adjust path if needed\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCHECKPOINT_DIR\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtiming_metadata.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      7\u001B[0m     checkpoint_timing \u001B[38;5;241m=\u001B[39m pickle\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(checkpoint_timing)\n",
      "File \u001B[0;32m/apps/gent/RHEL9/cascadelake-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints_tmp2/timing_metadata.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints_tmp2\")  # adjust path if needed\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'timing_metadata.pkl', 'rb') as f:\n",
    "    checkpoint_timing = pickle.load(f)\n",
    "\n",
    "print(checkpoint_timing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c70513-6017-40ef-80ef-4f674647d345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

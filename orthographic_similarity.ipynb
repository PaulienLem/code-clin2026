{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7252686-05bd-4c43-8175-a32a47a66cf8",
   "metadata": {},
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from orthography_helpers import LSHIndex, MinHashLshClustering, MinHashProcessor, ShingleGenerator, TextPreprocessor, UnionFind\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "DATA_FILE = 'paper_verses.csv'\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"GPU detected - using CuPy acceleration\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"No GPU - using NumPy (CPU mode)\")\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_id']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "def calculate_poem_cluster_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def cluster_poems(poem_to_clusters: Dict, similarity_threshold: float = 0.60):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            poem_a = poem_ids[i]\n",
    "            poem_b = poem_ids[j]\n",
    "            similarity = calculate_poem_cluster_similarity(\n",
    "                poem_to_clusters[poem_a],\n",
    "                poem_to_clusters[poem_b]\n",
    "            )\n",
    "            if similarity >= similarity_threshold:\n",
    "                edges.append((poem_a, poem_b, similarity))\n",
    "\n",
    "    class PoemUnionFind:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return\n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "    for poem_a, poem_b, _ in edges:\n",
    "        uf.union(poem_a, poem_b)\n",
    "\n",
    "    poem_clusters = {poem_id: uf.find(poem_id) for poem_id in poem_ids}\n",
    "    n_clusters = len(set(poem_clusters.values()))\n",
    "\n",
    "    return poem_clusters, edges, n_clusters\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    return ari, v_measure\n",
    "\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=18, fontweight='bold')\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.heatmap(ari_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax1,\n",
    "                cbar_kws={'label': 'ARI'}, xticklabels=col_labels)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('Adjusted Rand Index (ARI)', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.heatmap(vmeasure_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax2,\n",
    "                cbar_kws={'label': 'V-measure'}, xticklabels=col_labels)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('V-measure', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.heatmap(clusters_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax3,\n",
    "                cbar_kws={'label': 'Clusters'}, xticklabels=col_labels)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax3.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax3.set_title('Number of Clusters', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.heatmap(similarities_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax4,\n",
    "                cbar_kws={'label': 'Similarities'}, xticklabels=col_labels)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax4.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax4.set_title('Number of Similarities Found', fontweight='bold', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_poem_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'poem_grid_search_results.png'\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Poem-Level Clustering Grid Search Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "    thresholds = results_df['threshold'].values\n",
    "    thresholds_pct = [f\"{t:.0%}\" for t in thresholds]\n",
    "\n",
    "    def normalize(vals):\n",
    "        return (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    norm_vals = normalize(results_df['ari'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax1.plot(thresholds_pct, results_df['ari'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['ari'].values)):\n",
    "        ax1.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax1.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "    ax1.set_title('ARI vs Threshold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    norm_vals = normalize(results_df['v_measure'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax2.plot(thresholds_pct, results_df['v_measure'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['v_measure'].values)):\n",
    "        ax2.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax2.set_ylabel('V-measure', fontweight='bold')\n",
    "    ax2.set_title('V-measure vs Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    prr_vals = results_df['perfect_reconstruction_rate'].values * 100\n",
    "    norm_vals = normalize(prr_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax3.plot(thresholds_pct, prr_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, prr_vals)):\n",
    "        ax3.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax3.text(i, y, f'{y:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax3.set_ylabel('Perfect Reconstruction Rate (%)', fontweight='bold')\n",
    "    ax3.set_title('Perfect Reconstruction Rate vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    n_clusters_vals = results_df['n_clusters'].values\n",
    "    norm_vals = normalize(n_clusters_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax4.plot(thresholds_pct, n_clusters_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, n_clusters_vals)):\n",
    "        ax4.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax4.text(i, y, f'{y}', ha='center', va='bottom', fontsize=9)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Poem Clusters', fontweight='bold')\n",
    "    ax4.set_title('Number of Clusters vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def verse_level_grid_search(texts, df, thresholds, shingle_sizes, num_perm=128):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_shingle_size = None\n",
    "    best_clusters = None\n",
    "    best_similarities = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL 2D GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\")\n",
    "    print(f\"Testing shingle sizes: {shingle_sizes}\\n\")\n",
    "\n",
    "    total_combinations = len(thresholds) * len(shingle_sizes)\n",
    "    print(f\"Total combinations: {total_combinations}\\n\")\n",
    "\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            print(f\"\\nTesting shingle_size={shingle_size}, threshold={threshold:.0%}...\")\n",
    "\n",
    "            clusterer = MinHashLshClustering(\n",
    "                threshold=threshold,\n",
    "                shingle_size=shingle_size,\n",
    "                num_perm=num_perm,\n",
    "                chunk_size=1\n",
    "            )\n",
    "\n",
    "            clusters, similarities = clusterer.cluster(texts)\n",
    "\n",
    "            if 'idgroup' in df.columns:\n",
    "                temp_df = df.copy()\n",
    "                temp_df['cluster_id'] = temp_df.index.map(clusters)\n",
    "\n",
    "                mask = temp_df['idgroup'].notna() & temp_df['cluster_id'].notna()\n",
    "                y_true = temp_df.loc[mask, 'idgroup'].tolist()\n",
    "                y_pred = temp_df.loc[mask, 'cluster_id'].tolist()\n",
    "\n",
    "                ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "                n_gt_clusters = len(set(y_true))\n",
    "            else:\n",
    "                ari, v_measure = 0, 0\n",
    "                n_gt_clusters = 0\n",
    "\n",
    "            n_clusters = len(set(clusters.values()))\n",
    "\n",
    "            results.append({\n",
    "                'shingle_size': shingle_size,\n",
    "                'threshold': threshold,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_similarities': len(similarities),\n",
    "                'ari': ari,\n",
    "                'v_measure': v_measure,\n",
    "                'n_gt_clusters': n_gt_clusters\n",
    "            })\n",
    "\n",
    "            if ari > best_ari:\n",
    "                best_ari = ari\n",
    "                best_threshold = threshold\n",
    "                best_shingle_size = shingle_size\n",
    "                best_clusters = clusters\n",
    "                best_similarities = similarities\n",
    "\n",
    "            print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Shingle':<10} {'Threshold':<12} {'Clusters':<10} {'Similarities':<15} {'ARI':<8} {'V-measure':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['shingle_size']:<10} \"\n",
    "              f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_similarities']:<15} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST VERSE-LEVEL PARAMETERS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest parameters by ARI:\")\n",
    "    print(f\"  Shingle size: {best_shingle_size}\")\n",
    "    print(f\"  Threshold: {best_threshold:.0%}\")\n",
    "    best_result = results_df[(results_df['threshold'] == best_threshold) &\n",
    "                              (results_df['shingle_size'] == best_shingle_size)].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Number of clusters: {best_result['n_clusters']}\")\n",
    "    print(f\"  Number of similarities found: {best_result['n_similarities']}\")\n",
    "\n",
    "    visualize_verse_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'verse_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nVerse grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_clusters, best_similarities, best_threshold, best_shingle_size, results_df\n",
    "\n",
    "\n",
    "def poem_level_grid_search(df, poem_to_clusters, thresholds):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_poem_clusters = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\\n\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nTesting threshold {threshold:.0%}...\")\n",
    "\n",
    "        poem_clusters, poem_edges, n_clusters = cluster_poems(poem_to_clusters, threshold)\n",
    "\n",
    "        if 'type_id' in df.columns:\n",
    "            poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for poem_id, predicted_cluster in poem_clusters.items():\n",
    "                if poem_id in poem_to_type:\n",
    "                    y_true.append(poem_to_type[poem_id])\n",
    "                    y_pred.append(predicted_cluster)\n",
    "\n",
    "            ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "            reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "        else:\n",
    "            ari, v_measure = 0, 0\n",
    "            reconstruction_rate, n_perfect, n_total_gt = 0, 0, 0\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_edges': len(poem_edges),\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'perfect_reconstruction_rate': reconstruction_rate,\n",
    "            'n_perfect_clusters': n_perfect,\n",
    "            'n_total_gt_clusters': n_total_gt\n",
    "        })\n",
    "\n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_threshold = threshold\n",
    "            best_poem_clusters = poem_clusters\n",
    "\n",
    "        print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Threshold':<12} {'Clusters':<10} {'Edges':<10} {'ARI':<8} {'V-measure':<12} {'Perfect Recon.':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_edges']:<10} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f} \"\n",
    "              f\"{result['perfect_reconstruction_rate']:<15.1%}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST POEM-LEVEL THRESHOLD\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest threshold by ARI: {best_threshold:.0%}\")\n",
    "    best_result = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Perfect reconstruction rate: {best_result['perfect_reconstruction_rate']:.1%}\")\n",
    "    print(f\"    ({best_result['n_perfect_clusters']:.0f}/{best_result['n_total_gt_clusters']:.0f} GT clusters perfectly reconstructed)\")\n",
    "\n",
    "    visualize_poem_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'poem_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nPoem grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_poem_clusters, best_threshold, results_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    if 'verse' in df.columns:\n",
    "        df['text'] = df['verse']\n",
    "    elif 'text' not in df.columns:\n",
    "        raise ValueError(\"Dataset must have either 'verse' or 'text' column\")\n",
    "\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    print(f\"\\nLoaded {df.shape[0]:,} verses\")\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    verse_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    shingle_sizes = [2, 3, 4, 5]\n",
    "\n",
    "    best_clusters, best_similarities, best_verse_threshold, best_shingle_size, verse_results = verse_level_grid_search(\n",
    "        texts, df, verse_thresholds, shingle_sizes, num_perm=128\n",
    "    )\n",
    "\n",
    "    df['cluster_id'] = df.index.map(best_clusters)\n",
    "\n",
    "    sim_dict = defaultdict(list)\n",
    "    for doc1, doc2, sim in best_similarities:\n",
    "        sim_dict[doc1].append(sim)\n",
    "        sim_dict[doc2].append(sim)\n",
    "\n",
    "    df['certainty'] = df.index.map(\n",
    "        lambda i: np.mean(sim_dict[i]) if i in sim_dict else 1.0\n",
    "    )\n",
    "\n",
    "    preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "    df['text_preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "    verse_output = RESULTS_DIR / \"dbbe_verse_clustered_results.csv\"\n",
    "    df.to_csv(verse_output, index=False)\n",
    "    print(f\"\\n{verse_output} saved with best parameters (shingle_size={best_shingle_size}, threshold={best_verse_threshold:.0%})\")\n",
    "\n",
    "    if 'idoriginal_poem' in df.columns and 'type_id' in df.columns:\n",
    "        poem_to_clusters, poem_verse_counts = reconstruct_poems(df)\n",
    "\n",
    "        poem_thresholds = [0.50, 0.60, 0.70, 0.8, 0.9]\n",
    "\n",
    "        best_poem_clusters, best_poem_threshold, poem_results = poem_level_grid_search(\n",
    "            df, poem_to_clusters, poem_thresholds\n",
    "        )\n",
    "\n",
    "        df['poem_cluster_id'] = df['idoriginal_poem'].map(best_poem_clusters)\n",
    "\n",
    "        poem_output = RESULTS_DIR / \"dbbe_poem_level_clusters.csv\"\n",
    "        df.to_csv(poem_output, index=False)\n",
    "        print(f\"\\n{poem_output} saved with best threshold ({best_poem_threshold:.0%})\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SKIPPING POEM-LEVEL CLUSTERING\")\n",
    "        print(\"=\"*100)\n",
    "        print(\"\\nRequired columns 'idoriginal_poem' and/or 'type_id' not found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "81cf9a8e0cbb78d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Full dataset",
   "id": "783e805855f9e54e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-31T15:16:06.051432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "import multiprocessing as mp\n",
    "import hashlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "from numba import njit, cuda\n",
    "import platform\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from orthography_helpers import PoemThresholdSelector, TextPreprocessor, SystemResourceAnalyzer, ResourceMonitor, TimingLogger, PerformanceReporter\n",
    "\n",
    "system_analyzer = SystemResourceAnalyzer()\n",
    "resource_monitor = ResourceMonitor()\n",
    "timing_logger = TimingLogger()\n",
    "\n",
    "def get_system_info():\n",
    "    info = {\n",
    "        'hostname': socket.gethostname(),\n",
    "        'platform': platform.platform(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'processor': platform.processor(),\n",
    "        'cpu_count_physical': system_analyzer.cpu_count_physical,\n",
    "        'cpu_count_logical': system_analyzer.cpu_count_logical,\n",
    "        'total_ram_gb': system_analyzer.total_ram_gb,\n",
    "        'available_ram_gb': system_analyzer.available_ram_gb,\n",
    "        'has_gpu': system_analyzer.has_gpu,\n",
    "        'gpu_count': system_analyzer.gpu_count,\n",
    "        'gpu_memory_gb': system_analyzer.gpu_memory_gb,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    return info\n",
    "\n",
    "system_analyzer.print_summary()\n",
    "resource_monitor.start()\n",
    "script_start_time = time.time()\n",
    "\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "class UnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            self.parent[px] = py\n",
    "        elif self.rank[px] > self.rank[py]:\n",
    "            self.parent[py] = px\n",
    "        else:\n",
    "            self.parent[py] = px\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "\n",
    "    def get_clusters(self):\n",
    "        return np.array([self.find(i) for i in range(len(self.parent))], dtype=np.int32)\n",
    "\n",
    "def get_ngrams_vectorized(text, n=4):\n",
    "    if not text or len(text) < n:\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "def compute_minhash_chunk(args):\n",
    "    texts, start_idx, n_gram_size, num_perm, seed = args\n",
    "    np.random.seed(seed)\n",
    "    minhashes = []\n",
    "    for text in texts:\n",
    "        ngrams = get_ngrams_vectorized(text, n_gram_size)\n",
    "        m = MinHash(num_perm=num_perm, seed=seed)\n",
    "        if ngrams:\n",
    "            for ngram in ngrams:\n",
    "                m.update(ngram.encode('utf8'))\n",
    "        minhashes.append(m)\n",
    "    return minhashes\n",
    "\n",
    "def compute_minhash_parallel(texts, n_gram_size=3, num_perm=128, n_cores=None):\n",
    "    if n_cores is None:\n",
    "        n_cores = system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "\n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(len(texts), n_cores)\n",
    "    chunks = [(texts[i:i+chunk_size], i, n_gram_size, num_perm, 42)\n",
    "              for i in range(0, len(texts), chunk_size)]\n",
    "\n",
    "    print(f\"  Using {n_cores} workers with chunk size {chunk_size}\")\n",
    "\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(compute_minhash_chunk, chunks),\n",
    "                          total=len(chunks), desc=f\"MinHash (n={n_gram_size})\", leave=False))\n",
    "\n",
    "    minhashes = [mh for chunk_mhs in results for mh in chunk_mhs]\n",
    "    return minhashes\n",
    "\n",
    "def fast_hash(data):\n",
    "    return int(hashlib.md5(data).hexdigest()[:16], 16)\n",
    "\n",
    "def find_exact_duplicates_fast(texts):\n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(len(texts), n_workers)\n",
    "\n",
    "    print(f\"  Using {n_workers} workers for hashing\")\n",
    "\n",
    "    def hash_chunk(chunk_data):\n",
    "        chunk_texts, start_idx = chunk_data\n",
    "        local_hashes = {}\n",
    "        for i, text in enumerate(chunk_texts):\n",
    "            normalized = str(text).strip().lower()\n",
    "            if not normalized:\n",
    "                continue\n",
    "            text_hash = fast_hash(normalized.encode('utf-8'))\n",
    "            local_hashes.setdefault(text_hash, []).append(start_idx + i)\n",
    "        return local_hashes\n",
    "\n",
    "    chunks = [(texts[i:i+chunk_size], i) for i in range(0, len(texts), chunk_size)]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        chunk_results = list(tqdm(executor.map(hash_chunk, chunks),\n",
    "                                 total=len(chunks), desc=\"Hashing\", leave=False))\n",
    "\n",
    "    text_hashes = {}\n",
    "    for chunk_hash_dict in chunk_results:\n",
    "        for hash_val, indices in chunk_hash_dict.items():\n",
    "            text_hashes.setdefault(hash_val, []).extend(indices)\n",
    "\n",
    "    duplicate_groups = [indices for indices in text_hashes.values() if len(indices) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        if n_from_dataset > 0:\n",
    "            sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "            sample_indices.extend(sampled)\n",
    "\n",
    "    return sorted(sample_indices)\n",
    "\n",
    "def compute_cluster_cohesion(minhashes, cluster_labels):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cohesions = []\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "\n",
    "        if len(cluster_indices) > 50:\n",
    "            sampled_indices = np.random.choice(cluster_indices, 50, replace=False)\n",
    "        else:\n",
    "            sampled_indices = cluster_indices\n",
    "\n",
    "        sims = []\n",
    "        for i in range(len(sampled_indices)):\n",
    "            for j in range(i+1, len(sampled_indices)):\n",
    "                sim = minhashes[sampled_indices[i]].jaccard(minhashes[sampled_indices[j]])\n",
    "                sims.append(sim)\n",
    "\n",
    "        if sims:\n",
    "            cohesions.append(np.mean(sims))\n",
    "\n",
    "    return np.mean(cohesions) if cohesions else 0.0\n",
    "\n",
    "def compute_cluster_separation(minhashes, cluster_labels, n_samples=500):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 1.0\n",
    "\n",
    "    separations = []\n",
    "    for _ in range(n_samples):\n",
    "        c1, c2 = np.random.choice(unique_clusters, 2, replace=False)\n",
    "        idx1 = np.random.choice(np.where(cluster_labels == c1)[0])\n",
    "        idx2 = np.random.choice(np.where(cluster_labels == c2)[0])\n",
    "        sim = minhashes[idx1].jaccard(minhashes[idx2])\n",
    "        separations.append(1 - sim)\n",
    "\n",
    "    return np.mean(separations) if separations else 0.0\n",
    "\n",
    "def compute_silhouette_approximation(minhashes, cluster_labels, n_samples=1000):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    n_total = len(cluster_labels)\n",
    "    if n_total > n_samples:\n",
    "        sample_indices = np.random.choice(n_total, n_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = np.arange(n_total)\n",
    "\n",
    "    silhouettes = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        cluster_id = cluster_labels[idx]\n",
    "        same_cluster = np.where(cluster_labels == cluster_id)[0]\n",
    "        same_cluster = same_cluster[same_cluster != idx]\n",
    "\n",
    "        if len(same_cluster) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(same_cluster) > 20:\n",
    "            same_cluster = np.random.choice(same_cluster, 20, replace=False)\n",
    "\n",
    "        a = np.mean([1 - minhashes[idx].jaccard(minhashes[j])\n",
    "                    for j in same_cluster])\n",
    "\n",
    "        other_clusters = unique_clusters[unique_clusters != cluster_id]\n",
    "        if len(other_clusters) == 0:\n",
    "            continue\n",
    "\n",
    "        min_b = float('inf')\n",
    "        for other_id in other_clusters:\n",
    "            other_cluster = np.where(cluster_labels == other_id)[0]\n",
    "\n",
    "            if len(other_cluster) > 20:\n",
    "                other_cluster = np.random.choice(other_cluster, 20, replace=False)\n",
    "\n",
    "            b = np.mean([1 - minhashes[idx].jaccard(minhashes[j])\n",
    "                        for j in other_cluster])\n",
    "            min_b = min(min_b, b)\n",
    "\n",
    "        s = (min_b - a) / max(a, min_b) if max(a, min_b) > 0 else 0\n",
    "        silhouettes.append(s)\n",
    "\n",
    "    return np.mean(silhouettes) if silhouettes else 0.0\n",
    "\n",
    "def evaluate_single_config(args):\n",
    "    shingle_size, threshold, texts, sample_indices, duplicate_groups = args\n",
    "\n",
    "    try:\n",
    "        sample_texts = [texts[i] for i in sample_indices]\n",
    "        minhashes_sample = compute_minhash_parallel(\n",
    "            sample_texts,\n",
    "            n_gram_size=shingle_size,\n",
    "            num_perm=128\n",
    "        )\n",
    "\n",
    "        forest = MinHashLSHForest(num_perm=128)\n",
    "        for idx, mh in enumerate(minhashes_sample):\n",
    "            forest.add(str(idx), mh)\n",
    "        forest.index()\n",
    "\n",
    "        n_sample = len(sample_indices)\n",
    "        uf = UnionFind(n_sample)\n",
    "\n",
    "        sample_set = set(sample_indices)\n",
    "        for group in duplicate_groups:\n",
    "            sample_group = [sample_indices.index(g) for g in group if g in sample_set]\n",
    "            if len(sample_group) > 1:\n",
    "                for i in range(1, len(sample_group)):\n",
    "                    uf.union(sample_group[0], sample_group[i])\n",
    "\n",
    "        top_k = 50\n",
    "        merges = 0\n",
    "        for idx in range(n_sample):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes_sample[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                sim = minhashes_sample[idx].jaccard(minhashes_sample[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        merges += 1\n",
    "\n",
    "        cluster_labels = uf.get_clusters()\n",
    "        unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "        n_clusters = len(unique_clusters)\n",
    "        n_multi = np.sum(cluster_sizes > 1)\n",
    "        n_singleton = np.sum(cluster_sizes == 1)\n",
    "        avg_size = float(cluster_sizes.mean())\n",
    "        max_size = int(cluster_sizes.max())\n",
    "\n",
    "        cohesion = compute_cluster_cohesion(minhashes_sample, cluster_labels)\n",
    "        separation = compute_cluster_separation(minhashes_sample, cluster_labels)\n",
    "        silhouette = compute_silhouette_approximation(minhashes_sample, cluster_labels)\n",
    "\n",
    "        return {\n",
    "            'shingle_size': shingle_size,\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_multi_clusters': n_multi,\n",
    "            'n_singletons': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'separation': separation,\n",
    "            'silhouette': silhouette,\n",
    "            'merges': merges\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at shingle={shingle_size}, threshold={threshold:.2f}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_parameters(texts, df, duplicate_groups,\n",
    "                          shingle_sizes=[2, 3,4,5],\n",
    "                          threshold_range=(0.3, 0.9, 7),\n",
    "                          n_sample=15000,\n",
    "                          results_folder=\"full_orthographic_results\",\n",
    "                          max_workers=None):\n",
    "    timing_logger.start_stage(\"01_verse_parameter_search\")\n",
    "\n",
    "    if max_workers is None:\n",
    "        max_workers = system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "\n",
    "\n",
    "    print(\"2D GRID SEARCH: SHINGLE SIZE Ã— THRESHOLD\")\n",
    "\n",
    "\n",
    "    sample_indices = stratified_sample(df, n_sample)\n",
    "    print(f\"Sample size: {len(sample_indices):,}\")\n",
    "\n",
    "    thresholds = np.linspace(threshold_range[0], threshold_range[1], threshold_range[2])\n",
    "\n",
    "    print(f\"\\nParameter grid:\")\n",
    "    print(f\"  Shingle sizes: {shingle_sizes}\")\n",
    "    print(f\"  Thresholds: {len(thresholds)} values from {thresholds[0]:.2f} to {thresholds[-1]:.2f}\")\n",
    "    print(f\"  Total combinations: {len(shingle_sizes) * len(thresholds)}\")\n",
    "\n",
    "    args_list = []\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            args_list.append((shingle_size, threshold, texts, sample_indices, duplicate_groups))\n",
    "\n",
    "    print(f\"\\nRunning grid search with {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(evaluate_single_config, args): args for args in args_list}\n",
    "\n",
    "        with tqdm(total=len(futures), desc=\"Grid search\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Grid search complete in {time.time()-start_time:.1f}s\")\n",
    "    print(f\"  Valid results: {len(results)} / {len(args_list)}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\nComputing quality scores...\")\n",
    "\n",
    "    def normalize(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val < 1e-10:\n",
    "            return pd.Series(0.5, index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "    silhouette_score = normalize(results_df['silhouette'])\n",
    "    cohesion_score = normalize(results_df['cohesion'])\n",
    "    separation_score = normalize(results_df['separation'])\n",
    "\n",
    "    singleton_ratio = results_df['n_singletons'] / len(sample_indices)\n",
    "    balance_score = 1 - singleton_ratio\n",
    "    balance_score = np.clip(balance_score, 0, 1)\n",
    "\n",
    "    results_df['quality_score'] = (\n",
    "        silhouette_score * 0.25 +\n",
    "        cohesion_score * 0.25 +\n",
    "        separation_score * 0.25 +\n",
    "        balance_score * 0.25\n",
    "    )\n",
    "\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    results_csv = os.path.join(results_folder, 'parameter_grid_search_results.csv')\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"Results saved: {results_csv}\")\n",
    "\n",
    "    create_verse_grid_search_heatmap(results_df, results_folder)\n",
    "\n",
    "    best_config = results_df.iloc[0]\n",
    "    best_shingle = int(best_config['shingle_size'])\n",
    "    best_threshold = float(best_config['threshold'])\n",
    "\n",
    "    print(\"TOP 5 CONFIGURATIONS (BY QUALITY SCORE)\")\n",
    "\n",
    "    for idx, (i, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "        print(f\"\\n#{idx}. Shingle size: {int(row['shingle_size'])}, Threshold: {row['threshold']:.3f}\")\n",
    "        print(f\"     Quality score: {row['quality_score']:.3f}\")\n",
    "        print(f\"     Silhouette: {row['silhouette']:.3f}, Cohesion: {row['cohesion']:.3f}, \"\n",
    "              f\"Separation: {row['separation']:.3f}\")\n",
    "        print(f\"     Clusters: {int(row['n_multi_clusters']):,}, Singletons: {int(row['n_singletons']):,}, \"\n",
    "              f\"Avg size: {row['avg_cluster_size']:.1f}\")\n",
    "\n",
    "    print(\"SELECTED CONFIGURATION (HIGHEST QUALITY)\")\n",
    "    print(f\"Shingle size: {best_shingle}\")\n",
    "    print(f\"Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Quality score: {best_config['quality_score']:.3f}\")\n",
    "    print(f\"  - Silhouette: {best_config['silhouette']:.3f}\")\n",
    "    print(f\"  - Cohesion: {best_config['cohesion']:.3f}\")\n",
    "    print(f\"  - Separation: {best_config['separation']:.3f}\")\n",
    "    print(f\"Multi-member clusters: {int(best_config['n_multi_clusters']):,}\")\n",
    "    print(f\"Singletons: {int(best_config['n_singletons']):,}\")\n",
    "\n",
    "    timing_logger.end_stage()\n",
    "    return best_shingle, best_threshold, results_df\n",
    "\n",
    "def create_verse_grid_search_heatmap(results_df, results_folder):\n",
    "    print(\"\\nCreating verse-level heatmap...\")\n",
    "\n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    pivot_quality = results_df.pivot_table(\n",
    "        values='quality_score',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_quality, annot=True, fmt='.3f', cmap='viridis', ax=ax,\n",
    "               cbar_kws={'label': 'Quality Score'})\n",
    "    ax.set_ylabel('Threshold', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Verse-Level Quality Score Heatmap', fontweight='bold', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_folder, 'verse_grid_search_heatmap.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Verse heatmap saved: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def cluster_with_lsh_forest(minhashes, duplicate_groups, threshold, top_k=100):\n",
    "    n_docs = len(minhashes)\n",
    "    uf = UnionFind(n_docs)\n",
    "\n",
    "    exact_merges = 0\n",
    "    for group in duplicate_groups:\n",
    "        for i in range(1, len(group)):\n",
    "            if uf.union(group[0], group[i]):\n",
    "                exact_merges += 1\n",
    "\n",
    "    forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "\n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "    chunk_size = system_analyzer.get_optimal_chunk_size(n_docs, n_workers)\n",
    "\n",
    "    print(f\"  Indexing with {n_workers} threads, chunk size {chunk_size}\")\n",
    "\n",
    "    def index_chunk(chunk_data):\n",
    "        chunk_minhashes, start_idx = chunk_data\n",
    "        local_forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "        for i, mh in enumerate(chunk_minhashes):\n",
    "            local_forest.add(str(start_idx + i), mh)\n",
    "        return local_forest\n",
    "\n",
    "    for idx, mh in enumerate(tqdm(minhashes, desc=\"Indexing\", leave=False)):\n",
    "        forest.add(str(idx), mh)\n",
    "    forest.index()\n",
    "\n",
    "    lsh_merges = 0\n",
    "    verified_pairs = 0\n",
    "\n",
    "    optimal_chunk = system_analyzer.get_optimal_chunk_size(n_docs, system_analyzer.get_optimal_workers('memory_intensive'))\n",
    "\n",
    "    for start_idx in tqdm(range(0, n_docs, optimal_chunk), desc=\"Clustering\"):\n",
    "        end_idx = min(start_idx + optimal_chunk, n_docs)\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                verified_pairs += 1\n",
    "                sim = minhashes[idx].jaccard(minhashes[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        lsh_merges += 1\n",
    "\n",
    "    cluster_labels = uf.get_clusters()\n",
    "    unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "    return cluster_labels, {\n",
    "        'n_clusters': len(unique_clusters),\n",
    "        'n_multi_clusters': np.sum(cluster_sizes > 1),\n",
    "        'n_singletons': np.sum(cluster_sizes == 1),\n",
    "        'avg_cluster_size': float(cluster_sizes.mean()),\n",
    "        'max_cluster_size': int(cluster_sizes.max()),\n",
    "        'exact_merges': exact_merges,\n",
    "        'lsh_merges': lsh_merges,\n",
    "        'threshold': threshold,\n",
    "        'verified_pairs': verified_pairs\n",
    "    }\n",
    "\n",
    "@njit\n",
    "def jaccard_numba(a_arr, b_arr):\n",
    "    intersection = 0\n",
    "    a_set = set(a_arr)\n",
    "    b_set = set(b_arr)\n",
    "\n",
    "    for item in a_set:\n",
    "        if item in b_set:\n",
    "            intersection += 1\n",
    "\n",
    "    union = len(a_set) + len(b_set) - intersection\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "@njit\n",
    "def count_shared_verses(a_arr, b_arr):\n",
    "    shared = 0\n",
    "    a_set = set(a_arr)\n",
    "    b_set = set(b_arr)\n",
    "\n",
    "    for item in a_set:\n",
    "        if item in b_set:\n",
    "            shared += 1\n",
    "\n",
    "    return shared\n",
    "\n",
    "class PoemUnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "\n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 0 for e in elements}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "\n",
    "    def get_clusters(self):\n",
    "        clusters = defaultdict(set)\n",
    "        for elem in self.parent.keys():\n",
    "            clusters[self.find(elem)].add(elem)\n",
    "        return dict(clusters)\n",
    "\n",
    "def compute_similarity_batch(args):\n",
    "    pairs_batch, poem_to_array_dict, min_shared = args\n",
    "\n",
    "    results = []\n",
    "    for p1, p2 in pairs_batch:\n",
    "        arr1 = poem_to_array_dict[p1]\n",
    "        arr2 = poem_to_array_dict[p2]\n",
    "\n",
    "        shared = count_shared_verses(arr1, arr2)\n",
    "\n",
    "        if shared >= min_shared:\n",
    "            sim = jaccard_numba(arr1, arr2)\n",
    "            results.append({\n",
    "                'poem1': p1,\n",
    "                'poem2': p2,\n",
    "                'similarity': sim,\n",
    "                'shared_verses': shared\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def cluster_all_poems_at_threshold(df, poem_threshold, poem_to_clusters, results_folder=\"full_orthographic_results\"):\n",
    "    timing_logger.start_stage(\"03_full_poem_clustering\")\n",
    "\n",
    "\n",
    "    print(\"CLUSTERING ALL POEMS WITH OPTIMAL THRESHOLD\")\n",
    "\n",
    "    print(f\"Threshold: {poem_threshold:.3f}\")\n",
    "\n",
    "    poem_to_dataset = df.groupby('idoriginal_poem')['source_dataset'].first().to_dict()\n",
    "\n",
    "    cluster_to_poems = defaultdict(set)\n",
    "    for poem_id, clusters in poem_to_clusters.items():\n",
    "        for c in clusters:\n",
    "            cluster_to_poems[c].add(poem_id)\n",
    "\n",
    "    print(\"\\nFinding cross-dataset candidate pairs...\")\n",
    "    datasets = df['source_dataset'].unique()\n",
    "\n",
    "    poems_by_dataset = defaultdict(list)\n",
    "    for poem_id, dataset in poem_to_dataset.items():\n",
    "        poems_by_dataset[dataset].append(poem_id)\n",
    "\n",
    "    all_pairs = set()\n",
    "\n",
    "    n_workers = system_analyzer.get_optimal_workers('io_intensive')\n",
    "\n",
    "    def process_dataset_pair(dataset_pair):\n",
    "        dataset1, dataset2 = dataset_pair\n",
    "        poems1 = poems_by_dataset[dataset1]\n",
    "        poems2 = poems_by_dataset[dataset2]\n",
    "        poems2_set = set(poems2)\n",
    "\n",
    "        local_pairs = set()\n",
    "        for poem_id in poems1:\n",
    "            clusters = poem_to_clusters.get(poem_id, [])\n",
    "\n",
    "            candidates = set()\n",
    "            for cluster_id in clusters:\n",
    "                if int(cluster_id) in cluster_to_poems:\n",
    "                    candidates.update(cluster_to_poems[int(cluster_id)])\n",
    "\n",
    "            candidates = candidates & poems2_set\n",
    "\n",
    "            for other_poem in candidates:\n",
    "                pair = tuple(sorted([poem_id, other_poem]))\n",
    "                local_pairs.add(pair)\n",
    "\n",
    "        return local_pairs\n",
    "\n",
    "    dataset_pairs = []\n",
    "    for i, dataset1 in enumerate(datasets):\n",
    "        for dataset2 in datasets[i+1:]:\n",
    "            dataset_pairs.append((dataset1, dataset2))\n",
    "\n",
    "    print(f\"  Processing {len(dataset_pairs)} dataset pairs with {n_workers} workers...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        pair_results = list(tqdm(executor.map(process_dataset_pair, dataset_pairs),\n",
    "                                total=len(dataset_pairs), desc=\"Dataset pairs\"))\n",
    "\n",
    "    for pair_set in pair_results:\n",
    "        all_pairs.update(pair_set)\n",
    "\n",
    "    print(f\"  Total candidate pairs: {len(all_pairs):,}\")\n",
    "\n",
    "    print(\"\\nClustering poems...\")\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "\n",
    "    merges = 0\n",
    "    for p1, p2 in tqdm(all_pairs, desc=\"Processing pairs\"):\n",
    "        clusters1 = poem_to_clusters[p1]\n",
    "        clusters2 = poem_to_clusters[p2]\n",
    "\n",
    "        intersection = len(set(clusters1) & set(clusters2))\n",
    "        union = len(set(clusters1) | set(clusters2))\n",
    "\n",
    "        if union > 0:\n",
    "            jaccard = intersection / union\n",
    "            if jaccard >= poem_threshold:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "\n",
    "    print(f\"  Performed {merges:,} merges\")\n",
    "\n",
    "    poem_clusters = uf.get_clusters()\n",
    "    cluster_assignments = {}\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        for poem in poems:\n",
    "            cluster_assignments[poem] = cluster_id\n",
    "\n",
    "    n_clusters = len(poem_clusters)\n",
    "    cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "    n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "\n",
    "    n_cross_dataset_clusters = 0\n",
    "    cross_dataset_cluster_ids = set()\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "        if len(datasets) > 1:\n",
    "            n_cross_dataset_clusters += 1\n",
    "            cross_dataset_cluster_ids.add(cluster_id)\n",
    "\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].astype(str).map(cluster_assignments)\n",
    "    df['is_cross_dataset_poem_cluster'] = df['poem_cluster_id'].isin(cross_dataset_cluster_ids)\n",
    "\n",
    "    output_csv = os.path.join(results_folder, \"poems_clustered_full.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nFull results saved: {output_csv}\")\n",
    "\n",
    "    poem_summary = {\n",
    "        'n_verses': len(df),\n",
    "        'n_poems': len(poem_to_clusters),\n",
    "        'n_datasets': len(set(poem_to_dataset.values())),\n",
    "        'best_jaccard_threshold': poem_threshold,\n",
    "        'n_poem_clusters': len(set(cluster_assignments.values())),\n",
    "        'n_cross_dataset_clusters': n_cross_dataset_clusters,\n",
    "        'n_poems_in_cross_dataset_clusters': sum(df['is_cross_dataset_poem_cluster'])\n",
    "    }\n",
    "\n",
    "    summary_csv = os.path.join(results_folder, 'poem_clustering_full_summary.csv')\n",
    "    pd.DataFrame([poem_summary]).to_csv(summary_csv, index=False)\n",
    "    print(f\"Summary saved: {summary_csv}\")\n",
    "    timing_logger.end_stage()\n",
    "    return df, poem_clusters, cluster_assignments, poem_summary\n",
    "\n",
    "def print_example_clusters(df, results_folder=\"full_orthographic_results\"):\n",
    "    print(\"5 EXAMPLE VERSE-LEVEL CLUSTERS (multi-member)\")\n",
    "    cluster_info = df[df['cluster_id'] != -1].groupby('cluster_id').agg({\n",
    "        'verse': 'count',\n",
    "        'source_dataset': lambda x: list(x.unique())\n",
    "    }).rename(columns={'verse': 'size'})\n",
    "\n",
    "    multi_clusters = cluster_info[cluster_info['size'] > 1].sort_values('size', ascending=False)\n",
    "\n",
    "    for idx, (cluster_id, row) in enumerate(multi_clusters.head(5).iterrows(), 1):\n",
    "        print(f\"\\nVerse Cluster {idx} (ID: {cluster_id})\")\n",
    "        print(f\"  Size: {row['size']} verses\")\n",
    "        print(f\"  Datasets: {', '.join(row['source_dataset'])}\")\n",
    "\n",
    "        cluster_verses = df[df['cluster_id'] == cluster_id]\n",
    "        print(f\"  Example verses:\")\n",
    "        for i, (_, verse_row) in enumerate(cluster_verses.head(3).iterrows(), 1):\n",
    "            verse_text = str(verse_row['verse'])[:80]\n",
    "            print(f\"    {i}. [{verse_row['source_dataset']}] {verse_text}...\")\n",
    "\n",
    "    print(\"5 EXAMPLE POEM-LEVEL CLUSTERS (multi-member)\")\n",
    "\n",
    "    if 'poem_cluster_id' in df.columns:\n",
    "        poem_cluster_info = df[df['poem_cluster_id'].notna()].groupby('poem_cluster_id').agg({\n",
    "            'idoriginal_poem': lambda x: len(set(x)),\n",
    "            'source_dataset': lambda x: list(set(x))\n",
    "        }).rename(columns={'idoriginal_poem': 'n_poems'})\n",
    "\n",
    "        multi_poem_clusters = poem_cluster_info[poem_cluster_info['n_poems'] > 1].sort_values('n_poems', ascending=False)\n",
    "\n",
    "        for idx, (cluster_id, row) in enumerate(multi_poem_clusters.head(5).iterrows(), 1):\n",
    "            print(f\"\\nPoem Cluster {idx} (ID: {cluster_id})\")\n",
    "            print(f\"  Size: {row['n_poems']} poems\")\n",
    "            print(f\"  Datasets: {', '.join(row['source_dataset'])}\")\n",
    "\n",
    "            cluster_poems = df[df['poem_cluster_id'] == cluster_id]['idoriginal_poem'].unique()\n",
    "            print(f\"  Poems in cluster:\")\n",
    "            for i, poem_id in enumerate(cluster_poems[:5], 1):\n",
    "                poem_data = df[df['idoriginal_poem'] == poem_id]\n",
    "                dataset = poem_data['source_dataset'].iloc[0]\n",
    "                n_verses = len(poem_data)\n",
    "                print(f\"    {i}. Poem {poem_id} [{dataset}] - {n_verses} verses\")\n",
    "\n",
    "                first_verse = poem_data.iloc[0]['verse'][:80]\n",
    "                print(f\"       First verse: {first_verse}...\")\n",
    "\n",
    "            if len(cluster_poems) > 5:\n",
    "                print(f\"    ... and {len(cluster_poems) - 5} more poems\")\n",
    "    else:\n",
    "        print(\"  Poem clustering not yet completed\")\n",
    "\n",
    "def main():\n",
    "    results_folder = \"full_orthographic_results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {results_folder}/\\n\")\n",
    "\n",
    "    clustered_file = os.path.join(results_folder, \"clustered_optimized.csv\")\n",
    "    metrics_file = os.path.join(results_folder, \"clustering_metrics.csv\")\n",
    "\n",
    "    if os.path.exists(clustered_file) and os.path.exists(metrics_file):\n",
    "        print(f\"Loading from: {clustered_file}\")\n",
    "\n",
    "        timing_logger.start_stage(\"00_load_existing_results\")\n",
    "\n",
    "        df = pd.read_csv(clustered_file)\n",
    "        metrics = pd.read_csv(metrics_file).iloc[0].to_dict()\n",
    "\n",
    "        print(f\"\\nLoaded {len(df):,} verses\")\n",
    "        print(f\"Verse clusters: {metrics['n_clusters']:,}\")\n",
    "        print(f\"Multi-member clusters: {metrics['n_multi_clusters']:,}\")\n",
    "        print(f\"Singletons: {metrics['n_singletons']:,}\")\n",
    "\n",
    "        verse_summary = {\n",
    "            'n_verses': len(df),\n",
    "            'best_shingle_size': int(metrics['best_shingle_size']),\n",
    "            'best_threshold': float(metrics['best_threshold']),\n",
    "            'n_clusters': int(metrics['n_clusters']),\n",
    "            'n_multi_clusters': int(metrics['n_multi_clusters']),\n",
    "            'n_singletons': int(metrics['n_singletons']),\n",
    "            'max_cluster_size': int(metrics['max_cluster_size'])\n",
    "        }\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "        print(\"\\nSkipping verse clustering - jumping to poem-level analysis\")\n",
    "    else:\n",
    "        timing_logger.start_stage(\"00_data_loading\")\n",
    "        df = pd.read_csv(\"~/Downloads/concatenated.csv\")\n",
    "        df = df[df['source_dataset'].isin(['rhoby', 'dbbe', 'phi', 'papyri'])]\n",
    "        df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "        preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "        df['verse'] = df['verse'].apply(preprocessor.preprocess)\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = df[df['verse'].str.strip().str.lower() != 'nan']\n",
    "        texts = df['verse'].fillna('').astype(str).tolist()\n",
    "\n",
    "        print(f\"Verses: {len(texts):,}\")\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "\n",
    "        timing_logger.start_stage(\"01_exact_duplicates\")\n",
    "\n",
    "        duplicate_groups = find_exact_duplicates_fast(texts)\n",
    "        print(f\"Found {len(duplicate_groups):,} exact duplicate groups\")\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "\n",
    "        best_shingle, best_threshold, grid_results = grid_search_parameters(\n",
    "            texts, df, duplicate_groups,\n",
    "            shingle_sizes=[2, 3, 4, 5],\n",
    "            threshold_range=(0.3, 0.85, 7),\n",
    "            n_sample=15000,\n",
    "            results_folder=results_folder,\n",
    "            max_workers=system_analyzer.get_optimal_workers('cpu_intensive')\n",
    "        )\n",
    "\n",
    "        timing_logger.start_stage(\"02_minhash_computation\")\n",
    "\n",
    "        print(f\"\\nComputing MinHashes with optimal shingle size={best_shingle}...\")\n",
    "        minhashes = compute_minhash_parallel(texts, n_gram_size=best_shingle, num_perm=128)\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "        timing_logger.start_stage(\"03_verse_clustering\")\n",
    "\n",
    "        print(f\"\\nClustering with threshold={best_threshold:.3f}...\")\n",
    "        cluster_labels, metrics = cluster_with_lsh_forest(\n",
    "            minhashes, duplicate_groups, best_threshold, top_k=100\n",
    "        )\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "\n",
    "        timing_logger.start_stage(\"04_save_verse_results\")\n",
    "\n",
    "        df['cluster_id'] = cluster_labels\n",
    "        output_csv = os.path.join(results_folder, \"clustered_optimized.csv\")\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Clustered data saved: {output_csv}\")\n",
    "\n",
    "        timing_logger.end_stage()\n",
    "\n",
    "        verse_summary = {\n",
    "            'n_verses': len(df),\n",
    "            'best_shingle_size': best_shingle,\n",
    "            'best_threshold': best_threshold,\n",
    "            'n_clusters': metrics['n_clusters'],\n",
    "            'n_multi_clusters': metrics['n_multi_clusters'],\n",
    "            'n_singletons': metrics['n_singletons'],\n",
    "            'max_cluster_size': metrics['max_cluster_size']\n",
    "        }\n",
    "\n",
    "        metrics.update({\n",
    "            'total_time_minutes': (time.time() - script_start_time) / 60,\n",
    "            'best_shingle_size': best_shingle,\n",
    "            'best_threshold': best_threshold\n",
    "        })\n",
    "        metrics_csv = os.path.join(results_folder, \"clustering_metrics.csv\")\n",
    "        pd.DataFrame([metrics]).to_csv(metrics_csv, index=False)\n",
    "        print(f\"Metrics saved: {metrics_csv}\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"POEM-LEVEL ANALYSIS\")\n",
    "\n",
    "    selector = PoemThresholdSelector(\n",
    "        sample_size=15000,\n",
    "        random_seed=42,\n",
    "        min_shared_verses=1\n",
    "    )\n",
    "\n",
    "    poem_threshold, poem_grid_results, poem_similarities_df, poem_to_clusters = selector.run_threshold_analysis(df)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    df_final, poem_clusters, cluster_assignments, poem_summary = cluster_all_poems_at_threshold(\n",
    "        df,\n",
    "        poem_threshold,\n",
    "        poem_to_clusters,\n",
    "        results_folder\n",
    "    )\n",
    "\n",
    "    print_example_clusters(df_final, results_folder)\n",
    "    return verse_summary, poem_threshold, poem_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verse_summary, poem_threshold, poem_summary = main()\n",
    "\n",
    "    resource_monitor.stop()\n",
    "    total_time = time.time() - script_start_time\n",
    "\n",
    "    system_info = get_system_info()\n",
    "    resource_stats = resource_monitor.get_stats()\n",
    "    timing_summary = timing_logger.get_summary()\n",
    "\n",
    "    reporter = PerformanceReporter(results_folder='full_orthographic_results')\n",
    "    report_path = reporter.generate_report(\n",
    "        system_info=get_system_info(),\n",
    "        resource_stats=resource_monitor.get_stats(),\n",
    "        timing_summary=timing_logger.get_summary(),\n",
    "        verse_summary=verse_summary,\n",
    "        poem_threshold=poem_threshold,\n",
    "        total_time=time.time() - script_start_time,\n",
    "        poem_summary=poem_summary\n",
    "    )"
   ],
   "id": "93daac64ec535dd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SYSTEM RESOURCE ANALYSIS\n",
      "================================================================================\n",
      "CPU Cores (Physical): 12\n",
      "CPU Cores (Logical):  14\n",
      "Total RAM:            30.80 GB\n",
      "Available RAM:        12.58 GB\n",
      "GPU Available:        No\n",
      "================================================================================\n",
      "Optimal Workers (CPU Intensive):    14\n",
      "Optimal Workers (Memory Intensive): 6\n",
      "Optimal Workers (I/O Intensive):    28\n",
      "================================================================================\n",
      "Results will be saved to: full_orthographic_results/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133616/1577838272.py:807: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"~/Downloads/concatenated.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verses: 1,537,740\n",
      "  Using 28 workers for hashing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83,415 exact duplicate groups\n",
      "2D GRID SEARCH: SHINGLE SIZE Ã— THRESHOLD\n",
      "Sample size: 14,997\n",
      "\n",
      "Parameter grid:\n",
      "  Shingle sizes: [2, 3, 4, 5]\n",
      "  Thresholds: 7 values from 0.30 to 0.85\n",
      "  Total combinations: 28\n",
      "\n",
      "Running grid search with 14 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):   0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 69/113 [00:01<00:00, 60.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):  14%|â–ˆâ–        | 16/113 [00:00<00:02, 42.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):  23%|â–ˆâ–ˆâ–Ž       | 26/113 [00:00<00:01, 48.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 56/113 [00:01<00:02, 27.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):   2%|â–         | 2/113 [00:00<00:27,  3.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=2):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 74/113 [00:02<00:00, 46.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=3):  12%|â–ˆâ–        | 13/113 [00:01<00:06, 14.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=3):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/113 [00:03<00:02, 21.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=3):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 77/113 [00:02<00:00, 41.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=3):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 105/113 [00:03<00:00, 47.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=3):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 43/113 [00:02<00:02, 23.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search:   4%|â–Ž         | 1/28 [00:55<25:11, 55.99s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search:   7%|â–‹         | 2/28 [01:02<11:42, 27.02s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=4):   1%|          | 1/113 [00:00<00:36,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search:  14%|â–ˆâ–        | 4/28 [01:13<05:07, 12.82s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MinHash (n=4):  26%|â–ˆâ–ˆâ–Œ       | 29/113 [00:01<00:02, 34.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search:  21%|â–ˆâ–ˆâ–       | 6/28 [01:24<03:18,  9.01s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 14 workers with chunk size 133\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16c57df2c77372"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7252686-05bd-4c43-8175-a32a47a66cf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51922e6-8776-4973-98ca-79e383e52cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"GPU detected - using CuPy acceleration\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"No GPU - using NumPy (CPU mode)\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, lowercase=True, remove_punctuation=True, remove_diacritics=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_diacritics = remove_diacritics\n",
    "        if remove_punctuation:\n",
    "            self.punct_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "            self.remove_chars_pattern = re.compile(r'[\\(\\)\\{\\}]')\n",
    "\n",
    "    def _remove_diacritics(self, text: str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) if pd.notna(text) else ''\n",
    "\n",
    "        if self.remove_diacritics:\n",
    "            text = self._remove_diacritics(text)\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = self.remove_chars_pattern.sub('', text)\n",
    "            text = self.punct_pattern.sub(' ', text)\n",
    "\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.preprocess(t) for t in texts]\n",
    "\n",
    "\n",
    "class ShingleGenerator:\n",
    "    def __init__(self, shingle_size: int = 4, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.shingle_size = shingle_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def generate_shingles(self, text: str) -> np.ndarray:\n",
    "        if len(text) < self.shingle_size:\n",
    "            return np.array([hash(text) % (2**31)], dtype=np.int32)\n",
    "\n",
    "        chars = self.xp.array([ord(c) for c in text], dtype=np.int32)\n",
    "        n_shingles = len(text) - self.shingle_size + 1\n",
    "\n",
    "        shingles = self.xp.zeros(n_shingles, dtype=np.int32)\n",
    "        for i in range(self.shingle_size):\n",
    "            shingles += chars[i:i+n_shingles] * (31 ** i)\n",
    "\n",
    "        unique_shingles = self.xp.unique(shingles)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            unique_shingles = cp.asnumpy(unique_shingles)\n",
    "\n",
    "        return unique_shingles\n",
    "\n",
    "    def generate_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        return [self.generate_shingles(t) for t in texts]\n",
    "\n",
    "\n",
    "class MinHashProcessor:\n",
    "    def __init__(self, num_perm: int = 128, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.num_perm = num_perm\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "        rng = self.xp.random.RandomState(42)\n",
    "        self.hash_a = rng.randint(1, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.hash_b = rng.randint(0, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.prime = np.int64(2**31-1)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            print(f\"Using GPU for MinHash ({num_perm} permutations)\")\n",
    "\n",
    "    def compute_signature(self, shingles: np.ndarray) -> np.ndarray:\n",
    "        if len(shingles) == 0:\n",
    "            return np.full(self.num_perm, self.prime, dtype=np.int64)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            shingles_gpu = self.xp.array(shingles, dtype=np.int64)\n",
    "        else:\n",
    "            shingles_gpu = shingles.astype(np.int64)\n",
    "\n",
    "        shingles_expanded = shingles_gpu[:, self.xp.newaxis]\n",
    "        hashes = (self.hash_a * shingles_expanded + self.hash_b) % self.prime\n",
    "        signature = self.xp.min(hashes, axis=0)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            signature = cp.asnumpy(signature)\n",
    "\n",
    "        return signature\n",
    "\n",
    "    def compute_batch(self, shingles_batch: List[np.ndarray]) -> np.ndarray:\n",
    "        signatures = np.zeros((len(shingles_batch), self.num_perm), dtype=np.int64)\n",
    "        for i, shingles in enumerate(shingles_batch):\n",
    "            signatures[i] = self.compute_signature(shingles)\n",
    "        return signatures\n",
    "\n",
    "\n",
    "class LSHIndex:\n",
    "    def __init__(self, threshold: float = 0.3, num_perm: int = 128):\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.bands = 16\n",
    "        self.rows = num_perm // self.bands\n",
    "        self.signatures = []\n",
    "        self.num_docs = 0\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.bands)]\n",
    "\n",
    "    def _hash_band(self, band: np.ndarray) -> int:\n",
    "        return int(hash(tuple(band)) % (2**31))\n",
    "\n",
    "    def insert_batch(self, signatures: np.ndarray, start_idx: int):\n",
    "        batch_size = signatures.shape[0]\n",
    "        self.signatures.append(signatures)\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                global_doc_id = start_idx + doc_idx\n",
    "                self.hash_tables[band_idx][band_hash].append(global_doc_id)\n",
    "\n",
    "        self.num_docs += batch_size\n",
    "\n",
    "    def query_batch(self, signatures: np.ndarray, start_idx: int) -> List[set]:\n",
    "        batch_size = signatures.shape[0]\n",
    "        candidates = [set() for _ in range(batch_size)]\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                query_doc_id = start_idx + doc_idx\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                bucket = self.hash_tables[band_idx].get(band_hash, [])\n",
    "                candidates[doc_idx].update(c for c in bucket if c < query_doc_id)\n",
    "\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SimilarityComputer:\n",
    "    def __init__(self, threshold: float = 0.3, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.threshold = threshold\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def compute_batch_similarities(self, query_sig: np.ndarray,\n",
    "                                   candidate_sigs: np.ndarray) -> np.ndarray:\n",
    "        if self.use_gpu:\n",
    "            query_gpu = self.xp.array(query_sig)\n",
    "            cands_gpu = self.xp.array(candidate_sigs)\n",
    "            query_expanded = self.xp.tile(query_gpu, (len(candidate_sigs), 1))\n",
    "            matches = self.xp.sum(query_expanded == cands_gpu, axis=1)\n",
    "            sims = matches.astype(np.float32) / query_sig.shape[0]\n",
    "            return cp.asnumpy(sims)\n",
    "        else:\n",
    "            query_expanded = np.tile(query_sig, (len(candidate_sigs), 1))\n",
    "            matches = np.sum(query_expanded == candidate_sigs, axis=1)\n",
    "            return matches.astype(np.float32) / query_sig.shape[0]\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n: int):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x: int, y: int):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "\n",
    "    def get_clusters(self) -> Dict[int, int]:\n",
    "        return {i: self.find(i) for i in range(len(self.parent))}\n",
    "\n",
    "\n",
    "class FastMinHashClustering:\n",
    "    def __init__(self, threshold: float = 0.3, shingle_size: int = 4,\n",
    "                 num_perm: int = 128, chunk_size: int = 50000,\n",
    "                 use_gpu: Optional[bool] = None):\n",
    "\n",
    "        if use_gpu is None:\n",
    "            use_gpu = GPU_AVAILABLE\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "\n",
    "        self.preprocessor = TextPreprocessor(\n",
    "            lowercase=True,\n",
    "            remove_punctuation=True,\n",
    "            remove_diacritics=True\n",
    "        )\n",
    "        self.shingler = ShingleGenerator(shingle_size, use_gpu)\n",
    "        self.minhash = MinHashProcessor(num_perm, use_gpu)\n",
    "        self.lsh_index = LSHIndex(threshold, num_perm)\n",
    "        self.similarity_computer = SimilarityComputer(threshold, use_gpu)\n",
    "        self.all_similarities = []\n",
    "\n",
    "        mode = \"GPU (CuPy)\" if self.use_gpu else \"CPU (NumPy)\"\n",
    "        print(f\"Initialized in {mode} mode\")\n",
    "\n",
    "    def cluster(self, texts: List[str]) -> Tuple[Dict[int, int], List[Tuple[int, int, float]]]:\n",
    "        n_docs = len(texts)\n",
    "        n_chunks = (n_docs + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        print(f\"\\nClustering {n_docs:,} documents in {n_chunks} chunks\")\n",
    "        print(f\"threshold={self.threshold}, chunk_size={self.chunk_size:,}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for chunk_idx in tqdm(range(n_chunks), desc=\"Processing\"):\n",
    "            chunk_start = chunk_idx * self.chunk_size\n",
    "            chunk_end = min(chunk_start + self.chunk_size, n_docs)\n",
    "            chunk_texts = texts[chunk_start:chunk_end]\n",
    "\n",
    "            processed = self.preprocessor.preprocess_batch(chunk_texts)\n",
    "            shingles = self.shingler.generate_batch(processed)\n",
    "            signatures = self.minhash.compute_batch(shingles)\n",
    "            self.lsh_index.insert_batch(signatures, chunk_start)\n",
    "\n",
    "            if chunk_start > 0:\n",
    "                candidates = self.lsh_index.query_batch(signatures, chunk_start)\n",
    "\n",
    "                for doc_idx, cand_set in enumerate(candidates):\n",
    "                    if not cand_set:\n",
    "                        continue\n",
    "\n",
    "                    query_doc_id = chunk_start + doc_idx\n",
    "                    query_sig = signatures[doc_idx]\n",
    "\n",
    "                    cand_list = sorted(cand_set)\n",
    "                    cand_sigs = []\n",
    "                    for cand_id in cand_list:\n",
    "                        batch_idx = cand_id // self.chunk_size\n",
    "                        local_idx = cand_id % self.chunk_size\n",
    "                        if batch_idx < len(self.lsh_index.signatures):\n",
    "                            cand_sigs.append(self.lsh_index.signatures[batch_idx][local_idx])\n",
    "\n",
    "                    if cand_sigs:\n",
    "                        cand_sigs = np.array(cand_sigs)\n",
    "                        sims = self.similarity_computer.compute_batch_similarities(\n",
    "                            query_sig, cand_sigs\n",
    "                        )\n",
    "\n",
    "                        for cand_id, sim in zip(cand_list[:len(sims)], sims):\n",
    "                            if sim >= self.threshold:\n",
    "                                self.all_similarities.append((cand_id, query_doc_id, float(sim)))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nFound {len(self.all_similarities):,} similarities in {elapsed:.2f}s\")\n",
    "        print(f\"Throughput: {n_docs/elapsed:,.0f} docs/sec\")\n",
    "\n",
    "        print(\"Building clusters...\")\n",
    "        uf = UnionFind(n_docs)\n",
    "        for doc1, doc2, _ in tqdm(self.all_similarities, desc=\"Clustering\"):\n",
    "            uf.union(doc1, doc2)\n",
    "\n",
    "        clusters = uf.get_clusters()\n",
    "        n_clusters = len(set(clusters.values()))\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nCreated {n_clusters:,} clusters in {total_time:.2f}s total\")\n",
    "\n",
    "        return clusters, self.all_similarities\n",
    "\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_id']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "\n",
    "def calculate_poem_cluster_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def cluster_poems(poem_to_clusters: Dict, similarity_threshold: float = 0.60):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            poem_a = poem_ids[i]\n",
    "            poem_b = poem_ids[j]\n",
    "            similarity = calculate_poem_cluster_similarity(\n",
    "                poem_to_clusters[poem_a],\n",
    "                poem_to_clusters[poem_b]\n",
    "            )\n",
    "            if similarity >= similarity_threshold:\n",
    "                edges.append((poem_a, poem_b, similarity))\n",
    "\n",
    "    class PoemUnionFind:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return\n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "    for poem_a, poem_b, _ in edges:\n",
    "        uf.union(poem_a, poem_b)\n",
    "\n",
    "    poem_clusters = {poem_id: uf.find(poem_id) for poem_id in poem_ids}\n",
    "    n_clusters = len(set(poem_clusters.values()))\n",
    "\n",
    "    return poem_clusters, edges, n_clusters\n",
    "\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    return ari, v_measure\n",
    "\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=18, fontweight='bold')\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.heatmap(ari_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax1,\n",
    "                cbar_kws={'label': 'ARI'}, xticklabels=col_labels)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('Adjusted Rand Index (ARI)', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.heatmap(vmeasure_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax2,\n",
    "                cbar_kws={'label': 'V-measure'}, xticklabels=col_labels)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('V-measure', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.heatmap(clusters_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax3,\n",
    "                cbar_kws={'label': 'Clusters'}, xticklabels=col_labels)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax3.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax3.set_title('Number of Clusters', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.heatmap(similarities_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax4,\n",
    "                cbar_kws={'label': 'Similarities'}, xticklabels=col_labels)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax4.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax4.set_title('Number of Similarities Found', fontweight='bold', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_poem_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'poem_grid_search_results.png'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Poem-Level Clustering Grid Search Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "    thresholds = results_df['threshold'].values\n",
    "    thresholds_pct = [f\"{t:.0%}\" for t in thresholds]\n",
    "\n",
    "    def normalize(vals):\n",
    "        return (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    norm_vals = normalize(results_df['ari'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax1.plot(thresholds_pct, results_df['ari'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['ari'].values)):\n",
    "        ax1.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax1.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "    ax1.set_title('ARI vs Threshold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    norm_vals = normalize(results_df['v_measure'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax2.plot(thresholds_pct, results_df['v_measure'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['v_measure'].values)):\n",
    "        ax2.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax2.set_ylabel('V-measure', fontweight='bold')\n",
    "    ax2.set_title('V-measure vs Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    prr_vals = results_df['perfect_reconstruction_rate'].values * 100\n",
    "    norm_vals = normalize(prr_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax3.plot(thresholds_pct, prr_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, prr_vals)):\n",
    "        ax3.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax3.text(i, y, f'{y:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax3.set_ylabel('Perfect Reconstruction Rate (%)', fontweight='bold')\n",
    "    ax3.set_title('Perfect Reconstruction Rate vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    n_clusters_vals = results_df['n_clusters'].values\n",
    "    norm_vals = normalize(n_clusters_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax4.plot(thresholds_pct, n_clusters_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, n_clusters_vals)):\n",
    "        ax4.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax4.text(i, y, f'{y}', ha='center', va='bottom', fontsize=9)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Poem Clusters', fontweight='bold')\n",
    "    ax4.set_title('Number of Clusters vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def verse_level_grid_search(texts, df, thresholds, shingle_sizes, num_perm=128):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_shingle_size = None\n",
    "    best_clusters = None\n",
    "    best_similarities = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL 2D GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\")\n",
    "    print(f\"Testing shingle sizes: {shingle_sizes}\\n\")\n",
    "\n",
    "    total_combinations = len(thresholds) * len(shingle_sizes)\n",
    "    print(f\"Total combinations: {total_combinations}\\n\")\n",
    "\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            print(f\"\\nTesting shingle_size={shingle_size}, threshold={threshold:.0%}...\")\n",
    "\n",
    "            clusterer = FastMinHashClustering(\n",
    "                threshold=threshold,\n",
    "                shingle_size=shingle_size,\n",
    "                num_perm=num_perm,\n",
    "                chunk_size=1\n",
    "            )\n",
    "\n",
    "            clusters, similarities = clusterer.cluster(texts)\n",
    "\n",
    "            if 'idgroup' in df.columns:\n",
    "                temp_df = df.copy()\n",
    "                temp_df['cluster_id'] = temp_df.index.map(clusters)\n",
    "\n",
    "                mask = temp_df['idgroup'].notna() & temp_df['cluster_id'].notna()\n",
    "                y_true = temp_df.loc[mask, 'idgroup'].tolist()\n",
    "                y_pred = temp_df.loc[mask, 'cluster_id'].tolist()\n",
    "\n",
    "                ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "                n_gt_clusters = len(set(y_true))\n",
    "            else:\n",
    "                ari, v_measure = 0, 0\n",
    "                n_gt_clusters = 0\n",
    "\n",
    "            n_clusters = len(set(clusters.values()))\n",
    "\n",
    "            results.append({\n",
    "                'shingle_size': shingle_size,\n",
    "                'threshold': threshold,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_similarities': len(similarities),\n",
    "                'ari': ari,\n",
    "                'v_measure': v_measure,\n",
    "                'n_gt_clusters': n_gt_clusters\n",
    "            })\n",
    "\n",
    "            if ari > best_ari:\n",
    "                best_ari = ari\n",
    "                best_threshold = threshold\n",
    "                best_shingle_size = shingle_size\n",
    "                best_clusters = clusters\n",
    "                best_similarities = similarities\n",
    "\n",
    "            print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Shingle':<10} {'Threshold':<12} {'Clusters':<10} {'Similarities':<15} {'ARI':<8} {'V-measure':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['shingle_size']:<10} \"\n",
    "              f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_similarities']:<15} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST VERSE-LEVEL PARAMETERS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest parameters by ARI:\")\n",
    "    print(f\"  Shingle size: {best_shingle_size}\")\n",
    "    print(f\"  Threshold: {best_threshold:.0%}\")\n",
    "    best_result = results_df[(results_df['threshold'] == best_threshold) &\n",
    "                              (results_df['shingle_size'] == best_shingle_size)].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Number of clusters: {best_result['n_clusters']}\")\n",
    "    print(f\"  Number of similarities found: {best_result['n_similarities']}\")\n",
    "\n",
    "    visualize_verse_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'verse_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nVerse grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_clusters, best_similarities, best_threshold, best_shingle_size, results_df\n",
    "\n",
    "\n",
    "def poem_level_grid_search(df, poem_to_clusters, thresholds):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_poem_clusters = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\\n\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nTesting threshold {threshold:.0%}...\")\n",
    "\n",
    "        poem_clusters, poem_edges, n_clusters = cluster_poems(poem_to_clusters, threshold)\n",
    "\n",
    "        if 'type_id' in df.columns:\n",
    "            poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for poem_id, predicted_cluster in poem_clusters.items():\n",
    "                if poem_id in poem_to_type:\n",
    "                    y_true.append(poem_to_type[poem_id])\n",
    "                    y_pred.append(predicted_cluster)\n",
    "\n",
    "            ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "            reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "        else:\n",
    "            ari, v_measure = 0, 0\n",
    "            reconstruction_rate, n_perfect, n_total_gt = 0, 0, 0\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_edges': len(poem_edges),\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'perfect_reconstruction_rate': reconstruction_rate,\n",
    "            'n_perfect_clusters': n_perfect,\n",
    "            'n_total_gt_clusters': n_total_gt\n",
    "        })\n",
    "\n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_threshold = threshold\n",
    "            best_poem_clusters = poem_clusters\n",
    "\n",
    "        print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Threshold':<12} {'Clusters':<10} {'Edges':<10} {'ARI':<8} {'V-measure':<12} {'Perfect Recon.':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_edges']:<10} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f} \"\n",
    "              f\"{result['perfect_reconstruction_rate']:<15.1%}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST POEM-LEVEL THRESHOLD\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest threshold by ARI: {best_threshold:.0%}\")\n",
    "    best_result = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Perfect reconstruction rate: {best_result['perfect_reconstruction_rate']:.1%}\")\n",
    "    print(f\"    ({best_result['n_perfect_clusters']:.0f}/{best_result['n_total_gt_clusters']:.0f} GT clusters perfectly reconstructed)\")\n",
    "\n",
    "    visualize_poem_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'poem_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nPoem grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_poem_clusters, best_threshold, results_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    DATA_FILE = 'dbbe_full.csv'\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    if 'verse' in df.columns:\n",
    "        df['text'] = df['verse']\n",
    "    elif 'text' not in df.columns:\n",
    "        raise ValueError(\"Dataset must have either 'verse' or 'text' column\")\n",
    "\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    print(f\"\\nLoaded {df.shape[0]:,} verses\")\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    verse_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    shingle_sizes = [2, 3, 4, 5]\n",
    "\n",
    "    best_clusters, best_similarities, best_verse_threshold, best_shingle_size, verse_results = verse_level_grid_search(\n",
    "        texts, df, verse_thresholds, shingle_sizes, num_perm=128\n",
    "    )\n",
    "\n",
    "    df['cluster_id'] = df.index.map(best_clusters)\n",
    "\n",
    "    sim_dict = defaultdict(list)\n",
    "    for doc1, doc2, sim in best_similarities:\n",
    "        sim_dict[doc1].append(sim)\n",
    "        sim_dict[doc2].append(sim)\n",
    "\n",
    "    df['certainty'] = df.index.map(\n",
    "        lambda i: np.mean(sim_dict[i]) if i in sim_dict else 1.0\n",
    "    )\n",
    "\n",
    "    preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "    df['text_preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "    verse_output = RESULTS_DIR / \"dbbe_verse_clustered_results.csv\"\n",
    "    df.to_csv(verse_output, index=False)\n",
    "    print(f\"\\n{verse_output} saved with best parameters (shingle_size={best_shingle_size}, threshold={best_verse_threshold:.0%})\")\n",
    "\n",
    "    if 'idoriginal_poem' in df.columns and 'type_id' in df.columns:\n",
    "        poem_to_clusters, poem_verse_counts = reconstruct_poems(df)\n",
    "\n",
    "        poem_thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "\n",
    "        best_poem_clusters, best_poem_threshold, poem_results = poem_level_grid_search(\n",
    "            df, poem_to_clusters, poem_thresholds\n",
    "        )\n",
    "\n",
    "        df['poem_cluster_id'] = df['idoriginal_poem'].map(best_poem_clusters)\n",
    "\n",
    "        poem_output = RESULTS_DIR / \"dbbe_poem_level_clusters.csv\"\n",
    "        df.to_csv(poem_output, index=False)\n",
    "        print(f\"\\n{poem_output} saved with best threshold ({best_poem_threshold:.0%})\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SKIPPING POEM-LEVEL CLUSTERING\")\n",
    "        print(\"=\"*100)\n",
    "        print(\"\\nRequired columns 'idoriginal_poem' and/or 'type_id' not found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015214-3208-4830-8b30-3a29155f56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "\n",
    "results_df = pd.read_csv(RESULTS_DIR / 'verse_grid_search_results.csv')\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search_larger_font(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results_large_font.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=22, fontweight='bold')  # larger\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    for ax, pivot, title, cbar_label in zip(\n",
    "        axes.flat,\n",
    "        [ari_pivot, vmeasure_pivot, clusters_pivot, similarities_pivot],\n",
    "        ['Adjusted Rand Index (ARI)', 'V-measure', 'Number of Clusters', 'Number of Similarities Found'],\n",
    "        ['ARI', 'V-measure', 'Clusters', 'Similarities']\n",
    "    ):\n",
    "        sns.heatmap(pivot, annot=True, fmt='.4f' if pivot is not clusters_pivot and pivot is not similarities_pivot else '.0f',\n",
    "                    cmap='viridis', ax=ax, cbar_kws={'label': cbar_label}, annot_kws={'fontsize':16})\n",
    "        ax.set_xlabel('Similarity Threshold', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Shingle Size', fontsize=14, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "visualize_verse_grid_search_larger_font(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a46f20-71a8-4a83-90ad-5fb2b40425ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five largest clusters saved to: dbbe_orthographic_results/five_largest_clusters.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('dbbe_orthographic_results/dbbe_poem_level_clusters.csv')\n",
    "output_file = Path(\"dbbe_orthographic_results/five_largest_clusters.txt\")\n",
    "\n",
    "# Reconstruct poems: poem_id -> list of (order, text)\n",
    "poem_to_verses = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    poem_id = row['idoriginal_poem']\n",
    "    order = row['order'] if 'order' in df.columns else 0\n",
    "    text = str(row['text']) if pd.notna(row['text']) else ''\n",
    "    poem_to_verses[poem_id].append((order, text))\n",
    "\n",
    "# Cluster -> list of poem_ids\n",
    "cluster_to_poems = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    cid = row['poem_cluster_id']\n",
    "    poem_id = row['idoriginal_poem']\n",
    "    if poem_id not in cluster_to_poems[cid]:\n",
    "        cluster_to_poems[cid].append(poem_id)\n",
    "\n",
    "# Compute cluster sizes by number of poems\n",
    "cluster_sizes = {cid: len(pids) for cid, pids in cluster_to_poems.items()}\n",
    "\n",
    "# Get five largest clusters\n",
    "top_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Write output\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for cid, size in top_clusters:\n",
    "        f.write(f\"CLUSTER ID={cid}, poems={size}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for poem_id in cluster_to_poems[cid]:\n",
    "            # Sort verses in original order\n",
    "            verses_sorted = [v for _, v in sorted(poem_to_verses[poem_id], key=lambda x: x[0])]\n",
    "            f.write(f\"Poem ID: {poem_id}\\n\")\n",
    "            for verse_text in verses_sorted:\n",
    "                # mark verse with asterisk if it's in this cluster\n",
    "                rows = df[(df['idoriginal_poem'] == poem_id) & (df['text'] == verse_text)]\n",
    "                prefix = \"* \" if (rows['poem_cluster_id'] == cid).any() else \"  \"\n",
    "                f.write(f\"{prefix}{verse_text}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Five largest clusters saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427e076-d1ec-4420-9e45-cf76a4467eb9",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e7dc4-ac21-46a9-b669-9da37e196df5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Verse level clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "202cb092-1fd2-4525-9209-0d455630b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: full_orthographic_results/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_306607/1585915595.py:245: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"concatenated.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verses: 1,537,740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing: 100%|██████████| 1537740/1537740 [00:22<00:00, 67545.43it/s] \n",
      "MinHash: 100%|██████████| 769/769 [03:38<00:00,  3.52it/s]\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold selection plot saved to: full_orthographic_results/threshold_selection.png\n",
      "Threshold: 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 154/154 [14:12<00:00,  5.53s/it]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered data saved to: full_orthographic_results/clustered_ultrafast.csv\n",
      "Metrics saved to: full_orthographic_results/clustering_metrics.csv\n",
      "\n",
      "======================================================================\n",
      "CLUSTERING COMPLETE\n",
      "======================================================================\n",
      "Time: 19.1 min\n",
      "Clusters (multi-member): 58,639\n",
      "Max cluster size: 1083656\n",
      "All results saved to: full_orthographic_results/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "import multiprocessing as mp\n",
    "import hashlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class FastUnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            self.parent[px] = py\n",
    "        elif self.rank[px] > self.rank[py]:\n",
    "            self.parent[py] = px\n",
    "        else:\n",
    "            self.parent[py] = px\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        return np.array([self.find(i) for i in range(len(self.parent))], dtype=np.int32)\n",
    "\n",
    "def get_ngrams_vectorized(text, n=4):\n",
    "    if not text or len(text) < n:\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "def compute_minhash_chunk(args):\n",
    "    texts, start_idx, n_gram_size, num_perm, seed = args\n",
    "    np.random.seed(seed)\n",
    "    minhashes = []\n",
    "    for text in texts:\n",
    "        ngrams = get_ngrams_vectorized(text, n_gram_size)\n",
    "        m = MinHash(num_perm=num_perm, seed=seed)\n",
    "        if ngrams:\n",
    "            for ngram in ngrams:\n",
    "                m.update(ngram.encode('utf8'))\n",
    "        minhashes.append(m)\n",
    "    return minhashes\n",
    "\n",
    "def compute_minhash_parallel(texts, n_gram_size=3, num_perm=128, n_cores=None):\n",
    "    if n_cores is None:\n",
    "        n_cores = min(mp.cpu_count(), 16)\n",
    "    chunk_size = max(100, min(2000, len(texts) // (n_cores * 2)))\n",
    "    chunks = [(texts[i:i+chunk_size], i, n_gram_size, num_perm, 42) \n",
    "              for i in range(0, len(texts), chunk_size)]\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(compute_minhash_chunk, chunks),\n",
    "                          total=len(chunks), desc=f\"MinHash (n={n_gram_size})\", leave=False))\n",
    "    minhashes = [mh for chunk_mhs in results for mh in chunk_mhs]\n",
    "    return minhashes\n",
    "\n",
    "def fast_hash(data):\n",
    "    return int(hashlib.md5(data).hexdigest()[:16], 16)\n",
    "\n",
    "def find_exact_duplicates_fast(texts):\n",
    "    text_hashes = {}\n",
    "    for i, text in enumerate(tqdm(texts, desc=\"Hashing\", disable=len(texts)<100000, leave=False)):\n",
    "        normalized = str(text).strip().lower()\n",
    "        if not normalized:\n",
    "            continue\n",
    "        text_hash = fast_hash(normalized.encode('utf-8'))\n",
    "        text_hashes.setdefault(text_hash, []).append(i)\n",
    "    duplicate_groups = [indices for indices in text_hashes.values() if len(indices) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        if n_from_dataset > 0:\n",
    "            sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "            sample_indices.extend(sampled)\n",
    "    \n",
    "    return sorted(sample_indices)\n",
    "\n",
    "def compute_cluster_cohesion(minhashes, cluster_labels):\n",
    "    \"\"\"\n",
    "    Compute average within-cluster similarity (cohesion).\n",
    "    Higher is better - indicates tight clusters.\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cohesions = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "        if len(cluster_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Sample pairs if cluster is too large\n",
    "        if len(cluster_indices) > 50:\n",
    "            sampled_indices = np.random.choice(cluster_indices, 50, replace=False)\n",
    "        else:\n",
    "            sampled_indices = cluster_indices\n",
    "        \n",
    "        # Compute average pairwise similarity\n",
    "        sims = []\n",
    "        for i in range(len(sampled_indices)):\n",
    "            for j in range(i+1, len(sampled_indices)):\n",
    "                sim = minhashes[sampled_indices[i]].jaccard(minhashes[sampled_indices[j]])\n",
    "                sims.append(sim)\n",
    "        \n",
    "        if sims:\n",
    "            cohesions.append(np.mean(sims))\n",
    "    \n",
    "    return np.mean(cohesions) if cohesions else 0.0\n",
    "\n",
    "def compute_cluster_separation(minhashes, cluster_labels, n_samples=500):\n",
    "    \"\"\"\n",
    "    Compute average between-cluster dissimilarity (separation).\n",
    "    Higher is better - indicates well-separated clusters.\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 1.0  # Perfect separation if only one cluster\n",
    "    \n",
    "    # Sample pairs from different clusters\n",
    "    separations = []\n",
    "    for _ in range(n_samples):\n",
    "        # Sample two different clusters\n",
    "        c1, c2 = np.random.choice(unique_clusters, 2, replace=False)\n",
    "        \n",
    "        # Sample one point from each cluster\n",
    "        idx1 = np.random.choice(np.where(cluster_labels == c1)[0])\n",
    "        idx2 = np.random.choice(np.where(cluster_labels == c2)[0])\n",
    "        \n",
    "        # Compute dissimilarity (1 - similarity)\n",
    "        sim = minhashes[idx1].jaccard(minhashes[idx2])\n",
    "        separations.append(1 - sim)\n",
    "    \n",
    "    return np.mean(separations) if separations else 0.0\n",
    "\n",
    "def compute_silhouette_approximation(minhashes, cluster_labels, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Approximate silhouette score using sampling.\n",
    "    Ranges from -1 to 1, higher is better.\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    if len(unique_clusters) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Sample points for efficiency\n",
    "    n_total = len(cluster_labels)\n",
    "    if n_total > n_samples:\n",
    "        sample_indices = np.random.choice(n_total, n_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = np.arange(n_total)\n",
    "    \n",
    "    silhouettes = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        cluster_id = cluster_labels[idx]\n",
    "        \n",
    "        # Compute average distance to points in same cluster (a)\n",
    "        same_cluster = np.where(cluster_labels == cluster_id)[0]\n",
    "        same_cluster = same_cluster[same_cluster != idx]\n",
    "        \n",
    "        if len(same_cluster) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sample if too many points\n",
    "        if len(same_cluster) > 20:\n",
    "            same_cluster = np.random.choice(same_cluster, 20, replace=False)\n",
    "        \n",
    "        a = np.mean([1 - minhashes[idx].jaccard(minhashes[j]) \n",
    "                    for j in same_cluster])\n",
    "        \n",
    "        # Compute average distance to nearest other cluster (b)\n",
    "        other_clusters = unique_clusters[unique_clusters != cluster_id]\n",
    "        if len(other_clusters) == 0:\n",
    "            continue\n",
    "        \n",
    "        min_b = float('inf')\n",
    "        for other_id in other_clusters:\n",
    "            other_cluster = np.where(cluster_labels == other_id)[0]\n",
    "            \n",
    "            # Sample if too many points\n",
    "            if len(other_cluster) > 20:\n",
    "                other_cluster = np.random.choice(other_cluster, 20, replace=False)\n",
    "            \n",
    "            b = np.mean([1 - minhashes[idx].jaccard(minhashes[j]) \n",
    "                        for j in other_cluster])\n",
    "            min_b = min(min_b, b)\n",
    "        \n",
    "        # Silhouette coefficient\n",
    "        s = (min_b - a) / max(a, min_b) if max(a, min_b) > 0 else 0\n",
    "        silhouettes.append(s)\n",
    "    \n",
    "    return np.mean(silhouettes) if silhouettes else 0.0\n",
    "\n",
    "def evaluate_single_config(args):\n",
    "    \"\"\"\n",
    "    Evaluate a single (shingle_size, threshold) configuration.\n",
    "    Returns metrics for quality assessment.\n",
    "    \"\"\"\n",
    "    shingle_size, threshold, texts, sample_indices, duplicate_groups = args\n",
    "    \n",
    "    try:\n",
    "        # Compute MinHashes for this shingle size\n",
    "        sample_texts = [texts[i] for i in sample_indices]\n",
    "        minhashes_sample = compute_minhash_parallel(\n",
    "            sample_texts, \n",
    "            n_gram_size=shingle_size, \n",
    "            num_perm=128\n",
    "        )\n",
    "        \n",
    "        # Build LSH Forest\n",
    "        forest = MinHashLSHForest(num_perm=128)\n",
    "        for idx, mh in enumerate(minhashes_sample):\n",
    "            forest.add(str(idx), mh)\n",
    "        forest.index()\n",
    "        \n",
    "        # Cluster using Union-Find\n",
    "        n_sample = len(sample_indices)\n",
    "        uf = FastUnionFind(n_sample)\n",
    "        \n",
    "        # Add exact duplicates from the sample\n",
    "        sample_set = set(sample_indices)\n",
    "        for group in duplicate_groups:\n",
    "            sample_group = [sample_indices.index(g) for g in group if g in sample_set]\n",
    "            if len(sample_group) > 1:\n",
    "                for i in range(1, len(sample_group)):\n",
    "                    uf.union(sample_group[0], sample_group[i])\n",
    "        \n",
    "        # Query and merge similar pairs\n",
    "        top_k = 50\n",
    "        merges = 0\n",
    "        for idx in range(n_sample):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes_sample[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                sim = minhashes_sample[idx].jaccard(minhashes_sample[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        merges += 1\n",
    "        \n",
    "        cluster_labels = uf.get_clusters()\n",
    "        unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "        \n",
    "        # Compute quality metrics\n",
    "        n_clusters = len(unique_clusters)\n",
    "        n_multi = np.sum(cluster_sizes > 1)\n",
    "        n_singleton = np.sum(cluster_sizes == 1)\n",
    "        avg_size = float(cluster_sizes.mean())\n",
    "        max_size = int(cluster_sizes.max())\n",
    "        \n",
    "        # Advanced metrics\n",
    "        cohesion = compute_cluster_cohesion(minhashes_sample, cluster_labels)\n",
    "        separation = compute_cluster_separation(minhashes_sample, cluster_labels)\n",
    "        silhouette = compute_silhouette_approximation(minhashes_sample, cluster_labels)\n",
    "        \n",
    "        return {\n",
    "            'shingle_size': shingle_size,\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_multi_clusters': n_multi,\n",
    "            'n_singletons': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'separation': separation,\n",
    "            'silhouette': silhouette,\n",
    "            'merges': merges\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at shingle={shingle_size}, threshold={threshold:.2f}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_parameters(texts, df, duplicate_groups, \n",
    "                          shingle_sizes=[2, 3, 4, 5],\n",
    "                          threshold_range=(0.3, 0.9, 7),\n",
    "                          n_sample=15000,\n",
    "                          results_folder=\"full_orthographic_results\",\n",
    "                          max_workers=8):\n",
    "    \"\"\"\n",
    "    Perform 2D grid search over shingle size and threshold.\n",
    "    \n",
    "    Selection Metric: Combined Quality Score\n",
    "    - Silhouette score (40%): Balance between cohesion and separation\n",
    "    - Cohesion (30%): Within-cluster similarity\n",
    "    - Separation (20%): Between-cluster dissimilarity\n",
    "    - Cluster balance (10%): Penalize too many singletons\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2D GRID SEARCH: SHINGLE SIZE × THRESHOLD\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sample for evaluation\n",
    "    sample_indices = stratified_sample(df, n_sample)\n",
    "    print(f\"Sample size: {len(sample_indices):,}\")\n",
    "    \n",
    "    # Generate threshold grid\n",
    "    thresholds = np.linspace(threshold_range[0], threshold_range[1], threshold_range[2])\n",
    "    \n",
    "    print(f\"\\nParameter grid:\")\n",
    "    print(f\"  Shingle sizes: {shingle_sizes}\")\n",
    "    print(f\"  Thresholds: {len(thresholds)} values from {thresholds[0]:.2f} to {thresholds[-1]:.2f}\")\n",
    "    print(f\"  Total combinations: {len(shingle_sizes) * len(thresholds)}\")\n",
    "    \n",
    "    # Prepare arguments for parallel evaluation\n",
    "    args_list = []\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            args_list.append((shingle_size, threshold, texts, sample_indices, duplicate_groups))\n",
    "    \n",
    "    # Parallel evaluation\n",
    "    print(f\"\\nRunning grid search with {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(evaluate_single_config, args): args for args in args_list}\n",
    "        \n",
    "        with tqdm(total=len(futures), desc=\"Grid search\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"✓ Grid search complete in {time.time()-start_time:.1f}s\")\n",
    "    print(f\"  Valid results: {len(results)} / {len(args_list)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Compute combined quality score\n",
    "    print(\"\\nComputing quality scores...\")\n",
    "    \n",
    "    # Normalize metrics to 0-1 range\n",
    "    def normalize(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val < 1e-10:\n",
    "            return pd.Series(0.5, index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    \n",
    "    silhouette_score = normalize(results_df['silhouette'])\n",
    "    cohesion_score = normalize(results_df['cohesion'])\n",
    "    separation_score = normalize(results_df['separation'])\n",
    "    \n",
    "    # Cluster balance: prefer moderate number of clusters, penalize too many singletons\n",
    "    singleton_ratio = results_df['n_singletons'] / len(sample_indices)\n",
    "    balance_score = 1 - singleton_ratio\n",
    "    balance_score = np.clip(balance_score, 0, 1)\n",
    "    \n",
    "    # Combined score with weights\n",
    "    results_df['quality_score'] = (\n",
    "        silhouette_score * 0.40 +\n",
    "        cohesion_score * 0.30 +\n",
    "        separation_score * 0.20 +\n",
    "        balance_score * 0.10\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    results_csv = os.path.join(results_folder, 'parameter_grid_search_results.csv')\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"✓ Results saved: {results_csv}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    create_grid_search_visualizations(results_df, results_folder)\n",
    "    \n",
    "    # Select best configuration\n",
    "    best_config = results_df.iloc[0]\n",
    "    best_shingle = int(best_config['shingle_size'])\n",
    "    best_threshold = float(best_config['threshold'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 5 CONFIGURATIONS (BY QUALITY SCORE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, (i, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "        print(f\"\\n#{idx}. Shingle size: {int(row['shingle_size'])}, Threshold: {row['threshold']:.3f}\")\n",
    "        print(f\"     Quality score: {row['quality_score']:.3f}\")\n",
    "        print(f\"     Silhouette: {row['silhouette']:.3f}, Cohesion: {row['cohesion']:.3f}, \"\n",
    "              f\"Separation: {row['separation']:.3f}\")\n",
    "        print(f\"     Clusters: {int(row['n_multi_clusters']):,}, Singletons: {int(row['n_singletons']):,}, \"\n",
    "              f\"Avg size: {row['avg_cluster_size']:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 SELECTED CONFIGURATION (HIGHEST QUALITY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Shingle size: {best_shingle}\")\n",
    "    print(f\"Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Quality score: {best_config['quality_score']:.3f}\")\n",
    "    print(f\"  - Silhouette: {best_config['silhouette']:.3f}\")\n",
    "    print(f\"  - Cohesion: {best_config['cohesion']:.3f}\")\n",
    "    print(f\"  - Separation: {best_config['separation']:.3f}\")\n",
    "    print(f\"Multi-member clusters: {int(best_config['n_multi_clusters']):,}\")\n",
    "    print(f\"Singletons: {int(best_config['n_singletons']):,}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return best_shingle, best_threshold, results_df\n",
    "\n",
    "def create_grid_search_visualizations(results_df, results_folder):\n",
    "    \"\"\"Create comprehensive visualizations of the grid search results.\"\"\"\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "    \n",
    "    # 1. Quality score heatmap\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    pivot_quality = results_df.pivot_table(\n",
    "        values='quality_score',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_quality, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax1,\n",
    "               cbar_kws={'label': 'Quality Score'})\n",
    "    ax1.set_ylabel('Threshold', fontweight='bold')\n",
    "    ax1.set_xlabel('Shingle Size', fontweight='bold')\n",
    "    ax1.set_title('Quality Score Heatmap', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Mark best combination\n",
    "    best = results_df.iloc[0]\n",
    "    if best['shingle_size'] in pivot_quality.columns and best['threshold'] in pivot_quality.index:\n",
    "        best_col = list(pivot_quality.columns).index(best['shingle_size'])\n",
    "        best_row = list(pivot_quality.index).index(best['threshold'])\n",
    "        ax1.add_patch(plt.Rectangle((best_col, best_row), 1, 1,\n",
    "                                   fill=False, edgecolor='blue', lw=3))\n",
    "    \n",
    "    # 2. Quality score distribution\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.hist(results_df['quality_score'], bins=20, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(best['quality_score'], color='red', linestyle='--', linewidth=2,\n",
    "               label=f\"Best: {best['quality_score']:.3f}\")\n",
    "    ax2.set_xlabel('Quality Score', fontweight='bold')\n",
    "    ax2.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax2.set_title('Score Distribution', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Silhouette heatmap\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    pivot_silhouette = results_df.pivot_table(\n",
    "        values='silhouette',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_silhouette, annot=True, fmt='.3f', cmap='coolwarm', ax=ax3,\n",
    "               cbar_kws={'label': 'Silhouette'})\n",
    "    ax3.set_ylabel('Threshold', fontweight='bold')\n",
    "    ax3.set_xlabel('Shingle Size', fontweight='bold')\n",
    "    ax3.set_title('Silhouette Score', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 4. Cohesion heatmap\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    pivot_cohesion = results_df.pivot_table(\n",
    "        values='cohesion',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_cohesion, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4,\n",
    "               cbar_kws={'label': 'Cohesion'})\n",
    "    ax4.set_ylabel('Threshold', fontweight='bold')\n",
    "    ax4.set_xlabel('Shingle Size', fontweight='bold')\n",
    "    ax4.set_title('Within-Cluster Cohesion', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 5. Separation heatmap\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    pivot_separation = results_df.pivot_table(\n",
    "        values='separation',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_separation, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax5,\n",
    "               cbar_kws={'label': 'Separation'})\n",
    "    ax5.set_ylabel('Threshold', fontweight='bold')\n",
    "    ax5.set_xlabel('Shingle Size', fontweight='bold')\n",
    "    ax5.set_title('Between-Cluster Separation', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 6. Quality vs threshold for different shingle sizes\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "    for shingle in sorted(results_df['shingle_size'].unique()):\n",
    "        data = results_df[results_df['shingle_size'] == shingle]\n",
    "        ax6.plot(data['threshold'], data['quality_score'], 'o-',\n",
    "                label=f'n={int(shingle)}', linewidth=2, markersize=5, alpha=0.7)\n",
    "    ax6.axhline(best['quality_score'], color='red', linestyle='--',\n",
    "               linewidth=1, alpha=0.5, label=f'Best: {best[\"quality_score\"]:.3f}')\n",
    "    ax6.set_xlabel('Threshold', fontweight='bold')\n",
    "    ax6.set_ylabel('Quality Score', fontweight='bold')\n",
    "    ax6.set_title('Quality vs Threshold', fontweight='bold')\n",
    "    ax6.legend(fontsize=8)\n",
    "    ax6.grid(alpha=0.3)\n",
    "    \n",
    "    # 7. Number of clusters heatmap\n",
    "    ax7 = fig.add_subplot(gs[2, 1])\n",
    "    pivot_clusters = results_df.pivot_table(\n",
    "        values='n_multi_clusters',\n",
    "        index='threshold',\n",
    "        columns='shingle_size',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    sns.heatmap(pivot_clusters, annot=True, fmt='.0f', cmap='viridis', ax=ax7,\n",
    "               cbar_kws={'label': 'Multi-Clusters'})\n",
    "    ax7.set_ylabel('Threshold', fontweight='bold')\n",
    "    ax7.set_xlabel('Shingle Size', fontweight='bold')\n",
    "    ax7.set_title('Multi-Member Clusters', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # 8. Silhouette vs Cohesion scatter\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    scatter = ax8.scatter(results_df['cohesion'], results_df['silhouette'],\n",
    "                         c=results_df['quality_score'], cmap='RdYlGn',\n",
    "                         s=80, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    ax8.scatter(best['cohesion'], best['silhouette'],\n",
    "               color='red', s=250, marker='*', edgecolors='black', linewidth=2,\n",
    "               label='Best', zorder=10)\n",
    "    ax8.set_xlabel('Cohesion', fontweight='bold')\n",
    "    ax8.set_ylabel('Silhouette', fontweight='bold')\n",
    "    ax8.set_title('Silhouette vs Cohesion', fontweight='bold')\n",
    "    plt.colorbar(scatter, ax=ax8, label='Quality Score')\n",
    "    ax8.legend()\n",
    "    ax8.grid(alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f'2D Grid Search: Shingle Size × Threshold (n={len(results_df)} configurations)',\n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_folder, 'grid_search_comprehensive.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Visualization saved: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def cluster_with_lsh_forest(minhashes, duplicate_groups, threshold, top_k=100):\n",
    "    n_docs = len(minhashes)\n",
    "    uf = FastUnionFind(n_docs)\n",
    "    \n",
    "    exact_merges = 0\n",
    "    for group in duplicate_groups:\n",
    "        for i in range(1, len(group)):\n",
    "            if uf.union(group[0], group[i]):\n",
    "                exact_merges += 1\n",
    "    \n",
    "    forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "    for idx, mh in enumerate(tqdm(minhashes, desc=\"Indexing\", leave=False)):\n",
    "        forest.add(str(idx), mh)\n",
    "    forest.index()\n",
    "    \n",
    "    lsh_merges = 0\n",
    "    verified_pairs = 0\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    for start_idx in tqdm(range(0, n_docs, chunk_size), desc=\"Clustering\"):\n",
    "        end_idx = min(start_idx + chunk_size, n_docs)\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                verified_pairs += 1\n",
    "                sim = minhashes[idx].jaccard(minhashes[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        lsh_merges += 1\n",
    "    \n",
    "    cluster_labels = uf.get_clusters()\n",
    "    unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "    \n",
    "    return cluster_labels, {\n",
    "        'n_clusters': len(unique_clusters),\n",
    "        'n_multi_clusters': np.sum(cluster_sizes > 1),\n",
    "        'n_singletons': np.sum(cluster_sizes == 1),\n",
    "        'avg_cluster_size': float(cluster_sizes.mean()),\n",
    "        'max_cluster_size': int(cluster_sizes.max()),\n",
    "        'exact_merges': exact_merges,\n",
    "        'lsh_merges': lsh_merges,\n",
    "        'threshold': threshold,\n",
    "        'verified_pairs': verified_pairs\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Create results folder\n",
    "    results_folder = \"full_orthographic_results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    print(f\"Results will be saved to: {results_folder}/\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(\"concatenated.csv\")\n",
    "    df = df[df['source_dataset'].isin(['rhoby', 'dbbe', 'phi', 'papyri'])]\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "    df['verse'] = df['verse'].apply(preprocess_text)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[df['verse'].str.strip().str.lower() != 'nan']\n",
    "    texts = df['verse'].fillna('').astype(str).tolist()\n",
    "    \n",
    "    print(f\"Verses: {len(texts):,}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find exact duplicates (fast, independent of parameters)\n",
    "    duplicate_groups = find_exact_duplicates_fast(texts)\n",
    "    t1 = time.time()\n",
    "    print(f\"✓ Found {len(duplicate_groups):,} exact duplicate groups in {t1-start_time:.1f}s\")\n",
    "    \n",
    "    # 2D Grid search for optimal shingle size and threshold\n",
    "    best_shingle, best_threshold, grid_results = grid_search_parameters(\n",
    "        texts, df, duplicate_groups,\n",
    "        shingle_sizes=[2, 3, 4, 5],\n",
    "        threshold_range=(0.3, 0.85, 7),\n",
    "        n_sample=15000,\n",
    "        results_folder=results_folder,\n",
    "        max_workers=8\n",
    "    )\n",
    "    t2 = time.time()\n",
    "    \n",
    "    # Compute MinHashes with optimal shingle size\n",
    "    print(f\"\\nComputing MinHashes with optimal shingle size={best_shingle}...\")\n",
    "    minhashes = compute_minhash_parallel(texts, n_gram_size=best_shingle, num_perm=128)\n",
    "    t3 = time.time()\n",
    "    \n",
    "    # Cluster with optimal parameters\n",
    "    print(f\"\\nClustering with threshold={best_threshold:.3f}...\")\n",
    "    cluster_labels, metrics = cluster_with_lsh_forest(\n",
    "        minhashes, duplicate_groups, best_threshold, top_k=100\n",
    "    )\n",
    "    t4 = time.time()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Save results\n",
    "    df['cluster_id'] = cluster_labels\n",
    "    output_csv = os.path.join(results_folder, \"clustered_optimized.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✓ Clustered data saved: {output_csv}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics.update({\n",
    "        'total_time_minutes': total_time / 60,\n",
    "        'best_shingle_size': best_shingle,\n",
    "        'best_threshold': best_threshold,\n",
    "        'grid_search_time_seconds': t2 - t1,\n",
    "        'minhash_time_seconds': t3 - t2,\n",
    "        'clustering_time_seconds': t4 - t3\n",
    "    })\n",
    "    metrics_csv = os.path.join(results_folder, \"clustering_metrics.csv\")\n",
    "    pd.DataFrame([metrics]).to_csv(metrics_csv, index=False)\n",
    "    print(f\"✓ Metrics saved: {metrics_csv}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CLUSTERING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total time: {total_time/60:.1f} min\")\n",
    "    print(f\"Optimal shingle size: {best_shingle}\")\n",
    "    print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Multi-member clusters: {metrics['n_multi_clusters']:,}\")\n",
    "    print(f\"Singletons: {metrics['n_singletons']:,}\")\n",
    "    print(f\"Max cluster size: {metrics['max_cluster_size']}\")\n",
    "    print(f\"All results saved to: {results_folder}/\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c311c-1ebe-490b-8b54-cf41ebef5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('full_orthographic_results/parameter_grid_search_results.csv')\n",
    "\n",
    "# Pivot the data\n",
    "pivot = df.pivot(index='threshold', columns='shingle_size', values='quality_score')\n",
    "\n",
    "# Ensure index is float and rounded\n",
    "pivot.index = pd.Index(np.round(pivot.index.astype(float), 2))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    pivot, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='viridis', \n",
    "    cbar_kws={'label': 'Quality Score'},\n",
    "    annot_kws={\"size\": 22}  # increase annotation font size\n",
    ")\n",
    "\n",
    "plt.title('Grid Search: Shingle Size × Threshold', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Shingle Size', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Threshold', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Round y-axis tick labels and increase font size\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "plt.gca().set_yticklabels([f\"{tick:.2f}\" for tick in pivot.index])\n",
    "%matplotlib inline\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_orthographic_results/parameter_grid_search.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de12244-0969-4361-9481-04187a254973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Poem level clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11f6ca9-0e4f-4cf7-a5b2-1af7f874fb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: full_orthographic_results/clustered_optimized.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3163273/552467930.py:1183: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 1,537,740 verses\n",
      "✓ Unique poems: 164,665\n",
      "✓ Unique verse clusters: 1,261,001\n",
      "✓ Source datasets: ['rhoby' 'dbbe' 'phi' 'papyri']\n",
      "\n",
      "======================================================================\n",
      "STARTING ENHANCED ANALYSIS\n",
      "======================================================================\n",
      "======================================================================\n",
      "ENHANCED POEM-LEVEL THRESHOLD SELECTION\n",
      "======================================================================\n",
      "Minimum shared verses: 2\n",
      "\n",
      "Step 1: Reconstructing poems...\n",
      "  Found 164,665 poems\n",
      "\n",
      "Step 2: Building inverted index...\n",
      "  Found 1,261,001 verse clusters\n",
      "\n",
      "Step 3: Sampling 15,000 poems...\n",
      "  Stratifying by source dataset and poem size...\n",
      "  Sampled 15,000 poems\n",
      "\n",
      "Step 4: Finding candidate pairs in sample...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sample pairs: 100%|██████████| 15000/15000 [00:01<00:00, 10430.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 1,157,993 candidate pairs\n",
      "\n",
      "Step 5: Converting to arrays...\n",
      "\n",
      "Step 6: Computing similarities (filtering pairs with <2 shared verses)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities: 100%|██████████| 1157993/1157993 [00:06<00:00, 189700.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computed 2,329 similarities (after filtering)\n",
      "\n",
      "Step 7: Grid search over thresholds...\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH: TESTING MULTIPLE THRESHOLDS\n",
      "======================================================================\n",
      "Testing 13 thresholds from 0.30 to 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating thresholds: 100%|██████████| 13/13 [00:11<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Grid search results saved to: poem_threshold_grid_search.csv\n",
      "\n",
      "======================================================================\n",
      "THRESHOLD SELECTION BASED ON QUALITY METRICS\n",
      "======================================================================\n",
      "Selected Threshold:   0.8500\n",
      "Quality Score:        0.8604\n",
      "Silhouette:           1.0000\n",
      "Cohesion:             1.0000\n",
      "Separation:           0.9996\n",
      "Clusters:             14,991\n",
      "Singletons:           14,984\n",
      "Avg Cluster Size:     1.00\n",
      "Pairs Above Threshold: 11 (0.47%)\n",
      "\n",
      "Step 9: Creating visualizations...\n",
      "✓ Grid search visualization saved\n",
      "Summary saved to: poem_enhanced_threshold_summary.csv\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TOTAL TIME: 0.42 minutes\n",
      "RECOMMENDED THRESHOLD: 0.8500\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TOP 5 CONFIGURATIONS\n",
      "======================================================================\n",
      "\n",
      "12. Threshold: 0.850\n",
      "   Quality: 0.860, Silhouette: 1.000\n",
      "   Clusters: 14,991, Avg Size: 1.00\n",
      "\n",
      "4. Threshold: 0.450\n",
      "   Quality: 0.481, Silhouette: 0.514\n",
      "   Clusters: 14,952, Avg Size: 1.00\n",
      "\n",
      "11. Threshold: 0.800\n",
      "   Quality: 0.472, Silhouette: 0.000\n",
      "   Clusters: 14,990, Avg Size: 1.00\n",
      "\n",
      "6. Threshold: 0.550\n",
      "   Quality: 0.395, Silhouette: 0.278\n",
      "   Clusters: 14,969, Avg Size: 1.00\n",
      "\n",
      "3. Threshold: 0.400\n",
      "   Quality: 0.383, Silhouette: 0.341\n",
      "   Clusters: 14,936, Avg Size: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numba import njit\n",
    "import time\n",
    "\n",
    "\n",
    "INPUT_CSV = \"full_orthographic_results/clustered_optimized.csv\"\n",
    "\n",
    "SAMPLE_SIZE = 15000  \n",
    "RANDOM_SEED = 42\n",
    "MIN_SHARED_VERSES = 2\n",
    "\n",
    "# Grid search parameters for testing multiple thresholds\n",
    "THRESHOLD_RANGE = (0.3, 0.9, 13)  # (min, max, n_points)\n",
    "\n",
    "\n",
    "class EnhancedPoemThresholdSelector:\n",
    "    \"\"\"\n",
    "    Analyze poem-level similarity distribution and select threshold\n",
    "    based on quality metrics (silhouette, cohesion, separation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_size: int = 10000, random_seed: int = 42, min_shared_verses: int = 2):\n",
    "        self.sample_size = sample_size\n",
    "        self.random_seed = random_seed\n",
    "        self.min_shared_verses = min_shared_verses\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct_poems_vectorized(df):\n",
    "        \"\"\"Vectorized poem reconstruction\"\"\"\n",
    "        valid_mask = df['cluster_id'] != -1\n",
    "        df_valid = df[valid_mask]\n",
    "        grouped = df_valid.groupby('idoriginal_poem')['cluster_id'].apply(set)\n",
    "        return grouped.to_dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_inverted_index_fast(poem_to_clusters):\n",
    "        \"\"\"Fast inverted index\"\"\"\n",
    "        cluster_to_poems = defaultdict(set)\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            for c in clusters:\n",
    "                cluster_to_poems[c].add(poem_id)\n",
    "        return cluster_to_poems\n",
    "    \n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def jaccard_numba(a_arr, b_arr):\n",
    "        \"\"\"Ultra-fast Jaccard using Numba\"\"\"\n",
    "        intersection = 0\n",
    "        a_set = set(a_arr)\n",
    "        b_set = set(b_arr)\n",
    "        \n",
    "        for item in a_set:\n",
    "            if item in b_set:\n",
    "                intersection += 1\n",
    "        \n",
    "        union = len(a_set) + len(b_set) - intersection\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n",
    "    \n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def count_shared_verses(a_arr, b_arr):\n",
    "        \"\"\"Count number of shared verse clusters\"\"\"\n",
    "        shared = 0\n",
    "        a_set = set(a_arr)\n",
    "        b_set = set(b_arr)\n",
    "        \n",
    "        for item in a_set:\n",
    "            if item in b_set:\n",
    "                shared += 1\n",
    "        \n",
    "        return shared\n",
    "    \n",
    "    def stratified_sample_poems(self, df, poem_to_clusters):\n",
    "        \"\"\"\n",
    "        Create a stratified sample of poems based on:\n",
    "        1. Source dataset distribution (if available)\n",
    "        2. Poem size (number of verse clusters)\n",
    "        \"\"\"\n",
    "        has_source = 'source_dataset' in df.columns\n",
    "        \n",
    "        if has_source:\n",
    "            poem_to_source = df.groupby('idoriginal_poem')['source_dataset'].first().to_dict()\n",
    "        \n",
    "        poem_metadata = []\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            metadata = {\n",
    "                'poem_id': poem_id,\n",
    "                'n_clusters': len(clusters)\n",
    "            }\n",
    "            \n",
    "            if has_source:\n",
    "                metadata['source'] = poem_to_source.get(poem_id, 'unknown')\n",
    "            \n",
    "            poem_metadata.append(metadata)\n",
    "        \n",
    "        poem_df = pd.DataFrame(poem_metadata)\n",
    "        \n",
    "        # Define size bins\n",
    "        poem_df['size_bin'] = pd.cut(poem_df['n_clusters'], \n",
    "                                      bins=[0, 5, 10, 20, 50, np.inf],\n",
    "                                      labels=['tiny', 'small', 'medium', 'large', 'huge'])\n",
    "        \n",
    "        # Stratified sampling\n",
    "        sample_indices = []\n",
    "        \n",
    "        if has_source:\n",
    "            print(\"  Stratifying by source dataset and poem size...\")\n",
    "            for (source, size_bin), group in poem_df.groupby(['source', 'size_bin']):\n",
    "                n_in_group = len(group)\n",
    "                proportion = n_in_group / len(poem_df)\n",
    "                n_sample = max(1, int(self.sample_size * proportion))\n",
    "                n_sample = min(n_sample, n_in_group)\n",
    "                \n",
    "                sampled = group.sample(n=n_sample, random_state=self.random_seed)\n",
    "                sample_indices.extend(sampled['poem_id'].tolist())\n",
    "        else:\n",
    "            print(\"  Stratifying by poem size only (no source_dataset column)...\")\n",
    "            for size_bin, group in poem_df.groupby('size_bin'):\n",
    "                n_in_group = len(group)\n",
    "                proportion = n_in_group / len(poem_df)\n",
    "                n_sample = max(1, int(self.sample_size * proportion))\n",
    "                n_sample = min(n_sample, n_in_group)\n",
    "                \n",
    "                sampled = group.sample(n=n_sample, random_state=self.random_seed)\n",
    "                sample_indices.extend(sampled['poem_id'].tolist())\n",
    "        \n",
    "        # If we haven't reached sample_size, add random poems\n",
    "        if len(sample_indices) < self.sample_size:\n",
    "            remaining = self.sample_size - len(sample_indices)\n",
    "            available = set(poem_df['poem_id']) - set(sample_indices)\n",
    "            if available:\n",
    "                additional = np.random.choice(list(available), \n",
    "                                             size=min(remaining, len(available)), \n",
    "                                             replace=False)\n",
    "                sample_indices.extend(additional)\n",
    "        \n",
    "        return sample_indices[:self.sample_size]\n",
    "    \n",
    "    def find_candidate_pairs_for_sample(self, sample_poems, poem_to_clusters, cluster_to_poems):\n",
    "        \"\"\"\n",
    "        Find candidate pairs within the sample using inverted index\n",
    "        \"\"\"\n",
    "        sample_set = set(sample_poems)\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        # Use inverted index to find pairs\n",
    "        for poem_id in tqdm(sample_poems, desc=\"Building sample pairs\"):\n",
    "            clusters = poem_to_clusters[poem_id]\n",
    "            \n",
    "            # Find all poems that share at least one cluster\n",
    "            potential_matches = set()\n",
    "            for cluster_id in clusters:\n",
    "                potential_matches.update(cluster_to_poems[cluster_id])\n",
    "            \n",
    "            # Keep only those in sample\n",
    "            potential_matches = potential_matches & sample_set\n",
    "            \n",
    "            # Create pairs\n",
    "            for other_poem in potential_matches:\n",
    "                if other_poem > poem_id:  # Avoid duplicates\n",
    "                    candidate_pairs.add((poem_id, other_poem))\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def compute_sample_similarities(self, candidate_pairs, poem_to_array):\n",
    "        \"\"\"\n",
    "        Compute Jaccard similarities for all candidate pairs\n",
    "        \"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for p1, p2 in tqdm(candidate_pairs, desc=\"Computing similarities\"):\n",
    "            # Count shared verses first\n",
    "            shared = self.count_shared_verses(poem_to_array[p1], poem_to_array[p2])\n",
    "            \n",
    "            # Only compute Jaccard if minimum shared verses met\n",
    "            if shared >= self.min_shared_verses:\n",
    "                sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "                similarities.append({\n",
    "                    'poem1': p1,\n",
    "                    'poem2': p2,\n",
    "                    'similarity': sim,\n",
    "                    'shared_verses': shared\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(similarities)\n",
    "    \n",
    "    class FastUnionFind:\n",
    "        \"\"\"Union-Find with path compression for efficient clustering\"\"\"\n",
    "        __slots__ = ['parent', 'rank']\n",
    "        \n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "        \n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "        \n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return False\n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "            return True\n",
    "        \n",
    "        def get_clusters(self):\n",
    "            clusters = defaultdict(set)\n",
    "            for elem in self.parent.keys():\n",
    "                clusters[self.find(elem)].add(elem)\n",
    "            return dict(clusters)\n",
    "    \n",
    "    def cluster_at_threshold(self, similarities_df, threshold, sample_poems, poem_to_array):\n",
    "        \"\"\"\n",
    "        Perform clustering at a specific threshold using Union-Find\n",
    "        \"\"\"\n",
    "        # Filter similarities by threshold\n",
    "        valid_pairs = similarities_df[similarities_df['similarity'] >= threshold]\n",
    "        \n",
    "        # Cluster using Union-Find\n",
    "        uf = self.FastUnionFind(sample_poems)\n",
    "        \n",
    "        for _, row in valid_pairs.iterrows():\n",
    "            uf.union(row['poem1'], row['poem2'])\n",
    "        \n",
    "        # Get cluster assignments\n",
    "        poem_clusters = uf.get_clusters()\n",
    "        cluster_assignments = {}\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            for poem in poems:\n",
    "                cluster_assignments[poem] = cluster_id\n",
    "        \n",
    "        return cluster_assignments\n",
    "    \n",
    "    def compute_cluster_cohesion(self, poem_to_array, cluster_assignments):\n",
    "        \"\"\"\n",
    "        Compute average within-cluster similarity (cohesion).\n",
    "        Higher is better - indicates tight clusters.\n",
    "        \"\"\"\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        cohesions = []\n",
    "        \n",
    "        # Group poems by cluster\n",
    "        clusters = defaultdict(list)\n",
    "        for poem_id in poem_ids:\n",
    "            cluster_id = cluster_assignments.get(poem_id)\n",
    "            if cluster_id is not None:\n",
    "                clusters[cluster_id].append(poem_id)\n",
    "        \n",
    "        # Compute cohesion for each cluster\n",
    "        for cluster_id, cluster_poems in clusters.items():\n",
    "            if len(cluster_poems) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Sample pairs if cluster is too large\n",
    "            if len(cluster_poems) > 30:\n",
    "                sampled = np.random.choice(cluster_poems, 30, replace=False)\n",
    "            else:\n",
    "                sampled = cluster_poems\n",
    "            \n",
    "            # Compute average pairwise similarity\n",
    "            sims = []\n",
    "            for i in range(len(sampled)):\n",
    "                for j in range(i+1, len(sampled)):\n",
    "                    sim = self.jaccard_numba(poem_to_array[sampled[i]], poem_to_array[sampled[j]])\n",
    "                    sims.append(sim)\n",
    "            \n",
    "            if sims:\n",
    "                cohesions.append(np.mean(sims))\n",
    "        \n",
    "        return np.mean(cohesions) if cohesions else 0.0\n",
    "    \n",
    "    def compute_cluster_separation(self, poem_to_array, cluster_assignments, n_samples=300):\n",
    "        \"\"\"\n",
    "        Compute average between-cluster dissimilarity (separation).\n",
    "        Higher is better - indicates well-separated clusters.\n",
    "        \"\"\"\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        unique_clusters = set(cluster_assignments.values())\n",
    "        \n",
    "        if len(unique_clusters) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        separations = []\n",
    "        cluster_to_poems = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            cluster_to_poems[cluster_id].append(poem_id)\n",
    "        \n",
    "        # Sample pairs from different clusters\n",
    "        unique_clusters = list(unique_clusters)\n",
    "        for _ in range(n_samples):\n",
    "            c1, c2 = np.random.choice(unique_clusters, 2, replace=False)\n",
    "            \n",
    "            p1 = np.random.choice(cluster_to_poems[c1])\n",
    "            p2 = np.random.choice(cluster_to_poems[c2])\n",
    "            \n",
    "            sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "            separations.append(1 - sim)\n",
    "        \n",
    "        return np.mean(separations) if separations else 0.0\n",
    "    \n",
    "    def compute_silhouette_approximation(self, poem_to_array, cluster_assignments, n_samples=500):\n",
    "        \"\"\"\n",
    "        Approximate silhouette score using sampling.\n",
    "        Ranges from -1 to 1, higher is better.\n",
    "        \"\"\"\n",
    "        poem_ids = list(poem_to_array.keys())\n",
    "        unique_clusters = set(cluster_assignments.values())\n",
    "        \n",
    "        if len(unique_clusters) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample poems\n",
    "        if len(poem_ids) > n_samples:\n",
    "            sampled_poems = np.random.choice(poem_ids, n_samples, replace=False)\n",
    "        else:\n",
    "            sampled_poems = poem_ids\n",
    "        \n",
    "        silhouettes = []\n",
    "        cluster_to_poems = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            cluster_to_poems[cluster_id].append(poem_id)\n",
    "        \n",
    "        for poem_id in sampled_poems:\n",
    "            cluster_id = cluster_assignments[poem_id]\n",
    "            same_cluster = [p for p in cluster_to_poems[cluster_id] if p != poem_id]\n",
    "            \n",
    "            if len(same_cluster) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Sample if too many\n",
    "            if len(same_cluster) > 15:\n",
    "                same_cluster = np.random.choice(same_cluster, 15, replace=False)\n",
    "            \n",
    "            # Average distance to same cluster\n",
    "            a = np.mean([1 - self.jaccard_numba(poem_to_array[poem_id], poem_to_array[p]) \n",
    "                        for p in same_cluster])\n",
    "            \n",
    "            # Average distance to nearest other cluster\n",
    "            other_clusters = [c for c in unique_clusters if c != cluster_id]\n",
    "            if len(other_clusters) == 0:\n",
    "                continue\n",
    "            \n",
    "            min_b = float('inf')\n",
    "            for other_cluster in other_clusters:\n",
    "                other_poems = cluster_to_poems[other_cluster]\n",
    "                \n",
    "                if len(other_poems) > 15:\n",
    "                    other_poems = np.random.choice(other_poems, 15, replace=False)\n",
    "                \n",
    "                b = np.mean([1 - self.jaccard_numba(poem_to_array[poem_id], poem_to_array[p]) \n",
    "                            for p in other_poems])\n",
    "                min_b = min(min_b, b)\n",
    "            \n",
    "            # Silhouette coefficient\n",
    "            s = (min_b - a) / max(a, min_b) if max(a, min_b) > 0 else 0\n",
    "            silhouettes.append(s)\n",
    "        \n",
    "        return np.mean(silhouettes) if silhouettes else 0.0\n",
    "    \n",
    "    def evaluate_threshold(self, threshold, similarities_df, sample_poems, poem_to_array):\n",
    "        \"\"\"\n",
    "        Evaluate a single threshold using quality metrics\n",
    "        \"\"\"\n",
    "        # Cluster at this threshold\n",
    "        cluster_assignments = self.cluster_at_threshold(\n",
    "            similarities_df, threshold, sample_poems, poem_to_array\n",
    "        )\n",
    "        \n",
    "        # Compute cluster statistics\n",
    "        clusters = defaultdict(list)\n",
    "        for poem_id, cluster_id in cluster_assignments.items():\n",
    "            clusters[cluster_id].append(poem_id)\n",
    "        \n",
    "        n_clusters = len(clusters)\n",
    "        cluster_sizes = [len(poems) for poems in clusters.values()]\n",
    "        n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "        avg_size = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "        max_size = max(cluster_sizes) if cluster_sizes else 0\n",
    "        \n",
    "        # Compute quality metrics\n",
    "        cohesion = self.compute_cluster_cohesion(poem_to_array, cluster_assignments)\n",
    "        separation = self.compute_cluster_separation(poem_to_array, cluster_assignments)\n",
    "        silhouette = self.compute_silhouette_approximation(poem_to_array, cluster_assignments)\n",
    "        \n",
    "        # Count pairs above threshold\n",
    "        n_pairs_above = len(similarities_df[similarities_df['similarity'] >= threshold])\n",
    "        pct_pairs_above = (n_pairs_above / len(similarities_df) * 100) if len(similarities_df) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singletons': n_singletons,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'separation': separation,\n",
    "            'silhouette': silhouette,\n",
    "            'n_pairs_above': n_pairs_above,\n",
    "            'pct_pairs_above': pct_pairs_above\n",
    "        }\n",
    "    \n",
    "    def grid_search_thresholds(self, similarities_df, sample_poems, poem_to_array):\n",
    "        \"\"\"\n",
    "        Test multiple thresholds and select best based on quality metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GRID SEARCH: TESTING MULTIPLE THRESHOLDS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Generate threshold range\n",
    "        thresholds = np.linspace(THRESHOLD_RANGE[0], THRESHOLD_RANGE[1], THRESHOLD_RANGE[2])\n",
    "        print(f\"Testing {len(thresholds)} thresholds from {thresholds[0]:.2f} to {thresholds[-1]:.2f}\")\n",
    "        \n",
    "        results = []\n",
    "        for threshold in tqdm(thresholds, desc=\"Evaluating thresholds\"):\n",
    "            result = self.evaluate_threshold(threshold, similarities_df, sample_poems, poem_to_array)\n",
    "            results.append(result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Compute composite quality score\n",
    "        def normalize(series):\n",
    "            min_val = series.min()\n",
    "            max_val = series.max()\n",
    "            if max_val - min_val < 1e-10:\n",
    "                return pd.Series(0.5, index=series.index)\n",
    "            return (series - min_val) / (max_val - min_val)\n",
    "        \n",
    "        silhouette_score = normalize(results_df['silhouette'])\n",
    "        cohesion_score = normalize(results_df['cohesion'])\n",
    "        separation_score = normalize(results_df['separation'])\n",
    "        \n",
    "        # Penalize too many singletons\n",
    "        singleton_ratio = results_df['n_singletons'] / len(sample_poems)\n",
    "        balance_score = np.clip(1 - singleton_ratio, 0, 1)\n",
    "        \n",
    "        # Composite quality score (same weights as semantic script)\n",
    "        results_df['quality_score'] = (\n",
    "            silhouette_score * 0.40 +\n",
    "            cohesion_score * 0.30 +\n",
    "            separation_score * 0.20 +\n",
    "            balance_score * 0.10\n",
    "        )\n",
    "        \n",
    "        # Sort by quality\n",
    "        results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv('full_orthographic_results/poem_threshold_grid_search.csv', index=False)\n",
    "        print(f\"\\n✓ Grid search results saved to: poem_threshold_grid_search.csv\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_grid_search_analysis(self, results_df, selected_threshold):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualization of grid search results\n",
    "        \"\"\"\n",
    "        sns.set_palette(\"colorblind\")\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "        \n",
    "        thresholds = results_df['threshold'].values\n",
    "        \n",
    "        # 1. Quality score vs threshold\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(thresholds, results_df['quality_score'], 'o-', \n",
    "                linewidth=2, markersize=8, color='#0173B2', label='Quality Score')\n",
    "        ax1.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=2,\n",
    "                   label=f'Selected: {selected_threshold:.3f}')\n",
    "        ax1.set_xlabel('Threshold', fontweight='bold', fontsize=12)\n",
    "        ax1.set_ylabel('Quality Score', fontweight='bold', fontsize=12)\n",
    "        ax1.set_title('Composite Quality Score vs Threshold', fontweight='bold', fontsize=14)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Mark the best point\n",
    "        best_idx = results_df['quality_score'].idxmax()\n",
    "        ax1.scatter(results_df.loc[best_idx, 'threshold'], \n",
    "                   results_df.loc[best_idx, 'quality_score'],\n",
    "                   color='red', s=200, marker='*', edgecolors='black', linewidth=2, zorder=10)\n",
    "        \n",
    "        # 2. Individual metrics\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        ax2.plot(thresholds, results_df['silhouette'], 'o-', linewidth=2, label='Silhouette', color='#029E73')\n",
    "        ax2.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax2.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax2.set_ylabel('Silhouette Score', fontweight='bold')\n",
    "        ax2.set_title('Silhouette Score', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        ax3.plot(thresholds, results_df['cohesion'], 'o-', linewidth=2, label='Cohesion', color='#DE8F05')\n",
    "        ax3.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax3.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax3.set_ylabel('Cohesion', fontweight='bold')\n",
    "        ax3.set_title('Within-Cluster Cohesion', fontweight='bold')\n",
    "        ax3.legend()\n",
    "        ax3.grid(alpha=0.3)\n",
    "        \n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        ax4.plot(thresholds, results_df['separation'], 'o-', linewidth=2, label='Separation', color='#CC78BC')\n",
    "        ax4.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax4.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax4.set_ylabel('Separation', fontweight='bold')\n",
    "        ax4.set_title('Between-Cluster Separation', fontweight='bold')\n",
    "        ax4.legend()\n",
    "        ax4.grid(alpha=0.3)\n",
    "        \n",
    "        # 3. Cluster statistics\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        ax5.plot(thresholds, results_df['n_clusters'], 'o-', linewidth=2, color='#0173B2')\n",
    "        ax5.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax5.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax5.set_ylabel('Number of Clusters', fontweight='bold')\n",
    "        ax5.set_title('Cluster Count', fontweight='bold')\n",
    "        ax5.grid(alpha=0.3)\n",
    "        \n",
    "        ax6 = fig.add_subplot(gs[2, 1])\n",
    "        ax6.plot(thresholds, results_df['avg_cluster_size'], 'o-', linewidth=2, color='#029E73')\n",
    "        ax6.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        ax6.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax6.set_ylabel('Average Cluster Size', fontweight='bold')\n",
    "        ax6.set_title('Average Cluster Size', fontweight='bold')\n",
    "        ax6.grid(alpha=0.3)\n",
    "        \n",
    "        # 4. Multi-metric comparison\n",
    "        ax7 = fig.add_subplot(gs[2, 2])\n",
    "        \n",
    "        # Normalize all metrics for comparison\n",
    "        def norm(series):\n",
    "            return (series - series.min()) / (series.max() - series.min() + 1e-10)\n",
    "        \n",
    "        ax7.plot(thresholds, norm(results_df['silhouette']), 'o-', label='Silhouette', linewidth=2, alpha=0.7)\n",
    "        ax7.plot(thresholds, norm(results_df['cohesion']), 's-', label='Cohesion', linewidth=2, alpha=0.7)\n",
    "        ax7.plot(thresholds, norm(results_df['separation']), '^-', label='Separation', linewidth=2, alpha=0.7)\n",
    "        ax7.plot(thresholds, results_df['quality_score'], 'd-', label='Quality', linewidth=2.5, color='black')\n",
    "        ax7.axvline(selected_threshold, color='#CC0000', linestyle='--', linewidth=2)\n",
    "        ax7.set_xlabel('Threshold', fontweight='bold')\n",
    "        ax7.set_ylabel('Normalized Score', fontweight='bold')\n",
    "        ax7.set_title('All Metrics (Normalized)', fontweight='bold')\n",
    "        ax7.legend()\n",
    "        ax7.grid(alpha=0.3)\n",
    "        \n",
    "        # Overall title\n",
    "        fig.suptitle(f'Orthographic Poem Threshold Grid Search (min_shared={self.min_shared_verses})', \n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "        \n",
    "        plt.savefig('full_orthographic_results/poem_threshold_grid_search_analysis.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Grid search visualization saved\")\n",
    "        plt.close()\n",
    "    \n",
    "    def run_threshold_analysis(self, df):\n",
    "        \"\"\"\n",
    "        Main method to run complete threshold analysis with quality metrics\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"ENHANCED POEM-LEVEL THRESHOLD SELECTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Minimum shared verses: {self.min_shared_verses}\")\n",
    "        \n",
    "        # Step 1: Reconstruct poems\n",
    "        print(\"\\nStep 1: Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"  Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        # Step 2: Build inverted index\n",
    "        print(\"\\nStep 2: Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"  Found {len(cluster_to_poems):,} verse clusters\")\n",
    "        \n",
    "        # Step 3: Sample poems\n",
    "        print(f\"\\nStep 3: Sampling {self.sample_size:,} poems...\")\n",
    "        sample_poems = self.stratified_sample_poems(df, poem_to_clusters)\n",
    "        print(f\"  Sampled {len(sample_poems):,} poems\")\n",
    "        \n",
    "        # Step 4: Find candidate pairs\n",
    "        print(\"\\nStep 4: Finding candidate pairs in sample...\")\n",
    "        candidate_pairs = self.find_candidate_pairs_for_sample(\n",
    "            sample_poems, poem_to_clusters, cluster_to_poems\n",
    "        )\n",
    "        print(f\"  Found {len(candidate_pairs):,} candidate pairs\")\n",
    "        \n",
    "        # Step 5: Convert to arrays for fast computation\n",
    "        print(\"\\nStep 5: Converting to arrays...\")\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(poem_to_clusters[p]), dtype=np.int32)\n",
    "            for p in sample_poems\n",
    "        }\n",
    "        \n",
    "        # Step 6: Compute similarities\n",
    "        print(f\"\\nStep 6: Computing similarities (filtering pairs with <{self.min_shared_verses} shared verses)...\")\n",
    "        similarities_df = self.compute_sample_similarities(candidate_pairs, poem_to_array)\n",
    "        similarities_df.to_csv('full_orthographic_results/poem_similarities_sample.csv', index=False)\n",
    "        print(f\"  Computed {len(similarities_df):,} similarities (after filtering)\")\n",
    "        \n",
    "        # Step 7: Grid search over thresholds\n",
    "        print(\"\\nStep 7: Grid search over thresholds...\")\n",
    "        grid_results = self.grid_search_thresholds(similarities_df, sample_poems, poem_to_array)\n",
    "        \n",
    "        # Step 8: Select best threshold\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"THRESHOLD SELECTION BASED ON QUALITY METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        best_result = grid_results.iloc[0]\n",
    "        threshold = float(best_result['threshold'])\n",
    "        \n",
    "        print(f\"Selected Threshold:   {threshold:.4f}\")\n",
    "        print(f\"Quality Score:        {best_result['quality_score']:.4f}\")\n",
    "        print(f\"Silhouette:           {best_result['silhouette']:.4f}\")\n",
    "        print(f\"Cohesion:             {best_result['cohesion']:.4f}\")\n",
    "        print(f\"Separation:           {best_result['separation']:.4f}\")\n",
    "        print(f\"Clusters:             {int(best_result['n_clusters']):,}\")\n",
    "        print(f\"Singletons:           {int(best_result['n_singletons']):,}\")\n",
    "        print(f\"Avg Cluster Size:     {best_result['avg_cluster_size']:.2f}\")\n",
    "        print(f\"Pairs Above Threshold: {int(best_result['n_pairs_above']):,} ({best_result['pct_pairs_above']:.2f}%)\")\n",
    "        \n",
    "        # Step 9: Create visualizations\n",
    "        print(\"\\nStep 9: Creating visualizations...\")\n",
    "        self.plot_grid_search_analysis(grid_results, threshold)\n",
    "        \n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'selected_threshold': threshold,\n",
    "            'quality_score': best_result['quality_score'],\n",
    "            'silhouette': best_result['silhouette'],\n",
    "            'cohesion': best_result['cohesion'],\n",
    "            'separation': best_result['separation'],\n",
    "            'n_clusters': int(best_result['n_clusters']),\n",
    "            'n_singletons': int(best_result['n_singletons']),\n",
    "            'avg_cluster_size': best_result['avg_cluster_size'],\n",
    "            'min_shared_verses': self.min_shared_verses,\n",
    "            'n_pairs_above': int(best_result['n_pairs_above']),\n",
    "            'pct_pairs_above': best_result['pct_pairs_above'],\n",
    "            'sample_size': len(sample_poems),\n",
    "            'total_poems': len(poem_to_clusters)\n",
    "        }\n",
    "        \n",
    "        pd.DataFrame([summary]).to_csv('full_orthographic_results/poem_enhanced_threshold_summary.csv', index=False)\n",
    "        print(f\"Summary saved to: poem_enhanced_threshold_summary.csv\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return threshold, grid_results, similarities_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "print(f\"Loading data from: {INPUT_CSV}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found: {INPUT_CSV}\")\n",
    "    print(f\"\\nPlease update the INPUT_CSV variable at the top of this script.\")\n",
    "    raise\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['cluster_id', 'idoriginal_poem']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\nError: Missing required columns: {missing_cols}\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "# Ensure correct data types\n",
    "df['cluster_id'] = pd.to_numeric(df['cluster_id'], errors='coerce').fillna(-1).astype(int)\n",
    "df['idoriginal_poem'] = df['idoriginal_poem'].astype(str)\n",
    "\n",
    "print(f\"✓ Loaded {len(df):,} verses\")\n",
    "print(f\"✓ Unique poems: {df['idoriginal_poem'].nunique():,}\")\n",
    "print(f\"✓ Unique verse clusters: {df[df['cluster_id'] != -1]['cluster_id'].nunique():,}\")\n",
    "\n",
    "if 'source_dataset' in df.columns:\n",
    "    print(f\"✓ Source datasets: {df['source_dataset'].unique()}\")\n",
    "else:\n",
    "    print(\"ℹ️  No source_dataset column - sampling by poem size only\")\n",
    "\n",
    "# Initialize and run\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ENHANCED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "selector = EnhancedPoemThresholdSelector(\n",
    "    sample_size=SAMPLE_SIZE, \n",
    "    random_seed=RANDOM_SEED, \n",
    "    min_shared_verses=MIN_SHARED_VERSES\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "threshold, grid_results, similarities_df = selector.run_threshold_analysis(df)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOTAL TIME: {elapsed/60:.2f} minutes\")\n",
    "print(f\"RECOMMENDED THRESHOLD: {threshold:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Show top 5 configurations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 5 CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "for i, row in grid_results.head(5).iterrows():\n",
    "    print(f\"\\n{i+1}. Threshold: {row['threshold']:.3f}\")\n",
    "    print(f\"   Quality: {row['quality_score']:.3f}, Silhouette: {row['silhouette']:.3f}\")\n",
    "    print(f\"   Clusters: {int(row['n_clusters']):,}, Avg Size: {row['avg_cluster_size']:.2f}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv('full_orthographic_results/poem_threshold_grid_search.csv')\n",
    "\n",
    "df = df.sort_values('threshold')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x='threshold',\n",
    "    y='quality_score',\n",
    "    marker='o',\n",
    "    linewidth=2,\n",
    "    markersize=8\n",
    ")\n",
    "\n",
    "plt.title(f'Quality Score vs Threshold)',\n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Cosine Similarity Threshold', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Quality Score', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_orthographic_results/poem_quality_vs_threshold.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35743538-734a-42fb-8ff5-0ee02c3da84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the threshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from numba import njit\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "INPUT_CSV = \"full_orthographic_results/clustered_optimized.csv\"\n",
    "THRESHOLD = 0.3\n",
    "MIN_SHARED_VERSES = 2\n",
    "OUTPUT_DIR = \"full_poem_clustering_results\"\n",
    "\n",
    "\n",
    "class FastPoemClusterer:\n",
    "    def __init__(self, threshold: float = 0.6, min_shared_verses: int = 2):\n",
    "        self.threshold = threshold\n",
    "        self.min_shared_verses = min_shared_verses\n",
    "        self.poem_clusters = {}\n",
    "        self.similarities = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct_poems_vectorized(df):\n",
    "        valid_mask = df['cluster_id'] != -1\n",
    "        df_valid = df[valid_mask]\n",
    "        grouped = df_valid.groupby('idoriginal_poem')['cluster_id'].apply(set)\n",
    "        return grouped.to_dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_inverted_index_fast(poem_to_clusters):\n",
    "        cluster_to_poems = defaultdict(set)\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            for c in clusters:\n",
    "                cluster_to_poems[c].add(poem_id)\n",
    "        return cluster_to_poems\n",
    "    \n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def jaccard_numba(a_arr, b_arr):\n",
    "        intersection = 0\n",
    "        a_set = set(a_arr)\n",
    "        b_set = set(b_arr)\n",
    "        \n",
    "        for item in a_set:\n",
    "            if item in b_set:\n",
    "                intersection += 1\n",
    "        \n",
    "        union = len(a_set) + len(b_set) - intersection\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n",
    "    \n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def count_shared_verses(a_arr, b_arr):\n",
    "        shared = 0\n",
    "        a_set = set(a_arr)\n",
    "        b_set = set(b_arr)\n",
    "        \n",
    "        for item in a_set:\n",
    "            if item in b_set:\n",
    "                shared += 1\n",
    "        \n",
    "        return shared\n",
    "    \n",
    "    def find_candidate_pairs_optimized(self, poem_to_clusters, cluster_to_poems):\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        multi_poem_clusters = {\n",
    "            c: poems for c, poems in cluster_to_poems.items() \n",
    "            if len(poems) > 1\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(multi_poem_clusters):,} clusters (filtered from {len(cluster_to_poems):,})\")\n",
    "        \n",
    "        small_threshold = 1000\n",
    "        \n",
    "        for cluster_id, poems in tqdm(multi_poem_clusters.items(), desc=\"Building candidates\"):\n",
    "            poems_list = list(poems)\n",
    "            n_poems = len(poems_list)\n",
    "            \n",
    "            if n_poems <= small_threshold:\n",
    "                for i in range(n_poems):\n",
    "                    for j in range(i + 1, n_poems):\n",
    "                        candidate_pairs.add(tuple(sorted((poems_list[i], poems_list[j]))))\n",
    "            else:\n",
    "                print(f\"  Warning: Large cluster {cluster_id} with {n_poems} poems - sampling pairs\")\n",
    "                indices = np.random.choice(n_poems, size=min(2000, n_poems), replace=False)\n",
    "                for i in range(len(indices)):\n",
    "                    for j in range(i + 1, len(indices)):\n",
    "                        candidate_pairs.add(tuple(sorted((poems_list[indices[i]], poems_list[indices[j]]))))\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def cluster_poems_fast(self, df):\n",
    "        print(\"Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        print(\"Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"Found {len(cluster_to_poems):,} clusters\")\n",
    "        \n",
    "        print(\"Finding candidate pairs...\")\n",
    "        candidate_pairs = self.find_candidate_pairs_optimized(poem_to_clusters, cluster_to_poems)\n",
    "        print(f\"Found {len(candidate_pairs):,} candidate pairs\")\n",
    "        \n",
    "        print(\"Converting to arrays...\")\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(clusters), dtype=np.int32) \n",
    "            for p, clusters in poem_to_clusters.items()\n",
    "        }\n",
    "        \n",
    "        poem_ids = list(poem_to_clusters.keys())\n",
    "        parent = {p: p for p in poem_ids}\n",
    "        rank = {p: 0 for p in poem_ids}\n",
    "        \n",
    "        def find(x):\n",
    "            if parent[x] != x:\n",
    "                parent[x] = find(parent[x])\n",
    "            return parent[x]\n",
    "        \n",
    "        def union(x, y):\n",
    "            px, py = find(x), find(y)\n",
    "            if px == py:\n",
    "                return False\n",
    "            \n",
    "            if rank[px] < rank[py]:\n",
    "                px, py = py, px\n",
    "            parent[py] = px\n",
    "            if rank[px] == rank[py]:\n",
    "                rank[px] += 1\n",
    "            return True\n",
    "        \n",
    "        print(f\"Computing similarities and clustering (min shared verses: {self.min_shared_verses})...\")\n",
    "        similarities = []\n",
    "        merges = 0\n",
    "        filtered_by_shared = 0\n",
    "        \n",
    "        batch_size = 100000\n",
    "        pairs_list = list(candidate_pairs)\n",
    "        \n",
    "        for batch_start in tqdm(range(0, len(pairs_list), batch_size), desc=\"Processing batches\"):\n",
    "            batch_end = min(batch_start + batch_size, len(pairs_list))\n",
    "            batch_pairs = pairs_list[batch_start:batch_end]\n",
    "            \n",
    "            for p1, p2 in batch_pairs:\n",
    "                if find(p1) == find(p2):\n",
    "                    continue\n",
    "                \n",
    "                shared = self.count_shared_verses(poem_to_array[p1], poem_to_array[p2])\n",
    "                \n",
    "                if shared < self.min_shared_verses:\n",
    "                    filtered_by_shared += 1\n",
    "                    continue\n",
    "                \n",
    "                sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "                \n",
    "                if sim >= self.threshold:\n",
    "                    if union(p1, p2):\n",
    "                        merges += 1\n",
    "                    similarities.append((p1, p2, sim, shared))\n",
    "            \n",
    "            if batch_start % (batch_size * 5) == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"Merged {merges:,} poem pairs\")\n",
    "        print(f\"Filtered {filtered_by_shared:,} pairs with <{self.min_shared_verses} shared verses\")\n",
    "        \n",
    "        self.poem_clusters = {p: find(p) for p in poem_ids}\n",
    "        self.similarities = similarities\n",
    "        \n",
    "        return self.poem_clusters, self.similarities\n",
    "    \n",
    "    def cluster_sizes(self):\n",
    "        cluster_count = defaultdict(int)\n",
    "        for cluster in self.poem_clusters.values():\n",
    "            cluster_count[cluster] += 1\n",
    "        sizes = np.array(list(cluster_count.values()))\n",
    "        return sizes\n",
    "\n",
    "\n",
    "class UltraFastPoemClusterer(FastPoemClusterer):\n",
    "    \n",
    "    def find_candidate_pairs_with_size_filter(self, poem_to_clusters, cluster_to_poems):\n",
    "        max_ratio = (1 + self.threshold) / self.threshold\n",
    "        candidate_pairs = set()\n",
    "        poem_sizes = {p: len(clusters) for p, clusters in poem_to_clusters.items()}\n",
    "        \n",
    "        multi_poem_clusters = {\n",
    "            c: poems for c, poems in cluster_to_poems.items() \n",
    "            if len(poems) > 1\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(multi_poem_clusters):,} clusters\")\n",
    "        print(f\"Max size ratio filter: {max_ratio:.2f}x\")\n",
    "        \n",
    "        filtered_by_size = 0\n",
    "        \n",
    "        for cluster_id, poems in tqdm(multi_poem_clusters.items(), desc=\"Building candidates\"):\n",
    "            poems_list = list(poems)\n",
    "            n_poems = len(poems_list)\n",
    "            \n",
    "            if n_poems > 1000:\n",
    "                print(f\"  Warning: Large cluster {cluster_id} with {n_poems} poems\")\n",
    "                poems_list = list(np.random.choice(poems_list, size=min(1000, n_poems), replace=False))\n",
    "                n_poems = len(poems_list)\n",
    "            \n",
    "            for i in range(n_poems):\n",
    "                p1 = poems_list[i]\n",
    "                size1 = poem_sizes[p1]\n",
    "                \n",
    "                for j in range(i + 1, n_poems):\n",
    "                    p2 = poems_list[j]\n",
    "                    size2 = poem_sizes[p2]\n",
    "                    \n",
    "                    ratio = max(size1, size2) / max(min(size1, size2), 1)\n",
    "                    if ratio > max_ratio:\n",
    "                        filtered_by_size += 1\n",
    "                        continue\n",
    "                    \n",
    "                    candidate_pairs.add(tuple(sorted((p1, p2))))\n",
    "        \n",
    "        print(f\"Filtered {filtered_by_size:,} pairs by size\")\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def cluster_poems_ultrafast(self, df):\n",
    "        print(\"Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        print(\"Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"Found {len(cluster_to_poems):,} clusters\")\n",
    "        \n",
    "        print(\"Finding candidate pairs with size filter...\")\n",
    "        candidate_pairs = self.find_candidate_pairs_with_size_filter(\n",
    "            poem_to_clusters, cluster_to_poems\n",
    "        )\n",
    "        print(f\"Found {len(candidate_pairs):,} candidate pairs\")\n",
    "        \n",
    "        return self._cluster_with_pairs(poem_to_clusters, candidate_pairs)\n",
    "    \n",
    "    def _cluster_with_pairs(self, poem_to_clusters, candidate_pairs):\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(clusters), dtype=np.int32) \n",
    "            for p, clusters in poem_to_clusters.items()\n",
    "        }\n",
    "        \n",
    "        poem_ids = list(poem_to_clusters.keys())\n",
    "        parent = {p: p for p in poem_ids}\n",
    "        rank = {p: 0 for p in poem_ids}\n",
    "        \n",
    "        def find(x):\n",
    "            if parent[x] != x:\n",
    "                parent[x] = find(parent[x])\n",
    "            return parent[x]\n",
    "        \n",
    "        def union(x, y):\n",
    "            px, py = find(x), find(y)\n",
    "            if px == py:\n",
    "                return False\n",
    "            if rank[px] < rank[py]:\n",
    "                px, py = py, px\n",
    "            parent[py] = px\n",
    "            if rank[px] == rank[py]:\n",
    "                rank[px] += 1\n",
    "            return True\n",
    "        \n",
    "        print(f\"Computing similarities (min shared verses: {self.min_shared_verses})...\")\n",
    "        similarities = []\n",
    "        merges = 0\n",
    "        filtered_by_shared = 0\n",
    "        \n",
    "        pairs_list = list(candidate_pairs)\n",
    "        \n",
    "        for p1, p2 in tqdm(pairs_list, desc=\"Clustering\"):\n",
    "            if find(p1) == find(p2):\n",
    "                continue\n",
    "            \n",
    "            shared = self.count_shared_verses(poem_to_array[p1], poem_to_array[p2])\n",
    "            \n",
    "            if shared < self.min_shared_verses:\n",
    "                filtered_by_shared += 1\n",
    "                continue\n",
    "            \n",
    "            sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "            \n",
    "            if sim >= self.threshold:\n",
    "                if union(p1, p2):\n",
    "                    merges += 1\n",
    "                similarities.append((p1, p2, sim, shared))\n",
    "        \n",
    "        print(f\"Merged {merges:,} poem pairs\")\n",
    "        print(f\"Filtered {filtered_by_shared:,} pairs with <{self.min_shared_verses} shared verses\")\n",
    "        \n",
    "        self.poem_clusters = {p: find(p) for p in poem_ids}\n",
    "        self.similarities = similarities\n",
    "        \n",
    "        return self.poem_clusters, self.similarities\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FULL POEM CLUSTERING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Input: {INPUT_CSV}\")\n",
    "    print(f\"Threshold: {THRESHOLD}\")\n",
    "    print(f\"Min shared verses: {MIN_SHARED_VERSES}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}/\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nLoading data...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    df['cluster_id'] = pd.to_numeric(df['cluster_id'], errors='coerce').fillna(-1).astype(int)\n",
    "    df['idoriginal_poem'] = df['idoriginal_poem'].astype(str)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} verses\")\n",
    "    print(f\"Unique poems: {df['idoriginal_poem'].nunique():,}\")\n",
    "    \n",
    "    n_unique_poems = df['idoriginal_poem'].nunique()\n",
    "    \n",
    "    if n_unique_poems > 100000:\n",
    "        print(\"\\nUsing UltraFastPoemClusterer (size-filtered)\")\n",
    "        clusterer = UltraFastPoemClusterer(threshold=THRESHOLD, min_shared_verses=MIN_SHARED_VERSES)\n",
    "        start = time.time()\n",
    "        poem_clusters, similarities = clusterer.cluster_poems_ultrafast(df)\n",
    "    else:\n",
    "        print(\"\\nUsing FastPoemClusterer\")\n",
    "        clusterer = FastPoemClusterer(threshold=THRESHOLD, min_shared_verses=MIN_SHARED_VERSES)\n",
    "        start = time.time()\n",
    "        poem_clusters, similarities = clusterer.cluster_poems_fast(df)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    sizes = clusterer.cluster_sizes()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Time:                  {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Poem clusters:         {len(set(poem_clusters.values())):,}\")\n",
    "    print(f\"Poems with matches:    {len(similarities):,} pairs\")\n",
    "    print(f\"Cluster size mean:     {sizes.mean():.2f}\")\n",
    "    print(f\"Cluster size median:   {np.median(sizes):.0f}\")\n",
    "    print(f\"Cluster size max:      {sizes.max()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(\"\\nAdding poem cluster IDs to dataframe...\")\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].map(poem_clusters)\n",
    "    \n",
    "    print(\"\\nSaving results...\")\n",
    "    \n",
    "    output_csv = os.path.join(OUTPUT_DIR, \"verses_with_poem_clusters.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✓ Full results: {output_csv}\")\n",
    "    \n",
    "    poem_cluster_summary = []\n",
    "    cluster_to_poems = defaultdict(list)\n",
    "    \n",
    "    for poem_id, cluster_id in poem_clusters.items():\n",
    "        cluster_to_poems[cluster_id].append(poem_id)\n",
    "    \n",
    "    for cluster_id, poems in cluster_to_poems.items():\n",
    "        if len(poems) > 1:\n",
    "            poem_cluster_summary.append({\n",
    "                'poem_cluster_id': cluster_id,\n",
    "                'n_poems': len(poems),\n",
    "                'poem_ids': ', '.join(poems) if len(poems) <= 10 else f\"{', '.join(poems[:10])}... (and {len(poems)-10} more)\"\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(poem_cluster_summary).sort_values('n_poems', ascending=False)\n",
    "    summary_csv = os.path.join(OUTPUT_DIR, \"poem_clusters_summary.csv\")\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(f\"✓ Cluster summary: {summary_csv}\")\n",
    "    \n",
    "    if len(similarities) > 0:\n",
    "        sim_df = pd.DataFrame(similarities, columns=['poem1', 'poem2', 'similarity', 'shared_verses'])\n",
    "        sim_df = sim_df.sort_values('similarity', ascending=False)\n",
    "        sim_csv = os.path.join(OUTPUT_DIR, \"poem_similarities.csv\")\n",
    "        sim_df.to_csv(sim_csv, index=False)\n",
    "        print(f\"✓ Similarities: {sim_csv}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'threshold': THRESHOLD,\n",
    "        'min_shared_verses': MIN_SHARED_VERSES,\n",
    "        'n_poems': n_unique_poems,\n",
    "        'n_clusters': len(set(poem_clusters.values())),\n",
    "        'n_multi_poem_clusters': np.sum(sizes > 1),\n",
    "        'n_singleton_clusters': np.sum(sizes == 1),\n",
    "        'avg_cluster_size': float(sizes.mean()),\n",
    "        'median_cluster_size': float(np.median(sizes)),\n",
    "        'max_cluster_size': int(sizes.max()),\n",
    "        'n_similarity_pairs': len(similarities),\n",
    "        'clustering_time_minutes': elapsed / 60\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_csv = os.path.join(OUTPUT_DIR, \"clustering_metrics.csv\")\n",
    "    metrics_df.to_csv(metrics_csv, index=False)\n",
    "    print(f\"✓ Metrics: {metrics_csv}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOP 20 POEM CLUSTERS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    if len(summary_df) > 0:\n",
    "        print(summary_df.head(20).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No multi-poem clusters found\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✅ CLUSTERING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"All results saved to: {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25357da7-6d58-44a6-a70b-bdabb970dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking results\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# === CONFIG ===\n",
    "OUTPUT_DIR = \"full_poem_clustering_results\"\n",
    "N_CLUSTERS_TO_SHOW = 5       # how many poem clusters to display\n",
    "N_POEMS_PER_CLUSTER = 3      # how many poems to sample per cluster\n",
    "MAX_LINES_PER_POEM = 20      # limit printed lines per poem (for readability)\n",
    "\n",
    "# === LOAD RESULTS ===\n",
    "df = pd.read_csv(f\"{OUTPUT_DIR}/verses_with_poem_clusters.csv\")\n",
    "summary_df = pd.read_csv(f\"{OUTPUT_DIR}/poem_clusters_summary.csv\")\n",
    "\n",
    "# --- Coerce types safely ---\n",
    "df['idoriginal_poem'] = df['idoriginal_poem'].astype(str)\n",
    "if 'poem_cluster_id' in df.columns:\n",
    "    df['poem_cluster_id'] = df['poem_cluster_id'].astype(str)\n",
    "if 'poem_cluster_id' in summary_df.columns:\n",
    "    summary_df['poem_cluster_id'] = summary_df['poem_cluster_id'].astype(str)\n",
    "if 'source_dataset' in df.columns:\n",
    "    df['source_dataset'] = df['source_dataset'].astype(str)\n",
    "if 'order' in df.columns:\n",
    "    df['order'] = pd.to_numeric(df['order'], errors='coerce').fillna(0)\n",
    "\n",
    "print(f\"Loaded {len(df):,} verses from {df['idoriginal_poem'].nunique():,} poems.\")\n",
    "print(f\"Found {len(summary_df):,} multi-poem clusters.\\n\")\n",
    "\n",
    "# === RECONSTRUCT FULL POEMS ===\n",
    "sort_cols = ['idoriginal_poem', 'order'] if 'order' in df.columns else ['idoriginal_poem']\n",
    "df_sorted = df.sort_values(sort_cols)\n",
    "\n",
    "poems_df = (\n",
    "    df_sorted.groupby('idoriginal_poem').agg({\n",
    "        'verse': lambda x: [v for v in x if isinstance(v, str)],\n",
    "        'source_dataset': 'first'\n",
    "    }).reset_index()\n",
    ")\n",
    "poems_df['full_poem'] = poems_df['verse'].apply(lambda verses: \"\\n\".join(verses))\n",
    "\n",
    "# Map poem_cluster_id\n",
    "poem_to_cluster = (\n",
    "    df_sorted.drop_duplicates('idoriginal_poem')[['idoriginal_poem', 'poem_cluster_id']]\n",
    ")\n",
    "poems_df = poems_df.merge(poem_to_cluster, on='idoriginal_poem', how='left')\n",
    "\n",
    "print(f\"Reconstructed {len(poems_df):,} poems with full text.\\n\")\n",
    "\n",
    "# === DISPLAY SAMPLE CLUSTERS ===\n",
    "sample_clusters = summary_df.head(N_CLUSTERS_TO_SHOW)['poem_cluster_id'].tolist()\n",
    "\n",
    "for cluster_id in sample_clusters:\n",
    "    cluster_poems = poems_df[poems_df['poem_cluster_id'] == cluster_id]\n",
    "    n_poems = len(cluster_poems)\n",
    "    display(Markdown(f\"## 🧩 Cluster `{cluster_id}` — {n_poems} poems\"))\n",
    "\n",
    "    sample_poems = cluster_poems.sample(min(N_POEMS_PER_CLUSTER, n_poems), random_state=42)\n",
    "    for _, row in sample_poems.iterrows():\n",
    "        verses = row['full_poem'].split(\"\\n\")\n",
    "        display(Markdown(f\"**📜 Poem ID:** `{row['idoriginal_poem']}` | **Dataset:** `{row['source_dataset']}`\"))\n",
    "        \n",
    "        # Ensure each verse is on a new line in Markdown\n",
    "        md_verses = \"<br>\".join(verses[:MAX_LINES_PER_POEM])\n",
    "        display(Markdown(md_verses))\n",
    "        \n",
    "        if len(verses) > MAX_LINES_PER_POEM:\n",
    "            display(Markdown(f\"... _(truncated, total {len(verses)} verses)_\"))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbea7b6-4101-4699-b9f1-a06393315d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

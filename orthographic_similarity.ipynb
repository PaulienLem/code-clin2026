{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7252686-05bd-4c43-8175-a32a47a66cf8",
   "metadata": {},
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51922e6-8776-4973-98ca-79e383e52cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected - using CuPy acceleration\n",
      "====================================================================================================\n",
      "LOADING DATA\n",
      "====================================================================================================\n",
      "\n",
      "Loaded 54,170 verses\n",
      "\n",
      "====================================================================================================\n",
      "VERSE-LEVEL 2D GRID SEARCH\n",
      "====================================================================================================\n",
      "\n",
      "Testing thresholds: ['20%', '30%', '40%', '50%']\n",
      "Testing shingle sizes: [2, 3, 4, 5]\n",
      "\n",
      "Total combinations: 16\n",
      "\n",
      "\n",
      "Testing shingle_size=2, threshold=20%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.2, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:51<00:00, 1056.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 571,991 similarities in 51.27s\n",
      "Throughput: 1,057 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 571991/571991 [00:00<00:00, 2156707.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20,443 clusters in 51.55s total\n",
      "  ARI: 0.0800, V-measure: 0.9455, Clusters: 20443\n",
      "\n",
      "Testing shingle_size=2, threshold=30%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.3, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:48<00:00, 1111.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 549,960 similarities in 48.75s\n",
      "Throughput: 1,111 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 549960/549960 [00:00<00:00, 2050463.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22,215 clusters in 49.03s total\n",
      "  ARI: 0.1567, V-measure: 0.9653, Clusters: 22215\n",
      "\n",
      "Testing shingle_size=2, threshold=40%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.4, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:48<00:00, 1111.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 537,327 similarities in 48.74s\n",
      "Throughput: 1,111 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 537327/537327 [00:00<00:00, 1961500.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24,327 clusters in 49.03s total\n",
      "  ARI: 0.7814, V-measure: 0.9884, Clusters: 24327\n",
      "\n",
      "Testing shingle_size=2, threshold=50%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.5, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:48<00:00, 1116.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 534,306 similarities in 48.51s\n",
      "Throughput: 1,117 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 534306/534306 [00:00<00:00, 1946281.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 25,057 clusters in 48.79s total\n",
      "  ARI: 0.9258, V-measure: 0.9924, Clusters: 25057\n",
      "\n",
      "Testing shingle_size=3, threshold=20%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.2, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:49<00:00, 1101.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 485,262 similarities in 49.19s\n",
      "Throughput: 1,101 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 485262/485262 [00:00<00:00, 1913023.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26,415 clusters in 49.46s total\n",
      "  ARI: 0.9156, V-measure: 0.9905, Clusters: 26415\n",
      "\n",
      "Testing shingle_size=3, threshold=30%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.3, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:49<00:00, 1098.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 485,235 similarities in 49.33s\n",
      "Throughput: 1,098 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 485235/485235 [00:00<00:00, 1980023.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26,425 clusters in 49.59s total\n",
      "  ARI: 0.9157, V-measure: 0.9905, Clusters: 26425\n",
      "\n",
      "Testing shingle_size=3, threshold=40%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.4, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:49<00:00, 1104.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 485,132 similarities in 49.02s\n",
      "Throughput: 1,105 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 485132/485132 [00:00<00:00, 1887342.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26,453 clusters in 49.29s total\n",
      "  ARI: 0.9156, V-measure: 0.9905, Clusters: 26453\n",
      "\n",
      "Testing shingle_size=3, threshold=50%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.5, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:49<00:00, 1099.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 484,836 similarities in 49.25s\n",
      "Throughput: 1,100 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 484836/484836 [00:00<00:00, 1914967.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26,507 clusters in 49.52s total\n",
      "  ARI: 0.9158, V-measure: 0.9906, Clusters: 26507\n",
      "\n",
      "Testing shingle_size=4, threshold=20%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.2, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:51<00:00, 1062.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 455,716 similarities in 51.01s\n",
      "Throughput: 1,062 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 455716/455716 [00:00<00:00, 1845168.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 27,384 clusters in 51.26s total\n",
      "  ARI: 0.9044, V-measure: 0.9877, Clusters: 27384\n",
      "\n",
      "Testing shingle_size=4, threshold=30%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.3, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:50<00:00, 1067.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 455,703 similarities in 50.75s\n",
      "Throughput: 1,067 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 455703/455703 [00:00<00:00, 1852786.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 27,389 clusters in 51.01s total\n",
      "  ARI: 0.9044, V-measure: 0.9877, Clusters: 27389\n",
      "\n",
      "Testing shingle_size=4, threshold=40%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.4, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:50<00:00, 1068.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 455,519 similarities in 50.69s\n",
      "Throughput: 1,069 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 455519/455519 [00:00<00:00, 1851570.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 27,402 clusters in 50.94s total\n",
      "  ARI: 0.9040, V-measure: 0.9877, Clusters: 27402\n",
      "\n",
      "Testing shingle_size=4, threshold=50%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.5, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:51<00:00, 1061.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 454,722 similarities in 51.05s\n",
      "Throughput: 1,061 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 454722/454722 [00:00<00:00, 1846943.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 27,451 clusters in 51.31s total\n",
      "  ARI: 0.9011, V-measure: 0.9876, Clusters: 27451\n",
      "\n",
      "Testing shingle_size=5, threshold=20%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.2, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:52<00:00, 1030.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 422,947 similarities in 52.57s\n",
      "Throughput: 1,030 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 422947/422947 [00:00<00:00, 1831338.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 28,483 clusters in 52.81s total\n",
      "  ARI: 0.8570, V-measure: 0.9827, Clusters: 28483\n",
      "\n",
      "Testing shingle_size=5, threshold=30%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.3, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:52<00:00, 1030.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 422,945 similarities in 52.57s\n",
      "Throughput: 1,030 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 422945/422945 [00:00<00:00, 1826140.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 28,483 clusters in 52.81s total\n",
      "  ARI: 0.8570, V-measure: 0.9827, Clusters: 28483\n",
      "\n",
      "Testing shingle_size=5, threshold=40%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.4, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:52<00:00, 1029.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 422,917 similarities in 52.61s\n",
      "Throughput: 1,030 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 422917/422917 [00:00<00:00, 1770674.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 28,494 clusters in 52.86s total\n",
      "  ARI: 0.8570, V-measure: 0.9827, Clusters: 28494\n",
      "\n",
      "Testing shingle_size=5, threshold=50%...\n",
      "Using GPU for MinHash (128 permutations)\n",
      "Initialized in GPU (CuPy) mode\n",
      "\n",
      "Clustering 54,170 documents in 54170 chunks\n",
      "threshold=0.5, chunk_size=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 54170/54170 [00:53<00:00, 1014.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 422,262 similarities in 53.37s\n",
      "Throughput: 1,015 docs/sec\n",
      "Building clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 422262/422262 [00:00<00:00, 1761605.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 28,538 clusters in 53.62s total\n",
      "  ARI: 0.8543, V-measure: 0.9825, Clusters: 28538\n",
      "\n",
      "====================================================================================================\n",
      "VERSE-LEVEL GRID SEARCH SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Shingle    Threshold    Clusters   Similarities    ARI      V-measure   \n",
      "--------------------------------------------------------------------------------\n",
      "2.0        20%          20443.0    571991.0        0.0800   0.9455      \n",
      "2.0        30%          22215.0    549960.0        0.1567   0.9653      \n",
      "2.0        40%          24327.0    537327.0        0.7814   0.9884      \n",
      "2.0        50%          25057.0    534306.0        0.9258   0.9924      \n",
      "3.0        20%          26415.0    485262.0        0.9156   0.9905      \n",
      "3.0        30%          26425.0    485235.0        0.9157   0.9905      \n",
      "3.0        40%          26453.0    485132.0        0.9156   0.9905      \n",
      "3.0        50%          26507.0    484836.0        0.9158   0.9906      \n",
      "4.0        20%          27384.0    455716.0        0.9044   0.9877      \n",
      "4.0        30%          27389.0    455703.0        0.9044   0.9877      \n",
      "4.0        40%          27402.0    455519.0        0.9040   0.9877      \n",
      "4.0        50%          27451.0    454722.0        0.9011   0.9876      \n",
      "5.0        20%          28483.0    422947.0        0.8570   0.9827      \n",
      "5.0        30%          28483.0    422945.0        0.8570   0.9827      \n",
      "5.0        40%          28494.0    422917.0        0.8570   0.9827      \n",
      "5.0        50%          28538.0    422262.0        0.8543   0.9825      \n",
      "\n",
      "====================================================================================================\n",
      "BEST VERSE-LEVEL PARAMETERS\n",
      "====================================================================================================\n",
      "\n",
      "Best parameters by ARI:\n",
      "  Shingle size: 2\n",
      "  Threshold: 50%\n",
      "  ARI: 0.9258\n",
      "  V-measure: 0.9924\n",
      "  Number of clusters: 25057.0\n",
      "  Number of similarities found: 534306.0\n",
      "\n",
      "Visualization saved to: verse_grid_search_results.png\n",
      "\n",
      "dbbe_verse_clustered_results.csv saved with best parameters (shingle_size=2, threshold=50%)\n",
      "\n",
      "Reconstructed 12783 poems\n",
      "\n",
      "====================================================================================================\n",
      "POEM-LEVEL GRID SEARCH\n",
      "====================================================================================================\n",
      "\n",
      "Testing thresholds: ['50%', '60%', '70%', '80%']\n",
      "\n",
      "\n",
      "Testing threshold 50%...\n",
      "  ARI: 0.8076, V-measure: 0.9767, Clusters: 5138\n",
      "\n",
      "Testing threshold 60%...\n",
      "  ARI: 0.8296, V-measure: 0.9782, Clusters: 5386\n",
      "\n",
      "Testing threshold 70%...\n",
      "  ARI: 0.8430, V-measure: 0.9770, Clusters: 5548\n",
      "\n",
      "Testing threshold 80%...\n",
      "  ARI: 0.8345, V-measure: 0.9756, Clusters: 5649\n",
      "\n",
      "====================================================================================================\n",
      "POEM-LEVEL GRID SEARCH SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Threshold    Clusters   Edges      ARI      V-measure    Perfect Recon. \n",
      "--------------------------------------------------------------------------------\n",
      "50%          5138.0     230178.0   0.8076   0.9767       84.0%          \n",
      "60%          5386.0     216127.0   0.8296   0.9782       85.6%          \n",
      "70%          5548.0     207873.0   0.8430   0.9770       85.3%          \n",
      "80%          5649.0     202888.0   0.8345   0.9756       84.9%          \n",
      "\n",
      "====================================================================================================\n",
      "BEST POEM-LEVEL THRESHOLD\n",
      "====================================================================================================\n",
      "\n",
      "Best threshold by ARI: 70%\n",
      "  ARI: 0.8430\n",
      "  V-measure: 0.9770\n",
      "  Perfect reconstruction rate: 85.3%\n",
      "    (4414/5175 GT clusters perfectly reconstructed)\n",
      "\n",
      "Visualization saved to: poem_grid_search_results.png\n",
      "\n",
      "dbbe_poem_level_clusters.csv saved with best threshold (70%)\n",
      "\n",
      "====================================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import Counter, defaultdict\n",
    "# from typing import List, Dict, Tuple, Optional, Set\n",
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# import unicodedata\n",
    "\n",
    "# try:\n",
    "#     import cupy as cp\n",
    "#     GPU_AVAILABLE = True\n",
    "#     print(\"GPU detected - using CuPy acceleration\")\n",
    "# except ImportError:\n",
    "#     cp = np\n",
    "#     GPU_AVAILABLE = False\n",
    "#     print(\"No GPU - using NumPy (CPU mode)\")\n",
    "\n",
    "\n",
    "# class TextPreprocessor:\n",
    "#     def __init__(self, lowercase=True, remove_punctuation=True, remove_diacritics=True):\n",
    "#         self.lowercase = lowercase\n",
    "#         self.remove_punctuation = remove_punctuation\n",
    "#         self.remove_diacritics = remove_diacritics\n",
    "#         if remove_punctuation:\n",
    "#             self.punct_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "#             self.remove_chars_pattern = re.compile(r'[\\(\\)\\{\\}]')\n",
    "\n",
    "#     def _remove_diacritics(self, text: str) -> str:\n",
    "#         return ''.join(\n",
    "#             c for c in unicodedata.normalize('NFD', text)\n",
    "#             if unicodedata.category(c) != 'Mn'\n",
    "#         )\n",
    "\n",
    "#     def preprocess(self, text: str) -> str:\n",
    "#         if not isinstance(text, str):\n",
    "#             text = str(text) if pd.notna(text) else ''\n",
    "\n",
    "#         if self.remove_diacritics:\n",
    "#             text = self._remove_diacritics(text)\n",
    "#         if self.lowercase:\n",
    "#             text = text.lower()\n",
    "#         if self.remove_punctuation:\n",
    "#             text = self.remove_chars_pattern.sub('', text)\n",
    "#             text = self.punct_pattern.sub(' ', text)\n",
    "\n",
    "#         return ' '.join(text.split())\n",
    "\n",
    "#     def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "#         return [self.preprocess(t) for t in texts]\n",
    "\n",
    "\n",
    "# class ShingleGenerator:\n",
    "#     def __init__(self, shingle_size: int = 4, use_gpu: bool = GPU_AVAILABLE):\n",
    "#         self.shingle_size = shingle_size\n",
    "#         self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "#         self.xp = cp if self.use_gpu else np\n",
    "\n",
    "#     def generate_shingles(self, text: str) -> np.ndarray:\n",
    "#         if len(text) < self.shingle_size:\n",
    "#             return np.array([hash(text) % (2**31)], dtype=np.int32)\n",
    "\n",
    "#         chars = self.xp.array([ord(c) for c in text], dtype=np.int32)\n",
    "#         n_shingles = len(text) - self.shingle_size + 1\n",
    "\n",
    "#         shingles = self.xp.zeros(n_shingles, dtype=np.int32)\n",
    "#         for i in range(self.shingle_size):\n",
    "#             shingles += chars[i:i+n_shingles] * (31 ** i)\n",
    "\n",
    "#         unique_shingles = self.xp.unique(shingles)\n",
    "\n",
    "#         if self.use_gpu:\n",
    "#             unique_shingles = cp.asnumpy(unique_shingles)\n",
    "\n",
    "#         return unique_shingles\n",
    "\n",
    "#     def generate_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "#         return [self.generate_shingles(t) for t in texts]\n",
    "\n",
    "\n",
    "# class MinHashProcessor:\n",
    "#     def __init__(self, num_perm: int = 128, use_gpu: bool = GPU_AVAILABLE):\n",
    "#         self.num_perm = num_perm\n",
    "#         self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "#         self.xp = cp if self.use_gpu else np\n",
    "\n",
    "#         rng = self.xp.random.RandomState(42)\n",
    "#         self.hash_a = rng.randint(1, 2**31-1, num_perm, dtype=np.int64)\n",
    "#         self.hash_b = rng.randint(0, 2**31-1, num_perm, dtype=np.int64)\n",
    "#         self.prime = np.int64(2**31-1)\n",
    "\n",
    "#         if self.use_gpu:\n",
    "#             print(f\"Using GPU for MinHash ({num_perm} permutations)\")\n",
    "\n",
    "#     def compute_signature(self, shingles: np.ndarray) -> np.ndarray:\n",
    "#         if len(shingles) == 0:\n",
    "#             return np.full(self.num_perm, self.prime, dtype=np.int64)\n",
    "\n",
    "#         if self.use_gpu:\n",
    "#             shingles_gpu = self.xp.array(shingles, dtype=np.int64)\n",
    "#         else:\n",
    "#             shingles_gpu = shingles.astype(np.int64)\n",
    "\n",
    "#         shingles_expanded = shingles_gpu[:, self.xp.newaxis]\n",
    "#         hashes = (self.hash_a * shingles_expanded + self.hash_b) % self.prime\n",
    "#         signature = self.xp.min(hashes, axis=0)\n",
    "\n",
    "#         if self.use_gpu:\n",
    "#             signature = cp.asnumpy(signature)\n",
    "\n",
    "#         return signature\n",
    "\n",
    "#     def compute_batch(self, shingles_batch: List[np.ndarray]) -> np.ndarray:\n",
    "#         signatures = np.zeros((len(shingles_batch), self.num_perm), dtype=np.int64)\n",
    "#         for i, shingles in enumerate(shingles_batch):\n",
    "#             signatures[i] = self.compute_signature(shingles)\n",
    "#         return signatures\n",
    "\n",
    "\n",
    "# class LSHIndex:\n",
    "#     def __init__(self, threshold: float = 0.3, num_perm: int = 128):\n",
    "#         self.threshold = threshold\n",
    "#         self.num_perm = num_perm\n",
    "#         self.bands = 16\n",
    "#         self.rows = num_perm // self.bands\n",
    "#         self.signatures = []\n",
    "#         self.num_docs = 0\n",
    "#         self.hash_tables = [defaultdict(list) for _ in range(self.bands)]\n",
    "\n",
    "#     def _hash_band(self, band: np.ndarray) -> int:\n",
    "#         return int(hash(tuple(band)) % (2**31))\n",
    "\n",
    "#     def insert_batch(self, signatures: np.ndarray, start_idx: int):\n",
    "#         batch_size = signatures.shape[0]\n",
    "#         self.signatures.append(signatures)\n",
    "\n",
    "#         for band_idx in range(self.bands):\n",
    "#             start_row = band_idx * self.rows\n",
    "#             end_row = start_row + self.rows\n",
    "\n",
    "#             for doc_idx in range(batch_size):\n",
    "#                 band = signatures[doc_idx, start_row:end_row]\n",
    "#                 band_hash = self._hash_band(band)\n",
    "#                 global_doc_id = start_idx + doc_idx\n",
    "#                 self.hash_tables[band_idx][band_hash].append(global_doc_id)\n",
    "\n",
    "#         self.num_docs += batch_size\n",
    "\n",
    "#     def query_batch(self, signatures: np.ndarray, start_idx: int) -> List[set]:\n",
    "#         batch_size = signatures.shape[0]\n",
    "#         candidates = [set() for _ in range(batch_size)]\n",
    "\n",
    "#         for band_idx in range(self.bands):\n",
    "#             start_row = band_idx * self.rows\n",
    "#             end_row = start_row + self.rows\n",
    "\n",
    "#             for doc_idx in range(batch_size):\n",
    "#                 query_doc_id = start_idx + doc_idx\n",
    "#                 band = signatures[doc_idx, start_row:end_row]\n",
    "#                 band_hash = self._hash_band(band)\n",
    "#                 bucket = self.hash_tables[band_idx].get(band_hash, [])\n",
    "#                 candidates[doc_idx].update(c for c in bucket if c < query_doc_id)\n",
    "\n",
    "#         return candidates\n",
    "\n",
    "\n",
    "# class SimilarityComputer:\n",
    "#     def __init__(self, threshold: float = 0.3, use_gpu: bool = GPU_AVAILABLE):\n",
    "#         self.threshold = threshold\n",
    "#         self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "#         self.xp = cp if self.use_gpu else np\n",
    "\n",
    "#     def compute_batch_similarities(self, query_sig: np.ndarray,\n",
    "#                                    candidate_sigs: np.ndarray) -> np.ndarray:\n",
    "#         if self.use_gpu:\n",
    "#             query_gpu = self.xp.array(query_sig)\n",
    "#             cands_gpu = self.xp.array(candidate_sigs)\n",
    "#             query_expanded = self.xp.tile(query_gpu, (len(candidate_sigs), 1))\n",
    "#             matches = self.xp.sum(query_expanded == cands_gpu, axis=1)\n",
    "#             sims = matches.astype(np.float32) / query_sig.shape[0]\n",
    "#             return cp.asnumpy(sims)\n",
    "#         else:\n",
    "#             query_expanded = np.tile(query_sig, (len(candidate_sigs), 1))\n",
    "#             matches = np.sum(query_expanded == candidate_sigs, axis=1)\n",
    "#             return matches.astype(np.float32) / query_sig.shape[0]\n",
    "\n",
    "\n",
    "# class UnionFind:\n",
    "#     def __init__(self, n: int):\n",
    "#         self.parent = list(range(n))\n",
    "#         self.rank = [0] * n\n",
    "\n",
    "#     def find(self, x: int) -> int:\n",
    "#         if self.parent[x] != x:\n",
    "#             self.parent[x] = self.find(self.parent[x])\n",
    "#         return self.parent[x]\n",
    "\n",
    "#     def union(self, x: int, y: int):\n",
    "#         px, py = self.find(x), self.find(y)\n",
    "#         if px == py:\n",
    "#             return\n",
    "#         if self.rank[px] < self.rank[py]:\n",
    "#             px, py = py, px\n",
    "#         self.parent[py] = px\n",
    "#         if self.rank[px] == self.rank[py]:\n",
    "#             self.rank[px] += 1\n",
    "\n",
    "#     def get_clusters(self) -> Dict[int, int]:\n",
    "#         return {i: self.find(i) for i in range(len(self.parent))}\n",
    "\n",
    "\n",
    "# class FastMinHashClustering:\n",
    "#     def __init__(self, threshold: float = 0.3, shingle_size: int = 4,\n",
    "#                  num_perm: int = 128, chunk_size: int = 50000,\n",
    "#                  use_gpu: Optional[bool] = None):\n",
    "\n",
    "#         if use_gpu is None:\n",
    "#             use_gpu = GPU_AVAILABLE\n",
    "\n",
    "#         self.threshold = threshold\n",
    "#         self.chunk_size = chunk_size\n",
    "#         self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "\n",
    "#         self.preprocessor = TextPreprocessor(\n",
    "#             lowercase=True,\n",
    "#             remove_punctuation=True,\n",
    "#             remove_diacritics=True\n",
    "#         )\n",
    "#         self.shingler = ShingleGenerator(shingle_size, use_gpu)\n",
    "#         self.minhash = MinHashProcessor(num_perm, use_gpu)\n",
    "#         self.lsh_index = LSHIndex(threshold, num_perm)\n",
    "#         self.similarity_computer = SimilarityComputer(threshold, use_gpu)\n",
    "#         self.all_similarities = []\n",
    "\n",
    "#         mode = \"GPU (CuPy)\" if self.use_gpu else \"CPU (NumPy)\"\n",
    "#         print(f\"Initialized in {mode} mode\")\n",
    "\n",
    "#     def cluster(self, texts: List[str]) -> Tuple[Dict[int, int], List[Tuple[int, int, float]]]:\n",
    "#         n_docs = len(texts)\n",
    "#         n_chunks = (n_docs + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "#         print(f\"\\nClustering {n_docs:,} documents in {n_chunks} chunks\")\n",
    "#         print(f\"threshold={self.threshold}, chunk_size={self.chunk_size:,}\")\n",
    "\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         for chunk_idx in tqdm(range(n_chunks), desc=\"Processing\"):\n",
    "#             chunk_start = chunk_idx * self.chunk_size\n",
    "#             chunk_end = min(chunk_start + self.chunk_size, n_docs)\n",
    "#             chunk_texts = texts[chunk_start:chunk_end]\n",
    "\n",
    "#             processed = self.preprocessor.preprocess_batch(chunk_texts)\n",
    "#             shingles = self.shingler.generate_batch(processed)\n",
    "#             signatures = self.minhash.compute_batch(shingles)\n",
    "#             self.lsh_index.insert_batch(signatures, chunk_start)\n",
    "\n",
    "#             if chunk_start > 0:\n",
    "#                 candidates = self.lsh_index.query_batch(signatures, chunk_start)\n",
    "\n",
    "#                 for doc_idx, cand_set in enumerate(candidates):\n",
    "#                     if not cand_set:\n",
    "#                         continue\n",
    "\n",
    "#                     query_doc_id = chunk_start + doc_idx\n",
    "#                     query_sig = signatures[doc_idx]\n",
    "\n",
    "#                     cand_list = sorted(cand_set)\n",
    "#                     cand_sigs = []\n",
    "#                     for cand_id in cand_list:\n",
    "#                         batch_idx = cand_id // self.chunk_size\n",
    "#                         local_idx = cand_id % self.chunk_size\n",
    "#                         if batch_idx < len(self.lsh_index.signatures):\n",
    "#                             cand_sigs.append(self.lsh_index.signatures[batch_idx][local_idx])\n",
    "\n",
    "#                     if cand_sigs:\n",
    "#                         cand_sigs = np.array(cand_sigs)\n",
    "#                         sims = self.similarity_computer.compute_batch_similarities(\n",
    "#                             query_sig, cand_sigs\n",
    "#                         )\n",
    "\n",
    "#                         for cand_id, sim in zip(cand_list[:len(sims)], sims):\n",
    "#                             if sim >= self.threshold:\n",
    "#                                 self.all_similarities.append((cand_id, query_doc_id, float(sim)))\n",
    "\n",
    "#         elapsed = time.time() - start_time\n",
    "#         print(f\"\\nFound {len(self.all_similarities):,} similarities in {elapsed:.2f}s\")\n",
    "#         print(f\"Throughput: {n_docs/elapsed:,.0f} docs/sec\")\n",
    "\n",
    "#         print(\"Building clusters...\")\n",
    "#         uf = UnionFind(n_docs)\n",
    "#         for doc1, doc2, _ in tqdm(self.all_similarities, desc=\"Clustering\"):\n",
    "#             uf.union(doc1, doc2)\n",
    "\n",
    "#         clusters = uf.get_clusters()\n",
    "#         n_clusters = len(set(clusters.values()))\n",
    "\n",
    "#         total_time = time.time() - start_time\n",
    "#         print(f\"\\nCreated {n_clusters:,} clusters in {total_time:.2f}s total\")\n",
    "\n",
    "#         return clusters, self.all_similarities\n",
    "\n",
    "\n",
    "# def reconstruct_poems(df):\n",
    "#     poem_to_clusters = defaultdict(set)\n",
    "#     poem_verse_counts = defaultdict(int)\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         poem_id = row['idoriginal_poem']\n",
    "#         cluster_id = row['cluster_id']\n",
    "#         poem_verse_counts[poem_id] += 1\n",
    "#         if cluster_id != -1:\n",
    "#             poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "#     print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "#     return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "\n",
    "# def calculate_poem_cluster_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "#     if not clusters_a or not clusters_b:\n",
    "#         return 0.0\n",
    "#     intersection = len(clusters_a & clusters_b)\n",
    "#     union = len(clusters_a | clusters_b)\n",
    "#     return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "# def cluster_poems(poem_to_clusters: Dict, similarity_threshold: float = 0.60):\n",
    "#     poem_ids = list(poem_to_clusters.keys())\n",
    "#     n_poems = len(poem_ids)\n",
    "\n",
    "#     edges = []\n",
    "#     for i in range(n_poems):\n",
    "#         for j in range(i + 1, n_poems):\n",
    "#             poem_a = poem_ids[i]\n",
    "#             poem_b = poem_ids[j]\n",
    "#             similarity = calculate_poem_cluster_similarity(\n",
    "#                 poem_to_clusters[poem_a],\n",
    "#                 poem_to_clusters[poem_b]\n",
    "#             )\n",
    "#             if similarity >= similarity_threshold:\n",
    "#                 edges.append((poem_a, poem_b, similarity))\n",
    "\n",
    "#     class PoemUnionFind:\n",
    "#         def __init__(self, elements):\n",
    "#             self.parent = {e: e for e in elements}\n",
    "#             self.rank = {e: 0 for e in elements}\n",
    "\n",
    "#         def find(self, x):\n",
    "#             if self.parent[x] != x:\n",
    "#                 self.parent[x] = self.find(self.parent[x])\n",
    "#             return self.parent[x]\n",
    "\n",
    "#         def union(self, x, y):\n",
    "#             px, py = self.find(x), self.find(y)\n",
    "#             if px == py:\n",
    "#                 return\n",
    "#             if self.rank[px] < self.rank[py]:\n",
    "#                 px, py = py, px\n",
    "#             self.parent[py] = px\n",
    "#             if self.rank[px] == self.rank[py]:\n",
    "#                 self.rank[px] += 1\n",
    "\n",
    "#     uf = PoemUnionFind(poem_ids)\n",
    "#     for poem_a, poem_b, _ in edges:\n",
    "#         uf.union(poem_a, poem_b)\n",
    "\n",
    "#     poem_clusters = {poem_id: uf.find(poem_id) for poem_id in poem_ids}\n",
    "#     n_clusters = len(set(poem_clusters.values()))\n",
    "\n",
    "#     return poem_clusters, edges, n_clusters\n",
    "\n",
    "\n",
    "# def evaluate_clustering(y_true, y_pred):\n",
    "#     ari = adjusted_rand_score(y_true, y_pred)\n",
    "#     v_measure = v_measure_score(y_true, y_pred)\n",
    "#     return ari, v_measure\n",
    "\n",
    "\n",
    "# def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "#     poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "#     gt_to_poems = defaultdict(set)\n",
    "#     for poem_id, gt_type in poem_to_type.items():\n",
    "#         gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "#     pred_to_poems = defaultdict(set)\n",
    "#     for poem_id, pred_cluster in poem_clusters.items():\n",
    "#         pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "#     perfectly_reconstructed = 0\n",
    "#     total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "#     for gt_type, gt_poems in gt_to_poems.items():\n",
    "#         for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "#             if gt_poems == pred_poems:\n",
    "#                 perfectly_reconstructed += 1\n",
    "#                 break\n",
    "\n",
    "#     reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "#     return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "# def visualize_verse_grid_search(results_df, save_path='verse_grid_search_results.png'):\n",
    "#     # sns.set_palette(\"colorblind\")\n",
    "\n",
    "#     ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "#     vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "#     clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "#     similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "#     fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=18, fontweight='bold')\n",
    "\n",
    "#     col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "#     ax1 = axes[0, 0]\n",
    "#     sns.heatmap(ari_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax1,\n",
    "#                 cbar_kws={'label': 'ARI'}, xticklabels=col_labels)\n",
    "#     ax1.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "#     ax1.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "#     ax1.set_title('Adjusted Rand Index (ARI)', fontweight='bold', fontsize=13)\n",
    "\n",
    "#     ax2 = axes[0, 1]\n",
    "#     sns.heatmap(vmeasure_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax2,\n",
    "#                 cbar_kws={'label': 'V-measure'}, xticklabels=col_labels)\n",
    "#     ax2.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "#     ax2.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "#     ax2.set_title('V-measure', fontweight='bold', fontsize=13)\n",
    "\n",
    "#     ax3 = axes[1, 0]\n",
    "#     sns.heatmap(clusters_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax3,\n",
    "#                 cbar_kws={'label': 'Clusters'}, xticklabels=col_labels)\n",
    "#     ax3.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "#     ax3.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "#     ax3.set_title('Number of Clusters', fontweight='bold', fontsize=13)\n",
    "\n",
    "#     ax4 = axes[1, 1]\n",
    "#     sns.heatmap(similarities_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax4,\n",
    "#                 cbar_kws={'label': 'Similarities'}, xticklabels=col_labels)\n",
    "#     ax4.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "#     ax4.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "#     ax4.set_title('Number of Similarities Found', fontweight='bold', fontsize=13)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"\\nVisualization saved to: {save_path}\")\n",
    "#     plt.close()\n",
    "\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# import numpy as np\n",
    "\n",
    "# def visualize_poem_grid_search(results_df, save_path='poem_grid_search_results.png'):\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "#     fig.suptitle('Poem-Level Clustering Grid Search Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "#     thresholds = results_df['threshold'].values\n",
    "#     thresholds_pct = [f\"{t:.0%}\" for t in thresholds]\n",
    "\n",
    "#     # Normalize values for colormap\n",
    "#     def normalize(vals):\n",
    "#         return (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "\n",
    "#     # ---------- AX1: ARI ----------\n",
    "#     ax1 = axes[0, 0]\n",
    "#     norm_vals = normalize(results_df['ari'].values)\n",
    "#     colors = cm.viridis(norm_vals)\n",
    "#     ax1.plot(thresholds_pct, results_df['ari'].values, marker='o', linewidth=2, markersize=8)\n",
    "#     for i, (x, y) in enumerate(zip(thresholds_pct, results_df['ari'].values)):\n",
    "#         # ax1.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "#         ax1.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "#     ax1.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "#     ax1.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "#     ax1.set_title('ARI vs Threshold')\n",
    "#     ax1.grid(True, alpha=0.3)\n",
    "\n",
    "#     # ---------- AX2: V-measure ----------\n",
    "#     ax2 = axes[0, 1]\n",
    "#     norm_vals = normalize(results_df['v_measure'].values)\n",
    "#     colors = cm.viridis(norm_vals)\n",
    "#     ax2.plot(thresholds_pct, results_df['v_measure'].values, marker='o', linewidth=2, markersize=8)\n",
    "#     for i, (x, y) in enumerate(zip(thresholds_pct, results_df['v_measure'].values)):\n",
    "#         # ax2.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "#         ax2.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "#     ax2.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "#     ax2.set_ylabel('V-measure', fontweight='bold')\n",
    "#     ax2.set_title('V-measure vs Threshold')\n",
    "#     ax2.grid(True, alpha=0.3)\n",
    "\n",
    "#     # ---------- AX3: Perfect Reconstruction Rate ----------\n",
    "#     ax3 = axes[1, 0]\n",
    "#     prr_vals = results_df['perfect_reconstruction_rate'].values * 100\n",
    "#     norm_vals = normalize(prr_vals)\n",
    "#     colors = cm.viridis(norm_vals)\n",
    "#     ax3.plot(thresholds_pct, prr_vals, marker='o', linewidth=2, markersize=8)\n",
    "#     for i, (x, y) in enumerate(zip(thresholds_pct, prr_vals)):\n",
    "#         ax3.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "#         ax3.text(i, y, f'{y:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "#     ax3.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "#     ax3.set_ylabel('Perfect Reconstruction Rate (%)', fontweight='bold')\n",
    "#     ax3.set_title('Perfect Reconstruction Rate vs Threshold')\n",
    "#     ax3.grid(True, alpha=0.3)\n",
    "\n",
    "#     # ---------- AX4: Number of Clusters ----------\n",
    "#     ax4 = axes[1, 1]\n",
    "#     n_clusters_vals = results_df['n_clusters'].values\n",
    "#     norm_vals = normalize(n_clusters_vals)\n",
    "#     colors = cm.viridis(norm_vals)\n",
    "#     ax4.plot(thresholds_pct, n_clusters_vals, marker='o', linewidth=2, markersize=8)\n",
    "#     for i, (x, y) in enumerate(zip(thresholds_pct, n_clusters_vals)):\n",
    "#         ax4.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "#         ax4.text(i, y, f'{y}', ha='center', va='bottom', fontsize=9)\n",
    "#     ax4.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "#     ax4.set_ylabel('Number of Poem Clusters', fontweight='bold')\n",
    "#     ax4.set_title('Number of Clusters vs Threshold')\n",
    "#     ax4.grid(True, alpha=0.3)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     print(f\"\\nVisualization saved to: {save_path}\")\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# def verse_level_grid_search(texts, df, thresholds, shingle_sizes, num_perm=128):\n",
    "#     results = []\n",
    "#     best_ari = -1\n",
    "#     best_threshold = None\n",
    "#     best_shingle_size = None\n",
    "#     best_clusters = None\n",
    "#     best_similarities = None\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"VERSE-LEVEL 2D GRID SEARCH\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\")\n",
    "#     print(f\"Testing shingle sizes: {shingle_sizes}\\n\")\n",
    "\n",
    "#     total_combinations = len(thresholds) * len(shingle_sizes)\n",
    "#     print(f\"Total combinations: {total_combinations}\\n\")\n",
    "\n",
    "#     for shingle_size in shingle_sizes:\n",
    "#         for threshold in thresholds:\n",
    "#             print(f\"\\nTesting shingle_size={shingle_size}, threshold={threshold:.0%}...\")\n",
    "\n",
    "#             clusterer = FastMinHashClustering(\n",
    "#                 threshold=threshold,\n",
    "#                 shingle_size=shingle_size,\n",
    "#                 num_perm=num_perm,\n",
    "#                 chunk_size=1\n",
    "#             )\n",
    "\n",
    "#             clusters, similarities = clusterer.cluster(texts)\n",
    "\n",
    "#             if 'idgroup' in df.columns:\n",
    "#                 temp_df = df.copy()\n",
    "#                 temp_df['cluster_id'] = temp_df.index.map(clusters)\n",
    "\n",
    "#                 mask = temp_df['idgroup'].notna() & temp_df['cluster_id'].notna()\n",
    "#                 y_true = temp_df.loc[mask, 'idgroup'].tolist()\n",
    "#                 y_pred = temp_df.loc[mask, 'cluster_id'].tolist()\n",
    "\n",
    "#                 ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "#                 n_gt_clusters = len(set(y_true))\n",
    "#             else:\n",
    "#                 ari, v_measure = 0, 0\n",
    "#                 n_gt_clusters = 0\n",
    "\n",
    "#             n_clusters = len(set(clusters.values()))\n",
    "\n",
    "#             results.append({\n",
    "#                 'shingle_size': shingle_size,\n",
    "#                 'threshold': threshold,\n",
    "#                 'n_clusters': n_clusters,\n",
    "#                 'n_similarities': len(similarities),\n",
    "#                 'ari': ari,\n",
    "#                 'v_measure': v_measure,\n",
    "#                 'n_gt_clusters': n_gt_clusters\n",
    "#             })\n",
    "\n",
    "#             if ari > best_ari:\n",
    "#                 best_ari = ari\n",
    "#                 best_threshold = threshold\n",
    "#                 best_shingle_size = shingle_size\n",
    "#                 best_clusters = clusters\n",
    "#                 best_similarities = similarities\n",
    "\n",
    "#             print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"VERSE-LEVEL GRID SEARCH SUMMARY\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\n{'Shingle':<10} {'Threshold':<12} {'Clusters':<10} {'Similarities':<15} {'ARI':<8} {'V-measure':<12}\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "#     for _, result in results_df.iterrows():\n",
    "#         print(f\"{result['shingle_size']:<10} \"\n",
    "#               f\"{result['threshold']:<12.0%} \"\n",
    "#               f\"{result['n_clusters']:<10} \"\n",
    "#               f\"{result['n_similarities']:<15} \"\n",
    "#               f\"{result['ari']:<8.4f} \"\n",
    "#               f\"{result['v_measure']:<12.4f}\")\n",
    "\n",
    "#     print(f\"\\n{'='*100}\")\n",
    "#     print(\"BEST VERSE-LEVEL PARAMETERS\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\nBest parameters by ARI:\")\n",
    "#     print(f\"  Shingle size: {best_shingle_size}\")\n",
    "#     print(f\"  Threshold: {best_threshold:.0%}\")\n",
    "#     best_result = results_df[(results_df['threshold'] == best_threshold) &\n",
    "#                               (results_df['shingle_size'] == best_shingle_size)].iloc[0]\n",
    "#     print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "#     print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "#     print(f\"  Number of clusters: {best_result['n_clusters']}\")\n",
    "#     print(f\"  Number of similarities found: {best_result['n_similarities']}\")\n",
    "\n",
    "#     visualize_verse_grid_search(results_df)\n",
    "\n",
    "#     return best_clusters, best_similarities, best_threshold, best_shingle_size, results_df\n",
    "\n",
    "\n",
    "# def poem_level_grid_search(df, poem_to_clusters, thresholds):\n",
    "#     results = []\n",
    "#     best_ari = -1\n",
    "#     best_threshold = None\n",
    "#     best_poem_clusters = None\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"POEM-LEVEL GRID SEARCH\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\\n\")\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         print(f\"\\nTesting threshold {threshold:.0%}...\")\n",
    "\n",
    "#         poem_clusters, poem_edges, n_clusters = cluster_poems(poem_to_clusters, threshold)\n",
    "\n",
    "#         if 'type_id' in df.columns:\n",
    "#             poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "#             y_true = []\n",
    "#             y_pred = []\n",
    "#             for poem_id, predicted_cluster in poem_clusters.items():\n",
    "#                 if poem_id in poem_to_type:\n",
    "#                     y_true.append(poem_to_type[poem_id])\n",
    "#                     y_pred.append(predicted_cluster)\n",
    "\n",
    "#             ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "#             reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "#         else:\n",
    "#             ari, v_measure = 0, 0\n",
    "#             reconstruction_rate, n_perfect, n_total_gt = 0, 0, 0\n",
    "\n",
    "#         results.append({\n",
    "#             'threshold': threshold,\n",
    "#             'n_clusters': n_clusters,\n",
    "#             'n_edges': len(poem_edges),\n",
    "#             'ari': ari,\n",
    "#             'v_measure': v_measure,\n",
    "#             'perfect_reconstruction_rate': reconstruction_rate,\n",
    "#             'n_perfect_clusters': n_perfect,\n",
    "#             'n_total_gt_clusters': n_total_gt\n",
    "#         })\n",
    "\n",
    "#         if ari > best_ari:\n",
    "#             best_ari = ari\n",
    "#             best_threshold = threshold\n",
    "#             best_poem_clusters = poem_clusters\n",
    "\n",
    "#         print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "#     results_df = pd.DataFrame(results)\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"POEM-LEVEL GRID SEARCH SUMMARY\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\n{'Threshold':<12} {'Clusters':<10} {'Edges':<10} {'ARI':<8} {'V-measure':<12} {'Perfect Recon.':<15}\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "#     for _, result in results_df.iterrows():\n",
    "#         print(f\"{result['threshold']:<12.0%} \"\n",
    "#               f\"{result['n_clusters']:<10} \"\n",
    "#               f\"{result['n_edges']:<10} \"\n",
    "#               f\"{result['ari']:<8.4f} \"\n",
    "#               f\"{result['v_measure']:<12.4f} \"\n",
    "#               f\"{result['perfect_reconstruction_rate']:<15.1%}\")\n",
    "\n",
    "#     print(f\"\\n{'='*100}\")\n",
    "#     print(\"BEST POEM-LEVEL THRESHOLD\")\n",
    "#     print(\"=\"*100)\n",
    "#     print(f\"\\nBest threshold by ARI: {best_threshold:.0%}\")\n",
    "#     best_result = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "#     print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "#     print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "#     print(f\"  Perfect reconstruction rate: {best_result['perfect_reconstruction_rate']:.1%}\")\n",
    "#     print(f\"    ({best_result['n_perfect_clusters']:.0f}/{best_result['n_total_gt_clusters']:.0f} GT clusters perfectly reconstructed)\")\n",
    "\n",
    "#     visualize_poem_grid_search(results_df)\n",
    "\n",
    "#     return best_poem_clusters, best_threshold, results_df\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     DATA_FILE = 'dbbe_full.csv'\n",
    "\n",
    "#     print(\"=\"*100)\n",
    "#     print(\"LOADING DATA\")\n",
    "#     print(\"=\"*100)\n",
    "\n",
    "#     df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "#     if 'verse' in df.columns:\n",
    "#         df['text'] = df['verse']\n",
    "#     elif 'text' not in df.columns:\n",
    "#         raise ValueError(\"Dataset must have either 'verse' or 'text' column\")\n",
    "\n",
    "#     df['text'] = df['text'].fillna('').astype(str)\n",
    "#     print(f\"\\nLoaded {df.shape[0]:,} verses\")\n",
    "\n",
    "#     texts = df['text'].tolist()\n",
    "\n",
    "#     verse_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "#     shingle_sizes = [2, 3, 4, 5]\n",
    "\n",
    "#     best_clusters, best_similarities, best_verse_threshold, best_shingle_size, verse_results = verse_level_grid_search(\n",
    "#         texts, df, verse_thresholds, shingle_sizes, num_perm=128\n",
    "#     )\n",
    "\n",
    "#     df['cluster_id'] = df.index.map(best_clusters)\n",
    "\n",
    "#     sim_dict = defaultdict(list)\n",
    "#     for doc1, doc2, sim in best_similarities:\n",
    "#         sim_dict[doc1].append(sim)\n",
    "#         sim_dict[doc2].append(sim)\n",
    "\n",
    "#     df['certainty'] = df.index.map(\n",
    "#         lambda i: np.mean(sim_dict[i]) if i in sim_dict else 1.0\n",
    "#     )\n",
    "\n",
    "#     preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "#     df['text_preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "#     verse_output = \"dbbe_verse_clustered_results.csv\"\n",
    "#     df.to_csv(verse_output, index=False)\n",
    "#     print(f\"\\n{verse_output} saved with best parameters (shingle_size={best_shingle_size}, threshold={best_verse_threshold:.0%})\")\n",
    "\n",
    "#     if 'idoriginal_poem' in df.columns and 'type_id' in df.columns:\n",
    "#         poem_to_clusters, poem_verse_counts = reconstruct_poems(df)\n",
    "\n",
    "#         poem_thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "\n",
    "#         best_poem_clusters, best_poem_threshold, poem_results = poem_level_grid_search(\n",
    "#             df, poem_to_clusters, poem_thresholds\n",
    "#         )\n",
    "\n",
    "#         df['poem_cluster_id'] = df['idoriginal_poem'].map(best_poem_clusters)\n",
    "\n",
    "#         poem_output = \"dbbe_poem_level_clusters.csv\"\n",
    "#         df.to_csv(poem_output, index=False)\n",
    "#         print(f\"\\n{poem_output} saved with best threshold ({best_poem_threshold:.0%})\")\n",
    "#     else:\n",
    "#         print(\"\\n\" + \"=\"*100)\n",
    "#         print(\"SKIPPING POEM-LEVEL CLUSTERING\")\n",
    "#         print(\"=\"*100)\n",
    "#         print(\"\\nRequired columns 'idoriginal_poem' and/or 'type_id' not found\")\n",
    "\n",
    "#     print(\"\\n\" + \"=\"*100)\n",
    "#     print(\"ANALYSIS COMPLETE\")\n",
    "#     print(\"=\"*100)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"GPU detected - using CuPy acceleration\")\n",
    "except ImportError:\n",
    "    cp = np\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"No GPU - using NumPy (CPU mode)\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, lowercase=True, remove_punctuation=True, remove_diacritics=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_diacritics = remove_diacritics\n",
    "        if remove_punctuation:\n",
    "            self.punct_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "            self.remove_chars_pattern = re.compile(r'[\\(\\)\\{\\}]')\n",
    "\n",
    "    def _remove_diacritics(self, text: str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', text)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) if pd.notna(text) else ''\n",
    "\n",
    "        if self.remove_diacritics:\n",
    "            text = self._remove_diacritics(text)\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = self.remove_chars_pattern.sub('', text)\n",
    "            text = self.punct_pattern.sub(' ', text)\n",
    "\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.preprocess(t) for t in texts]\n",
    "\n",
    "\n",
    "class ShingleGenerator:\n",
    "    def __init__(self, shingle_size: int = 4, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.shingle_size = shingle_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def generate_shingles(self, text: str) -> np.ndarray:\n",
    "        if len(text) < self.shingle_size:\n",
    "            return np.array([hash(text) % (2**31)], dtype=np.int32)\n",
    "\n",
    "        chars = self.xp.array([ord(c) for c in text], dtype=np.int32)\n",
    "        n_shingles = len(text) - self.shingle_size + 1\n",
    "\n",
    "        shingles = self.xp.zeros(n_shingles, dtype=np.int32)\n",
    "        for i in range(self.shingle_size):\n",
    "            shingles += chars[i:i+n_shingles] * (31 ** i)\n",
    "\n",
    "        unique_shingles = self.xp.unique(shingles)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            unique_shingles = cp.asnumpy(unique_shingles)\n",
    "\n",
    "        return unique_shingles\n",
    "\n",
    "    def generate_batch(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        return [self.generate_shingles(t) for t in texts]\n",
    "\n",
    "\n",
    "class MinHashProcessor:\n",
    "    def __init__(self, num_perm: int = 128, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.num_perm = num_perm\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "        rng = self.xp.random.RandomState(42)\n",
    "        self.hash_a = rng.randint(1, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.hash_b = rng.randint(0, 2**31-1, num_perm, dtype=np.int64)\n",
    "        self.prime = np.int64(2**31-1)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            print(f\"Using GPU for MinHash ({num_perm} permutations)\")\n",
    "\n",
    "    def compute_signature(self, shingles: np.ndarray) -> np.ndarray:\n",
    "        if len(shingles) == 0:\n",
    "            return np.full(self.num_perm, self.prime, dtype=np.int64)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            shingles_gpu = self.xp.array(shingles, dtype=np.int64)\n",
    "        else:\n",
    "            shingles_gpu = shingles.astype(np.int64)\n",
    "\n",
    "        shingles_expanded = shingles_gpu[:, self.xp.newaxis]\n",
    "        hashes = (self.hash_a * shingles_expanded + self.hash_b) % self.prime\n",
    "        signature = self.xp.min(hashes, axis=0)\n",
    "\n",
    "        if self.use_gpu:\n",
    "            signature = cp.asnumpy(signature)\n",
    "\n",
    "        return signature\n",
    "\n",
    "    def compute_batch(self, shingles_batch: List[np.ndarray]) -> np.ndarray:\n",
    "        signatures = np.zeros((len(shingles_batch), self.num_perm), dtype=np.int64)\n",
    "        for i, shingles in enumerate(shingles_batch):\n",
    "            signatures[i] = self.compute_signature(shingles)\n",
    "        return signatures\n",
    "\n",
    "\n",
    "class LSHIndex:\n",
    "    def __init__(self, threshold: float = 0.3, num_perm: int = 128):\n",
    "        self.threshold = threshold\n",
    "        self.num_perm = num_perm\n",
    "        self.bands = 16\n",
    "        self.rows = num_perm // self.bands\n",
    "        self.signatures = []\n",
    "        self.num_docs = 0\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.bands)]\n",
    "\n",
    "    def _hash_band(self, band: np.ndarray) -> int:\n",
    "        return int(hash(tuple(band)) % (2**31))\n",
    "\n",
    "    def insert_batch(self, signatures: np.ndarray, start_idx: int):\n",
    "        batch_size = signatures.shape[0]\n",
    "        self.signatures.append(signatures)\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                global_doc_id = start_idx + doc_idx\n",
    "                self.hash_tables[band_idx][band_hash].append(global_doc_id)\n",
    "\n",
    "        self.num_docs += batch_size\n",
    "\n",
    "    def query_batch(self, signatures: np.ndarray, start_idx: int) -> List[set]:\n",
    "        batch_size = signatures.shape[0]\n",
    "        candidates = [set() for _ in range(batch_size)]\n",
    "\n",
    "        for band_idx in range(self.bands):\n",
    "            start_row = band_idx * self.rows\n",
    "            end_row = start_row + self.rows\n",
    "\n",
    "            for doc_idx in range(batch_size):\n",
    "                query_doc_id = start_idx + doc_idx\n",
    "                band = signatures[doc_idx, start_row:end_row]\n",
    "                band_hash = self._hash_band(band)\n",
    "                bucket = self.hash_tables[band_idx].get(band_hash, [])\n",
    "                candidates[doc_idx].update(c for c in bucket if c < query_doc_id)\n",
    "\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SimilarityComputer:\n",
    "    def __init__(self, threshold: float = 0.3, use_gpu: bool = GPU_AVAILABLE):\n",
    "        self.threshold = threshold\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "        self.xp = cp if self.use_gpu else np\n",
    "\n",
    "    def compute_batch_similarities(self, query_sig: np.ndarray,\n",
    "                                   candidate_sigs: np.ndarray) -> np.ndarray:\n",
    "        if self.use_gpu:\n",
    "            query_gpu = self.xp.array(query_sig)\n",
    "            cands_gpu = self.xp.array(candidate_sigs)\n",
    "            query_expanded = self.xp.tile(query_gpu, (len(candidate_sigs), 1))\n",
    "            matches = self.xp.sum(query_expanded == cands_gpu, axis=1)\n",
    "            sims = matches.astype(np.float32) / query_sig.shape[0]\n",
    "            return cp.asnumpy(sims)\n",
    "        else:\n",
    "            query_expanded = np.tile(query_sig, (len(candidate_sigs), 1))\n",
    "            matches = np.sum(query_expanded == candidate_sigs, axis=1)\n",
    "            return matches.astype(np.float32) / query_sig.shape[0]\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self, n: int):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x: int, y: int):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "\n",
    "    def get_clusters(self) -> Dict[int, int]:\n",
    "        return {i: self.find(i) for i in range(len(self.parent))}\n",
    "\n",
    "\n",
    "class FastMinHashClustering:\n",
    "    def __init__(self, threshold: float = 0.3, shingle_size: int = 4,\n",
    "                 num_perm: int = 128, chunk_size: int = 50000,\n",
    "                 use_gpu: Optional[bool] = None):\n",
    "\n",
    "        if use_gpu is None:\n",
    "            use_gpu = GPU_AVAILABLE\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.use_gpu = use_gpu and GPU_AVAILABLE\n",
    "\n",
    "        self.preprocessor = TextPreprocessor(\n",
    "            lowercase=True,\n",
    "            remove_punctuation=True,\n",
    "            remove_diacritics=True\n",
    "        )\n",
    "        self.shingler = ShingleGenerator(shingle_size, use_gpu)\n",
    "        self.minhash = MinHashProcessor(num_perm, use_gpu)\n",
    "        self.lsh_index = LSHIndex(threshold, num_perm)\n",
    "        self.similarity_computer = SimilarityComputer(threshold, use_gpu)\n",
    "        self.all_similarities = []\n",
    "\n",
    "        mode = \"GPU (CuPy)\" if self.use_gpu else \"CPU (NumPy)\"\n",
    "        print(f\"Initialized in {mode} mode\")\n",
    "\n",
    "    def cluster(self, texts: List[str]) -> Tuple[Dict[int, int], List[Tuple[int, int, float]]]:\n",
    "        n_docs = len(texts)\n",
    "        n_chunks = (n_docs + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        print(f\"\\nClustering {n_docs:,} documents in {n_chunks} chunks\")\n",
    "        print(f\"threshold={self.threshold}, chunk_size={self.chunk_size:,}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for chunk_idx in tqdm(range(n_chunks), desc=\"Processing\"):\n",
    "            chunk_start = chunk_idx * self.chunk_size\n",
    "            chunk_end = min(chunk_start + self.chunk_size, n_docs)\n",
    "            chunk_texts = texts[chunk_start:chunk_end]\n",
    "\n",
    "            processed = self.preprocessor.preprocess_batch(chunk_texts)\n",
    "            shingles = self.shingler.generate_batch(processed)\n",
    "            signatures = self.minhash.compute_batch(shingles)\n",
    "            self.lsh_index.insert_batch(signatures, chunk_start)\n",
    "\n",
    "            if chunk_start > 0:\n",
    "                candidates = self.lsh_index.query_batch(signatures, chunk_start)\n",
    "\n",
    "                for doc_idx, cand_set in enumerate(candidates):\n",
    "                    if not cand_set:\n",
    "                        continue\n",
    "\n",
    "                    query_doc_id = chunk_start + doc_idx\n",
    "                    query_sig = signatures[doc_idx]\n",
    "\n",
    "                    cand_list = sorted(cand_set)\n",
    "                    cand_sigs = []\n",
    "                    for cand_id in cand_list:\n",
    "                        batch_idx = cand_id // self.chunk_size\n",
    "                        local_idx = cand_id % self.chunk_size\n",
    "                        if batch_idx < len(self.lsh_index.signatures):\n",
    "                            cand_sigs.append(self.lsh_index.signatures[batch_idx][local_idx])\n",
    "\n",
    "                    if cand_sigs:\n",
    "                        cand_sigs = np.array(cand_sigs)\n",
    "                        sims = self.similarity_computer.compute_batch_similarities(\n",
    "                            query_sig, cand_sigs\n",
    "                        )\n",
    "\n",
    "                        for cand_id, sim in zip(cand_list[:len(sims)], sims):\n",
    "                            if sim >= self.threshold:\n",
    "                                self.all_similarities.append((cand_id, query_doc_id, float(sim)))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nFound {len(self.all_similarities):,} similarities in {elapsed:.2f}s\")\n",
    "        print(f\"Throughput: {n_docs/elapsed:,.0f} docs/sec\")\n",
    "\n",
    "        print(\"Building clusters...\")\n",
    "        uf = UnionFind(n_docs)\n",
    "        for doc1, doc2, _ in tqdm(self.all_similarities, desc=\"Clustering\"):\n",
    "            uf.union(doc1, doc2)\n",
    "\n",
    "        clusters = uf.get_clusters()\n",
    "        n_clusters = len(set(clusters.values()))\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nCreated {n_clusters:,} clusters in {total_time:.2f}s total\")\n",
    "\n",
    "        return clusters, self.all_similarities\n",
    "\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_id']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "\n",
    "def calculate_poem_cluster_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def cluster_poems(poem_to_clusters: Dict, similarity_threshold: float = 0.60):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            poem_a = poem_ids[i]\n",
    "            poem_b = poem_ids[j]\n",
    "            similarity = calculate_poem_cluster_similarity(\n",
    "                poem_to_clusters[poem_a],\n",
    "                poem_to_clusters[poem_b]\n",
    "            )\n",
    "            if similarity >= similarity_threshold:\n",
    "                edges.append((poem_a, poem_b, similarity))\n",
    "\n",
    "    class PoemUnionFind:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py:\n",
    "                return\n",
    "            if self.rank[px] < self.rank[py]:\n",
    "                px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]:\n",
    "                self.rank[px] += 1\n",
    "\n",
    "    uf = PoemUnionFind(poem_ids)\n",
    "    for poem_a, poem_b, _ in edges:\n",
    "        uf.union(poem_a, poem_b)\n",
    "\n",
    "    poem_clusters = {poem_id: uf.find(poem_id) for poem_id in poem_ids}\n",
    "    n_clusters = len(set(poem_clusters.values()))\n",
    "\n",
    "    return poem_clusters, edges, n_clusters\n",
    "\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "    return ari, v_measure\n",
    "\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "def visualize_verse_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'verse_grid_search_results.png'\n",
    "\n",
    "    ari_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='ari')\n",
    "    vmeasure_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='v_measure')\n",
    "    clusters_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_clusters')\n",
    "    similarities_pivot = results_df.pivot(index='shingle_size', columns='threshold', values='n_similarities')\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Verse-Level Clustering Grid Search Results', fontsize=18, fontweight='bold')\n",
    "\n",
    "    col_labels = [f\"{col:.0%}\" for col in ari_pivot.columns]\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.heatmap(ari_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax1,\n",
    "                cbar_kws={'label': 'ARI'}, xticklabels=col_labels)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('Adjusted Rand Index (ARI)', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    sns.heatmap(vmeasure_pivot, annot=True, fmt='.4f', cmap='viridis', ax=ax2,\n",
    "                cbar_kws={'label': 'V-measure'}, xticklabels=col_labels)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('V-measure', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.heatmap(clusters_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax3,\n",
    "                cbar_kws={'label': 'Clusters'}, xticklabels=col_labels)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax3.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax3.set_title('Number of Clusters', fontweight='bold', fontsize=13)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    sns.heatmap(similarities_pivot, annot=True, fmt='.0f', cmap='viridis', ax=ax4,\n",
    "                cbar_kws={'label': 'Similarities'}, xticklabels=col_labels)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold', fontsize=12)\n",
    "    ax4.set_ylabel('Shingle Size', fontweight='bold', fontsize=12)\n",
    "    ax4.set_title('Number of Similarities Found', fontweight='bold', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_poem_grid_search(results_df, save_path=None):\n",
    "    if save_path is None:\n",
    "        save_path = RESULTS_DIR / 'poem_grid_search_results.png'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Poem-Level Clustering Grid Search Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "    thresholds = results_df['threshold'].values\n",
    "    thresholds_pct = [f\"{t:.0%}\" for t in thresholds]\n",
    "\n",
    "    def normalize(vals):\n",
    "        return (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    norm_vals = normalize(results_df['ari'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax1.plot(thresholds_pct, results_df['ari'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['ari'].values)):\n",
    "        ax1.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax1.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax1.set_ylabel('Adjusted Rand Index (ARI)', fontweight='bold')\n",
    "    ax1.set_title('ARI vs Threshold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    norm_vals = normalize(results_df['v_measure'].values)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax2.plot(thresholds_pct, results_df['v_measure'].values, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, results_df['v_measure'].values)):\n",
    "        ax2.text(i, y, f'{y:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax2.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax2.set_ylabel('V-measure', fontweight='bold')\n",
    "    ax2.set_title('V-measure vs Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    prr_vals = results_df['perfect_reconstruction_rate'].values * 100\n",
    "    norm_vals = normalize(prr_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax3.plot(thresholds_pct, prr_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, prr_vals)):\n",
    "        ax3.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax3.text(i, y, f'{y:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    ax3.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax3.set_ylabel('Perfect Reconstruction Rate (%)', fontweight='bold')\n",
    "    ax3.set_title('Perfect Reconstruction Rate vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    n_clusters_vals = results_df['n_clusters'].values\n",
    "    norm_vals = normalize(n_clusters_vals)\n",
    "    colors = plt.cm.viridis(norm_vals)\n",
    "    ax4.plot(thresholds_pct, n_clusters_vals, marker='o', linewidth=2, markersize=8)\n",
    "    for i, (x, y) in enumerate(zip(thresholds_pct, n_clusters_vals)):\n",
    "        ax4.plot(x, y, marker='o', color=colors[i], markersize=10)\n",
    "        ax4.text(i, y, f'{y}', ha='center', va='bottom', fontsize=9)\n",
    "    ax4.set_xlabel('Similarity Threshold', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Poem Clusters', fontweight='bold')\n",
    "    ax4.set_title('Number of Clusters vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def verse_level_grid_search(texts, df, thresholds, shingle_sizes, num_perm=128):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_shingle_size = None\n",
    "    best_clusters = None\n",
    "    best_similarities = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL 2D GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\")\n",
    "    print(f\"Testing shingle sizes: {shingle_sizes}\\n\")\n",
    "\n",
    "    total_combinations = len(thresholds) * len(shingle_sizes)\n",
    "    print(f\"Total combinations: {total_combinations}\\n\")\n",
    "\n",
    "    for shingle_size in shingle_sizes:\n",
    "        for threshold in thresholds:\n",
    "            print(f\"\\nTesting shingle_size={shingle_size}, threshold={threshold:.0%}...\")\n",
    "\n",
    "            clusterer = FastMinHashClustering(\n",
    "                threshold=threshold,\n",
    "                shingle_size=shingle_size,\n",
    "                num_perm=num_perm,\n",
    "                chunk_size=1\n",
    "            )\n",
    "\n",
    "            clusters, similarities = clusterer.cluster(texts)\n",
    "\n",
    "            if 'idgroup' in df.columns:\n",
    "                temp_df = df.copy()\n",
    "                temp_df['cluster_id'] = temp_df.index.map(clusters)\n",
    "\n",
    "                mask = temp_df['idgroup'].notna() & temp_df['cluster_id'].notna()\n",
    "                y_true = temp_df.loc[mask, 'idgroup'].tolist()\n",
    "                y_pred = temp_df.loc[mask, 'cluster_id'].tolist()\n",
    "\n",
    "                ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "                n_gt_clusters = len(set(y_true))\n",
    "            else:\n",
    "                ari, v_measure = 0, 0\n",
    "                n_gt_clusters = 0\n",
    "\n",
    "            n_clusters = len(set(clusters.values()))\n",
    "\n",
    "            results.append({\n",
    "                'shingle_size': shingle_size,\n",
    "                'threshold': threshold,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_similarities': len(similarities),\n",
    "                'ari': ari,\n",
    "                'v_measure': v_measure,\n",
    "                'n_gt_clusters': n_gt_clusters\n",
    "            })\n",
    "\n",
    "            if ari > best_ari:\n",
    "                best_ari = ari\n",
    "                best_threshold = threshold\n",
    "                best_shingle_size = shingle_size\n",
    "                best_clusters = clusters\n",
    "                best_similarities = similarities\n",
    "\n",
    "            print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"VERSE-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Shingle':<10} {'Threshold':<12} {'Clusters':<10} {'Similarities':<15} {'ARI':<8} {'V-measure':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['shingle_size']:<10} \"\n",
    "              f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_similarities']:<15} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST VERSE-LEVEL PARAMETERS\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest parameters by ARI:\")\n",
    "    print(f\"  Shingle size: {best_shingle_size}\")\n",
    "    print(f\"  Threshold: {best_threshold:.0%}\")\n",
    "    best_result = results_df[(results_df['threshold'] == best_threshold) &\n",
    "                              (results_df['shingle_size'] == best_shingle_size)].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Number of clusters: {best_result['n_clusters']}\")\n",
    "    print(f\"  Number of similarities found: {best_result['n_similarities']}\")\n",
    "\n",
    "    visualize_verse_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'verse_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nVerse grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_clusters, best_similarities, best_threshold, best_shingle_size, results_df\n",
    "\n",
    "\n",
    "def poem_level_grid_search(df, poem_to_clusters, thresholds):\n",
    "    results = []\n",
    "    best_ari = -1\n",
    "    best_threshold = None\n",
    "    best_poem_clusters = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nTesting thresholds: {[f'{t:.0%}' for t in thresholds]}\\n\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nTesting threshold {threshold:.0%}...\")\n",
    "\n",
    "        poem_clusters, poem_edges, n_clusters = cluster_poems(poem_to_clusters, threshold)\n",
    "\n",
    "        if 'type_id' in df.columns:\n",
    "            poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for poem_id, predicted_cluster in poem_clusters.items():\n",
    "                if poem_id in poem_to_type:\n",
    "                    y_true.append(poem_to_type[poem_id])\n",
    "                    y_pred.append(predicted_cluster)\n",
    "\n",
    "            ari, v_measure = evaluate_clustering(y_true, y_pred)\n",
    "            reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "        else:\n",
    "            ari, v_measure = 0, 0\n",
    "            reconstruction_rate, n_perfect, n_total_gt = 0, 0, 0\n",
    "\n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_edges': len(poem_edges),\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'perfect_reconstruction_rate': reconstruction_rate,\n",
    "            'n_perfect_clusters': n_perfect,\n",
    "            'n_total_gt_clusters': n_total_gt\n",
    "        })\n",
    "\n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_threshold = threshold\n",
    "            best_poem_clusters = poem_clusters\n",
    "\n",
    "        print(f\"  ARI: {ari:.4f}, V-measure: {v_measure:.4f}, Clusters: {n_clusters}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"POEM-LEVEL GRID SEARCH SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\n{'Threshold':<12} {'Clusters':<10} {'Edges':<10} {'ARI':<8} {'V-measure':<12} {'Perfect Recon.':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for _, result in results_df.iterrows():\n",
    "        print(f\"{result['threshold']:<12.0%} \"\n",
    "              f\"{result['n_clusters']:<10} \"\n",
    "              f\"{result['n_edges']:<10} \"\n",
    "              f\"{result['ari']:<8.4f} \"\n",
    "              f\"{result['v_measure']:<12.4f} \"\n",
    "              f\"{result['perfect_reconstruction_rate']:<15.1%}\")\n",
    "\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BEST POEM-LEVEL THRESHOLD\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nBest threshold by ARI: {best_threshold:.0%}\")\n",
    "    best_result = results_df[results_df['threshold'] == best_threshold].iloc[0]\n",
    "    print(f\"  ARI: {best_result['ari']:.4f}\")\n",
    "    print(f\"  V-measure: {best_result['v_measure']:.4f}\")\n",
    "    print(f\"  Perfect reconstruction rate: {best_result['perfect_reconstruction_rate']:.1%}\")\n",
    "    print(f\"    ({best_result['n_perfect_clusters']:.0f}/{best_result['n_total_gt_clusters']:.0f} GT clusters perfectly reconstructed)\")\n",
    "\n",
    "    visualize_poem_grid_search(results_df)\n",
    "\n",
    "    results_csv = RESULTS_DIR / 'poem_grid_search_results.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nPoem grid search results saved to: {results_csv}\")\n",
    "\n",
    "    return best_poem_clusters, best_threshold, results_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    DATA_FILE = 'dbbe_full.csv'\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    if 'verse' in df.columns:\n",
    "        df['text'] = df['verse']\n",
    "    elif 'text' not in df.columns:\n",
    "        raise ValueError(\"Dataset must have either 'verse' or 'text' column\")\n",
    "\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    print(f\"\\nLoaded {df.shape[0]:,} verses\")\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    verse_thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    shingle_sizes = [2, 3, 4, 5]\n",
    "\n",
    "    best_clusters, best_similarities, best_verse_threshold, best_shingle_size, verse_results = verse_level_grid_search(\n",
    "        texts, df, verse_thresholds, shingle_sizes, num_perm=128\n",
    "    )\n",
    "\n",
    "    df['cluster_id'] = df.index.map(best_clusters)\n",
    "\n",
    "    sim_dict = defaultdict(list)\n",
    "    for doc1, doc2, sim in best_similarities:\n",
    "        sim_dict[doc1].append(sim)\n",
    "        sim_dict[doc2].append(sim)\n",
    "\n",
    "    df['certainty'] = df.index.map(\n",
    "        lambda i: np.mean(sim_dict[i]) if i in sim_dict else 1.0\n",
    "    )\n",
    "\n",
    "    preprocessor = TextPreprocessor(lowercase=True, remove_punctuation=True, remove_diacritics=True)\n",
    "    df['text_preprocessed'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "    verse_output = RESULTS_DIR / \"dbbe_verse_clustered_results.csv\"\n",
    "    df.to_csv(verse_output, index=False)\n",
    "    print(f\"\\n{verse_output} saved with best parameters (shingle_size={best_shingle_size}, threshold={best_verse_threshold:.0%})\")\n",
    "\n",
    "    if 'idoriginal_poem' in df.columns and 'type_id' in df.columns:\n",
    "        poem_to_clusters, poem_verse_counts = reconstruct_poems(df)\n",
    "\n",
    "        poem_thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "\n",
    "        best_poem_clusters, best_poem_threshold, poem_results = poem_level_grid_search(\n",
    "            df, poem_to_clusters, poem_thresholds\n",
    "        )\n",
    "\n",
    "        df['poem_cluster_id'] = df['idoriginal_poem'].map(best_poem_clusters)\n",
    "\n",
    "        poem_output = RESULTS_DIR / \"dbbe_poem_level_clusters.csv\"\n",
    "        df.to_csv(poem_output, index=False)\n",
    "        print(f\"\\n{poem_output} saved with best threshold ({best_poem_threshold:.0%})\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"SKIPPING POEM-LEVEL CLUSTERING\")\n",
    "        print(\"=\"*100)\n",
    "        print(\"\\nRequired columns 'idoriginal_poem' and/or 'type_id' not found\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385f19c1-f9ed-47c2-862b-b9219d2dc934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "INTERESTING CLUSTER IDENTIFICATION & ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "Loaded 54,170 verses\n",
      "Total clusters: 25,057\n",
      "\n",
      "====================================================================================================\n",
      "CLUSTER STATISTICS\n",
      "====================================================================================================\n",
      "✓ Cluster statistics saved to: dbbe_orthographic_results/interesting_clusters_analysis/cluster_statistics.csv\n",
      "\n",
      "====================================================================================================\n",
      "INTERESTING CLUSTER CATEGORIES\n",
      "====================================================================================================\n",
      "\n",
      "1. CROSS-DATASET CLUSTERS: 0\n",
      "   (Verses appearing in multiple datasets - potential formulaic expressions)\n",
      "\n",
      "2. LARGE CLUSTERS (≥5 verses): 1756\n",
      "   (Frequently repeated verses - common formulaic patterns)\n",
      "\n",
      "   Top 10 by size:\n",
      "   Cluster 459: 350 verses\n",
      "   Cluster 7389: 297 verses\n",
      "   Cluster 1065: 245 verses\n",
      "   Cluster 1901: 226 verses\n",
      "   Cluster 62: 207 verses\n",
      "   Cluster 454: 197 verses\n",
      "   Cluster 2315: 177 verses\n",
      "   Cluster 3246: 176 verses\n",
      "   Cluster 2439: 175 verses\n",
      "   Cluster 2880: 174 verses\n",
      "\n",
      "3. CROSS-TYPE CLUSTERS: 822\n",
      "   (Verses shared across different poem types - borrowing/influence)\n",
      "\n",
      "   Top 10 by size:\n",
      "   Cluster 459: 350 verses across 250 types\n",
      "   Cluster 7389: 297 verses across 30 types\n",
      "   Cluster 1065: 245 verses across 37 types\n",
      "   Cluster 1901: 226 verses across 25 types\n",
      "   Cluster 62: 207 verses across 23 types\n",
      "   Cluster 454: 197 verses across 35 types\n",
      "   Cluster 2315: 177 verses across 3 types\n",
      "   Cluster 3246: 176 verses across 2 types\n",
      "   Cluster 2439: 175 verses across 2 types\n",
      "   Cluster 2880: 174 verses across 2 types\n",
      "\n",
      "4. HIGH-CERTAINTY CLUSTERS: 3454\n",
      "   (Very similar verses - exact/near-exact repetitions)\n",
      "\n",
      "   Top 10 by certainty:\n",
      "   Cluster 12830: 3 verses, certainty=1.000\n",
      "   Cluster 41297: 6 verses, certainty=1.000\n",
      "   Cluster 11515: 3 verses, certainty=1.000\n",
      "   Cluster 11536: 6 verses, certainty=1.000\n",
      "   Cluster 11550: 6 verses, certainty=1.000\n",
      "   Cluster 3963: 6 verses, certainty=1.000\n",
      "   Cluster 3933: 6 verses, certainty=1.000\n",
      "   Cluster 34276: 3 verses, certainty=1.000\n",
      "   Cluster 3926: 6 verses, certainty=1.000\n",
      "   Cluster 11681: 3 verses, certainty=1.000\n",
      "\n",
      "5. MEDIUM-SIZED CLUSTERS (3-10 verses): 3326\n",
      "   (Moderately repeated - good for detailed case studies)\n",
      "\n",
      "====================================================================================================\n",
      "GENERATING DETAILED REPORTS FOR TOP CLUSTERS\n",
      "====================================================================================================\n",
      "\n",
      "Generating detailed reports for top 5 clusters in each category...\n",
      "\n",
      "Category: LARGE\n",
      "  1. Cluster 459 → dbbe_orthographic_results/interesting_clusters_analysis/large/cluster_459_report.txt\n",
      "  2. Cluster 7389 → dbbe_orthographic_results/interesting_clusters_analysis/large/cluster_7389_report.txt\n",
      "  3. Cluster 1065 → dbbe_orthographic_results/interesting_clusters_analysis/large/cluster_1065_report.txt\n",
      "  4. Cluster 1901 → dbbe_orthographic_results/interesting_clusters_analysis/large/cluster_1901_report.txt\n",
      "  5. Cluster 62 → dbbe_orthographic_results/interesting_clusters_analysis/large/cluster_62_report.txt\n",
      "Category: CROSS_TYPE\n",
      "  1. Cluster 459 → dbbe_orthographic_results/interesting_clusters_analysis/cross_type/cluster_459_report.txt\n",
      "  2. Cluster 7389 → dbbe_orthographic_results/interesting_clusters_analysis/cross_type/cluster_7389_report.txt\n",
      "  3. Cluster 1065 → dbbe_orthographic_results/interesting_clusters_analysis/cross_type/cluster_1065_report.txt\n",
      "  4. Cluster 1901 → dbbe_orthographic_results/interesting_clusters_analysis/cross_type/cluster_1901_report.txt\n",
      "  5. Cluster 62 → dbbe_orthographic_results/interesting_clusters_analysis/cross_type/cluster_62_report.txt\n",
      "Category: HIGH_CERTAINTY\n",
      "  1. Cluster 12830 → dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/cluster_12830_report.txt\n",
      "  2. Cluster 41297 → dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/cluster_41297_report.txt\n",
      "  3. Cluster 11515 → dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/cluster_11515_report.txt\n",
      "  4. Cluster 11536 → dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/cluster_11536_report.txt\n",
      "  5. Cluster 11550 → dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/cluster_11550_report.txt\n",
      "Category: MEDIUM\n",
      "  1. Cluster 5 → dbbe_orthographic_results/interesting_clusters_analysis/medium/cluster_5_report.txt\n",
      "  2. Cluster 7 → dbbe_orthographic_results/interesting_clusters_analysis/medium/cluster_7_report.txt\n",
      "  3. Cluster 8 → dbbe_orthographic_results/interesting_clusters_analysis/medium/cluster_8_report.txt\n",
      "  4. Cluster 1052 → dbbe_orthographic_results/interesting_clusters_analysis/medium/cluster_1052_report.txt\n",
      "  5. Cluster 19 → dbbe_orthographic_results/interesting_clusters_analysis/medium/cluster_19_report.txt\n",
      "\n",
      "====================================================================================================\n",
      "CREATING VISUALIZATIONS\n",
      "====================================================================================================\n",
      "\n",
      "✓ Overview visualization saved: dbbe_orthographic_results/interesting_clusters_analysis/cluster_analysis_overview.png\n",
      "\n",
      "====================================================================================================\n",
      "CREATING SUMMARY TABLES\n",
      "====================================================================================================\n",
      "✓ Summary table saved: dbbe_orthographic_results/interesting_clusters_analysis/interesting_clusters_summary.txt\n",
      "\n",
      "====================================================================================================\n",
      "RECOMMENDATIONS FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "Based on the analysis, here are recommendations for selecting clusters to discuss:\n",
      "\n",
      "1. **CROSS-DATASET CLUSTERS** (for formulaic language analysis):\n",
      "   - Select 2-3 large cross-dataset clusters\n",
      "   - These show verses that appear in multiple manuscripts/datasets\n",
      "   - Good for discussing transmission and formulaic tradition\n",
      "\n",
      "2. **HIGH-CERTAINTY CLUSTERS** (for exact repetitions):\n",
      "   - Select 1-2 high-certainty, medium-sized clusters\n",
      "   - These show near-exact repetitions\n",
      "   - Good for discussing memorization and standardization\n",
      "\n",
      "3. **CROSS-TYPE CLUSTERS** (for borrowing/influence):\n",
      "   - Select 1-2 clusters that span different poem types\n",
      "   - These show inter-textual relationships\n",
      "   - Good for discussing literary borrowing\n",
      "\n",
      "4. **CASE STUDIES** (for detailed analysis):\n",
      "   - Select 2-3 medium-sized clusters (3-10 verses)\n",
      "   - Not too large (overwhelming) or too small (not interesting)\n",
      "   - Good for showing actual verse texts and variations\n",
      "\n",
      "5. **ANOMALIES** (for interesting exceptions):\n",
      "   - Look for clusters with unusual characteristics\n",
      "   - E.g., very long verses, very short verses, unexpected type combinations\n",
      "\n",
      "Check the detailed reports in the subdirectories for specific clusters to include.\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "✅ ANALYSIS COMPLETE\n",
      "====================================================================================================\n",
      "\n",
      "All results saved to: dbbe_orthographic_results/interesting_clusters_analysis\n",
      "\n",
      "Next steps:\n",
      "1. Review cluster statistics: dbbe_orthographic_results/interesting_clusters_analysis/cluster_statistics.csv\n",
      "2. Review summary table: dbbe_orthographic_results/interesting_clusters_analysis/interesting_clusters_summary.txt\n",
      "3. Check detailed reports in subdirectories:\n",
      "   - dbbe_orthographic_results/interesting_clusters_analysis/large/\n",
      "   - dbbe_orthographic_results/interesting_clusters_analysis/cross_type/\n",
      "   - dbbe_orthographic_results/interesting_clusters_analysis/high_certainty/\n",
      "   - dbbe_orthographic_results/interesting_clusters_analysis/medium/\n",
      "4. Use the visualizations in your paper: dbbe_orthographic_results/interesting_clusters_analysis/cluster_analysis_overview.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RESULTS_DIR = Path(\"dbbe_orthographic_results\")\n",
    "ANALYSIS_DIR = RESULTS_DIR / \"interesting_clusters_analysis\"\n",
    "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"INTERESTING CLUSTER IDENTIFICATION & ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "verse_file = RESULTS_DIR / \"dbbe_verse_clustered_results.csv\"\n",
    "df = pd.read_csv(verse_file)\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} verses\")\n",
    "print(f\"Total clusters: {df['cluster_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CLUSTER STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "cluster_stats = []\n",
    "for cluster_id in df['cluster_id'].unique():\n",
    "    cluster_df = df[df['cluster_id'] == cluster_id]\n",
    "    \n",
    "    n_verses = len(cluster_df)\n",
    "    \n",
    "    if 'source_dataset' in df.columns:\n",
    "        datasets = cluster_df['source_dataset'].unique()\n",
    "        n_datasets = len(datasets)\n",
    "        dataset_list = list(datasets)\n",
    "    else:\n",
    "        n_datasets = 1\n",
    "        dataset_list = ['unknown']\n",
    "    \n",
    "    if 'idoriginal_poem' in df.columns:\n",
    "        n_poems = cluster_df['idoriginal_poem'].nunique()\n",
    "    else:\n",
    "        n_poems = 0\n",
    "    \n",
    "    if 'type_id' in df.columns:\n",
    "        n_types = cluster_df['type_id'].nunique()\n",
    "        type_list = cluster_df['type_id'].unique().tolist()\n",
    "    else:\n",
    "        n_types = 0\n",
    "        type_list = []\n",
    "    \n",
    "    avg_certainty = cluster_df['certainty'].mean() if 'certainty' in df.columns else 1.0\n",
    "    \n",
    "    avg_verse_length = cluster_df['text'].str.len().mean()\n",
    "    \n",
    "    cluster_stats.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'n_verses': n_verses,\n",
    "        'n_datasets': n_datasets,\n",
    "        'n_poems': n_poems,\n",
    "        'n_types': n_types,\n",
    "        'avg_certainty': avg_certainty,\n",
    "        'avg_verse_length': avg_verse_length,\n",
    "        'datasets': ','.join(map(str, dataset_list)),\n",
    "        'types': ','.join(map(str, type_list))\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "stats_csv = ANALYSIS_DIR / \"cluster_statistics.csv\"\n",
    "stats_df.to_csv(stats_csv, index=False)\n",
    "print(f\"✓ Cluster statistics saved to: {stats_csv}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"INTERESTING CLUSTER CATEGORIES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "interesting_clusters = defaultdict(list)\n",
    "\n",
    "cross_dataset_clusters = stats_df[stats_df['n_datasets'] > 1].sort_values('n_verses', ascending=False)\n",
    "print(f\"\\n1. CROSS-DATASET CLUSTERS: {len(cross_dataset_clusters)}\")\n",
    "print(\"   (Verses appearing in multiple datasets - potential formulaic expressions)\")\n",
    "if len(cross_dataset_clusters) > 0:\n",
    "    top_cross = cross_dataset_clusters.head(10)\n",
    "    print(f\"\\n   Top 10 by size:\")\n",
    "    for _, row in top_cross.iterrows():\n",
    "        print(f\"   Cluster {row['cluster_id']}: {row['n_verses']} verses across {row['n_datasets']} datasets\")\n",
    "    interesting_clusters['cross_dataset'] = cross_dataset_clusters['cluster_id'].tolist()\n",
    "\n",
    "large_clusters = stats_df[stats_df['n_verses'] >= 5].sort_values('n_verses', ascending=False)\n",
    "print(f\"\\n2. LARGE CLUSTERS (≥5 verses): {len(large_clusters)}\")\n",
    "print(\"   (Frequently repeated verses - common formulaic patterns)\")\n",
    "if len(large_clusters) > 0:\n",
    "    top_large = large_clusters.head(10)\n",
    "    print(f\"\\n   Top 10 by size:\")\n",
    "    for _, row in top_large.iterrows():\n",
    "        print(f\"   Cluster {row['cluster_id']}: {row['n_verses']} verses\")\n",
    "    interesting_clusters['large'] = large_clusters['cluster_id'].tolist()\n",
    "\n",
    "if 'n_types' in stats_df.columns:\n",
    "    cross_type_clusters = stats_df[(stats_df['n_types'] > 1) & (stats_df['n_verses'] >= 3)].sort_values('n_verses', ascending=False)\n",
    "    print(f\"\\n3. CROSS-TYPE CLUSTERS: {len(cross_type_clusters)}\")\n",
    "    print(\"   (Verses shared across different poem types - borrowing/influence)\")\n",
    "    if len(cross_type_clusters) > 0:\n",
    "        top_cross_type = cross_type_clusters.head(10)\n",
    "        print(f\"\\n   Top 10 by size:\")\n",
    "        for _, row in top_cross_type.iterrows():\n",
    "            print(f\"   Cluster {row['cluster_id']}: {row['n_verses']} verses across {row['n_types']} types\")\n",
    "        interesting_clusters['cross_type'] = cross_type_clusters['cluster_id'].tolist()\n",
    "\n",
    "high_certainty_clusters = stats_df[(stats_df['avg_certainty'] >= 0.8) & (stats_df['n_verses'] >= 3)].sort_values('avg_certainty', ascending=False)\n",
    "print(f\"\\n4. HIGH-CERTAINTY CLUSTERS: {len(high_certainty_clusters)}\")\n",
    "print(\"   (Very similar verses - exact/near-exact repetitions)\")\n",
    "if len(high_certainty_clusters) > 0:\n",
    "    top_certainty = high_certainty_clusters.head(10)\n",
    "    print(f\"\\n   Top 10 by certainty:\")\n",
    "    for _, row in top_certainty.iterrows():\n",
    "        print(f\"   Cluster {row['cluster_id']}: {row['n_verses']} verses, certainty={row['avg_certainty']:.3f}\")\n",
    "    interesting_clusters['high_certainty'] = high_certainty_clusters['cluster_id'].tolist()\n",
    "\n",
    "medium_clusters = stats_df[(stats_df['n_verses'] >= 3) & (stats_df['n_verses'] <= 10)]\n",
    "print(f\"\\n5. MEDIUM-SIZED CLUSTERS (3-10 verses): {len(medium_clusters)}\")\n",
    "print(\"   (Moderately repeated - good for detailed case studies)\")\n",
    "if len(medium_clusters) > 0:\n",
    "    interesting_clusters['medium'] = medium_clusters['cluster_id'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GENERATING DETAILED REPORTS FOR TOP CLUSTERS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "def generate_cluster_report(cluster_id, df, output_file):\n",
    "    cluster_df = df[df['cluster_id'] == cluster_id].copy()\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*100 + \"\\n\")\n",
    "        f.write(f\"CLUSTER {cluster_id} - DETAILED REPORT\\n\")\n",
    "        f.write(\"=\"*100 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Number of verses: {len(cluster_df)}\\n\")\n",
    "        \n",
    "        if 'source_dataset' in cluster_df.columns:\n",
    "            f.write(f\"Datasets: {cluster_df['source_dataset'].unique().tolist()}\\n\")\n",
    "            f.write(f\"Dataset distribution:\\n\")\n",
    "            for dataset, count in cluster_df['source_dataset'].value_counts().items():\n",
    "                f.write(f\"  - {dataset}: {count} verses\\n\")\n",
    "        \n",
    "        if 'idoriginal_poem' in cluster_df.columns:\n",
    "            f.write(f\"\\nNumber of poems: {cluster_df['idoriginal_poem'].nunique()}\\n\")\n",
    "        \n",
    "        if 'type_id' in cluster_df.columns:\n",
    "            f.write(f\"Poem types: {cluster_df['type_id'].unique().tolist()}\\n\")\n",
    "        \n",
    "        if 'certainty' in cluster_df.columns:\n",
    "            f.write(f\"\\nAverage certainty: {cluster_df['certainty'].mean():.3f}\\n\")\n",
    "            f.write(f\"Certainty range: {cluster_df['certainty'].min():.3f} - {cluster_df['certainty'].max():.3f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nAverage verse length: {cluster_df['text'].str.len().mean():.1f} characters\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "        f.write(\"ALL VERSES IN CLUSTER\\n\")\n",
    "        f.write(\"=\"*100 + \"\\n\\n\")\n",
    "        \n",
    "        for idx, row in cluster_df.iterrows():\n",
    "            f.write(f\"Verse {idx}:\\n\")\n",
    "            f.write(f\"  Text: {row['text']}\\n\")\n",
    "            if 'text_preprocessed' in row:\n",
    "                f.write(f\"  Preprocessed: {row['text_preprocessed']}\\n\")\n",
    "            if 'source_dataset' in row:\n",
    "                f.write(f\"  Dataset: {row['source_dataset']}\\n\")\n",
    "            if 'idoriginal_poem' in row:\n",
    "                f.write(f\"  Poem ID: {row['idoriginal_poem']}\\n\")\n",
    "            if 'type_id' in row:\n",
    "                f.write(f\"  Type: {row['type_id']}\\n\")\n",
    "            if 'certainty' in row:\n",
    "                f.write(f\"  Certainty: {row['certainty']:.3f}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if 'text_preprocessed' in cluster_df.columns:\n",
    "            f.write(\"=\"*100 + \"\\n\")\n",
    "            f.write(\"UNIQUE PREPROCESSED FORMS\\n\")\n",
    "            f.write(\"=\"*100 + \"\\n\\n\")\n",
    "            unique_forms = cluster_df['text_preprocessed'].value_counts()\n",
    "            for form, count in unique_forms.items():\n",
    "                f.write(f\"{form} (appears {count} times)\\n\\n\")\n",
    "\n",
    "n_reports = 5\n",
    "print(f\"\\nGenerating detailed reports for top {n_reports} clusters in each category...\\n\")\n",
    "\n",
    "for category, cluster_ids in interesting_clusters.items():\n",
    "    category_dir = ANALYSIS_DIR / category\n",
    "    category_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    top_clusters = cluster_ids[:n_reports]\n",
    "    \n",
    "    print(f\"Category: {category.upper()}\")\n",
    "    for i, cluster_id in enumerate(top_clusters, 1):\n",
    "        output_file = category_dir / f\"cluster_{cluster_id}_report.txt\"\n",
    "        generate_cluster_report(cluster_id, df, output_file)\n",
    "        print(f\"  {i}. Cluster {cluster_id} → {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Cluster Analysis Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "cluster_size_dist = stats_df['n_verses'].value_counts().sort_index()\n",
    "ax1.bar(cluster_size_dist.index, cluster_size_dist.values, color='#0173B2', alpha=0.7)\n",
    "ax1.set_xlabel('Cluster Size (# verses)', fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontweight='bold')\n",
    "ax1.set_title('Cluster Size Distribution')\n",
    "ax1.set_xlim(0, min(50, cluster_size_dist.index.max()))\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "if 'n_datasets' in stats_df.columns:\n",
    "    dataset_dist = stats_df['n_datasets'].value_counts().sort_index()\n",
    "    ax2.bar(dataset_dist.index, dataset_dist.values, color='#ECB01F', alpha=0.7)\n",
    "    ax2.set_xlabel('Number of Datasets', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Clusters', fontweight='bold')\n",
    "    ax2.set_title('Cross-Dataset Distribution')\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "if 'avg_certainty' in stats_df.columns:\n",
    "    ax3.hist(stats_df['avg_certainty'], bins=30, color='#029E73', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(stats_df['avg_certainty'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {stats_df[\"avg_certainty\"].mean():.3f}')\n",
    "    ax3.set_xlabel('Average Certainty', fontweight='bold')\n",
    "    ax3.set_ylabel('Number of Clusters', fontweight='bold')\n",
    "    ax3.set_title('Certainty Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(alpha=0.3)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "top_20 = stats_df.nlargest(20, 'n_verses')\n",
    "ax4.barh(range(len(top_20)), top_20['n_verses'].values, color='#CC0000', alpha=0.7)\n",
    "ax4.set_yticks(range(len(top_20)))\n",
    "ax4.set_yticklabels([f\"Cluster {cid}\" for cid in top_20['cluster_id'].values], fontsize=8)\n",
    "ax4.set_xlabel('Number of Verses', fontweight='bold')\n",
    "ax4.set_title('Top 20 Largest Clusters')\n",
    "ax4.invert_yaxis()\n",
    "ax4.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "overview_plot = ANALYSIS_DIR / \"cluster_analysis_overview.png\"\n",
    "plt.savefig(overview_plot, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"\\n✓ Overview visualization saved: {overview_plot}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CREATING SUMMARY TABLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "summary_file = ANALYSIS_DIR / \"interesting_clusters_summary.txt\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"INTERESTING CLUSTERS SUMMARY\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    for category, cluster_ids in interesting_clusters.items():\n",
    "        f.write(f\"\\n{category.upper().replace('_', ' ')} ({len(cluster_ids)} clusters)\\n\")\n",
    "        f.write(\"-\"*100 + \"\\n\")\n",
    "        \n",
    "        category_stats = stats_df[stats_df['cluster_id'].isin(cluster_ids[:20])]\n",
    "        f.write(f\"{'Cluster ID':<12} {'Verses':<8} {'Datasets':<10} {'Poems':<8} {'Types':<8} {'Certainty':<10}\\n\")\n",
    "        f.write(\"-\"*100 + \"\\n\")\n",
    "        \n",
    "        for _, row in category_stats.iterrows():\n",
    "            f.write(f\"{row['cluster_id']:<12} \"\n",
    "                   f\"{row['n_verses']:<8} \"\n",
    "                   f\"{row['n_datasets']:<10} \"\n",
    "                   f\"{row['n_poems']:<8} \"\n",
    "                   f\"{row['n_types']:<8} \"\n",
    "                   f\"{row['avg_certainty']:<10.3f}\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427e076-d1ec-4420-9e45-cf76a4467eb9",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202cb092-1fd2-4525-9209-0d455630b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682239/2073978938.py:234: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"concatenated.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verses: 1,537,740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing: 100%|██████████| 1537740/1537740 [00:03<00:00, 439811.80it/s]\n",
      "MinHash: 100%|██████████| 769/769 [02:30<00:00,  5.10it/s]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 154/154 [15:18<00:00,  5.96s/it]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19.1 min\n",
      "Clusters: 58,593\n",
      "Max size: 1083331\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "import multiprocessing as mp\n",
    "import hashlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class FastUnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            self.parent[px] = py\n",
    "        elif self.rank[px] > self.rank[py]:\n",
    "            self.parent[py] = px\n",
    "        else:\n",
    "            self.parent[py] = px\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        return np.array([self.find(i) for i in range(len(self.parent))], dtype=np.int32)\n",
    "\n",
    "def get_ngrams_vectorized(text, n=4):\n",
    "    if not text or len(text) < n:\n",
    "        return set()\n",
    "    text = str(text).lower()\n",
    "    return set(text[i:i+n] for i in range(len(text)-n+1))\n",
    "\n",
    "def compute_minhash_chunk(args):\n",
    "    texts, start_idx, n_gram_size, num_perm, seed = args\n",
    "    np.random.seed(seed)\n",
    "    minhashes = []\n",
    "    for text in texts:\n",
    "        ngrams = get_ngrams_vectorized(text, n_gram_size)\n",
    "        m = MinHash(num_perm=num_perm, seed=seed)\n",
    "        if ngrams:\n",
    "            for ngram in ngrams:\n",
    "                m.update(ngram.encode('utf8'))\n",
    "        minhashes.append(m)\n",
    "    return minhashes\n",
    "\n",
    "def compute_minhash_parallel(texts, n_gram_size=3, num_perm=128, n_cores=None):\n",
    "    if n_cores is None:\n",
    "        n_cores = min(mp.cpu_count(), 16)\n",
    "    chunk_size = max(100, min(2000, len(texts) // (n_cores * 2)))\n",
    "    chunks = [(texts[i:i+chunk_size], i, n_gram_size, num_perm, 42) \n",
    "              for i in range(0, len(texts), chunk_size)]\n",
    "    with mp.Pool(n_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(compute_minhash_chunk, chunks),\n",
    "                          total=len(chunks), desc=\"MinHash\"))\n",
    "    minhashes = [mh for chunk_mhs in results for mh in chunk_mhs]\n",
    "    return minhashes\n",
    "\n",
    "def fast_hash(data):\n",
    "    return int(hashlib.md5(data).hexdigest()[:16], 16)\n",
    "\n",
    "def find_exact_duplicates_fast(texts):\n",
    "    text_hashes = {}\n",
    "    for i, text in enumerate(tqdm(texts, desc=\"Hashing\", disable=len(texts)<100000)):\n",
    "        normalized = str(text).strip().lower()\n",
    "        if not normalized:\n",
    "            continue\n",
    "        text_hash = fast_hash(normalized.encode('utf-8'))\n",
    "        text_hashes.setdefault(text_hash, []).append(i)\n",
    "    duplicate_groups = [indices for indices in text_hashes.values() if len(indices) > 1]\n",
    "    return duplicate_groups\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "        sample_indices.extend(sampled)\n",
    "    \n",
    "    return sorted(sample_indices)\n",
    "\n",
    "def quick_threshold_selection(texts, minhashes, df, n_sample=15000):\n",
    "    sample_indices = stratified_sample(df, n_sample)\n",
    "    sample_size = len(sample_indices)\n",
    "    \n",
    "    forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "    for idx in tqdm(sample_indices, desc=\"Indexing sample\", leave=False):\n",
    "        forest.add(str(idx), minhashes[idx])\n",
    "    forest.index()\n",
    "    \n",
    "    k = 20\n",
    "    similarities = []\n",
    "    for idx in tqdm(sample_indices, desc=\"Querying sample\", leave=False):\n",
    "        neighbors = forest.query(minhashes[idx], k)\n",
    "        for neighbor_idx_str in neighbors[1:]:\n",
    "            neighbor_idx = int(neighbor_idx_str)\n",
    "            sim = minhashes[idx].jaccard(minhashes[neighbor_idx])\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    if np.percentile(similarities, 95) > 0.5:\n",
    "        threshold = np.percentile(similarities, 85)\n",
    "    elif np.percentile(similarities, 95) > 0.3:\n",
    "        threshold = np.percentile(similarities, 80)\n",
    "    else:\n",
    "        threshold = np.percentile(similarities, 75)\n",
    "    \n",
    "    threshold = max(0.3, min(0.85, threshold))\n",
    "\n",
    "    \n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax1.hist(similarities, bins=50, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(threshold, color='#CC0000', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.3f}')\n",
    "    ax1.set_xlabel('Jaccard Similarity', fontweight='bold')\n",
    "    ax1.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax1.set_title('Similarity Distribution', fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    n_above = np.sum(similarities >= threshold)\n",
    "    pct_above = (n_above / len(similarities)) * 100\n",
    "    \n",
    "    ax2.hist(similarities, bins=50, color='#0173B2', alpha=0.7, edgecolor='black', cumulative=True, density=True)\n",
    "    ax2.axvline(threshold, color='#CC0000', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.3f}')\n",
    "    ax2.axhline(0.85, color='gray', linestyle=':', alpha=0.5, label='85th percentile')\n",
    "    ax2.set_xlabel('Jaccard Similarity', fontweight='bold')\n",
    "    ax2.set_ylabel('Cumulative Probability', fontweight='bold')\n",
    "    ax2.set_title('Cumulative Distribution', fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f'Threshold Selection (n={len(similarities):,} pairs, {pct_above:.1f}% kept)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('threshold_selection.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return threshold\n",
    "\n",
    "def cluster_with_lsh_forest(minhashes, duplicate_groups, threshold, top_k=100):\n",
    "    n_docs = len(minhashes)\n",
    "    uf = FastUnionFind(n_docs)\n",
    "    \n",
    "    exact_merges = 0\n",
    "    for group in duplicate_groups:\n",
    "        for i in range(1, len(group)):\n",
    "            if uf.union(group[0], group[i]):\n",
    "                exact_merges += 1\n",
    "    \n",
    "    forest = MinHashLSHForest(num_perm=len(minhashes[0].hashvalues))\n",
    "    for idx, mh in enumerate(tqdm(minhashes, desc=\"Indexing\", leave=False)):\n",
    "        forest.add(str(idx), mh)\n",
    "    forest.index()\n",
    "    \n",
    "    lsh_merges = 0\n",
    "    verified_pairs = 0\n",
    "    chunk_size = 10000\n",
    "    \n",
    "    for start_idx in tqdm(range(0, n_docs, chunk_size), desc=\"Clustering\"):\n",
    "        end_idx = min(start_idx + chunk_size, n_docs)\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            if uf.find(idx) != idx:\n",
    "                continue\n",
    "            neighbors = forest.query(minhashes[idx], top_k)\n",
    "            for neighbor_str in neighbors[1:]:\n",
    "                neighbor_idx = int(neighbor_str)\n",
    "                if uf.find(idx) == uf.find(neighbor_idx):\n",
    "                    continue\n",
    "                verified_pairs += 1\n",
    "                sim = minhashes[idx].jaccard(minhashes[neighbor_idx])\n",
    "                if sim >= threshold:\n",
    "                    if uf.union(idx, neighbor_idx):\n",
    "                        lsh_merges += 1\n",
    "    \n",
    "    cluster_labels = uf.get_clusters()\n",
    "    unique_clusters, cluster_sizes = np.unique(cluster_labels, return_counts=True)\n",
    "    \n",
    "    return cluster_labels, {\n",
    "        'n_clusters': len(unique_clusters),\n",
    "        'n_multi_clusters': np.sum(cluster_sizes > 1),\n",
    "        'n_singletons': np.sum(cluster_sizes == 1),\n",
    "        'avg_cluster_size': float(cluster_sizes.mean()),\n",
    "        'max_cluster_size': int(cluster_sizes.max()),\n",
    "        'exact_merges': exact_merges,\n",
    "        'lsh_merges': lsh_merges,\n",
    "        'threshold': threshold,\n",
    "        'verified_pairs': verified_pairs\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\"concatenated.csv\")\n",
    "    df = df[df['source_dataset'].isin(['rhoby', 'dbbe', 'phi', 'papyri'])]\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "    df['verse'] = df['verse'].apply(preprocess_text)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[df['verse'].str.strip().str.lower() != 'nan']\n",
    "    texts = df['verse'].fillna('').astype(str).tolist()\n",
    "    \n",
    "    print(f\"Verses: {len(texts):,}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    duplicate_groups = find_exact_duplicates_fast(texts)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    minhashes = compute_minhash_parallel(texts, n_gram_size=3, num_perm=128)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    threshold = quick_threshold_selection(texts, minhashes, df, n_sample=15000)\n",
    "    t3 = time.time()\n",
    "    print(f\"Threshold: {threshold:.3f}\")\n",
    "    \n",
    "    cluster_labels, metrics = cluster_with_lsh_forest(\n",
    "        minhashes, duplicate_groups, threshold, top_k=100\n",
    "    )\n",
    "    t4 = time.time()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    df['cluster_id'] = cluster_labels\n",
    "    df.to_csv(\"clustered_ultrafast.csv\", index=False)\n",
    "    \n",
    "    metrics['total_time_minutes'] = total_time / 60\n",
    "    pd.DataFrame([metrics]).to_csv(\"clustering_metrics.csv\", index=False)\n",
    "    \n",
    "    print(f\"Time: {total_time/60:.1f} min\")\n",
    "    print(f\"Clusters: {metrics['n_multi_clusters']:,}\")\n",
    "    print(f\"Max size: {metrics['max_cluster_size']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6aaf9fb-1200-4433-a3ed-003a5c5cd56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3364275/1821844082.py:339: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"full_verse_level_ortho.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,537,740 verses\n",
      "\n",
      "Using UltraFastPoemClusterer (size-filtered)\n",
      "Reconstructing poems...\n",
      "Found 164,665 poems\n",
      "Building inverted index...\n",
      "Found 310,769 clusters\n",
      "Finding candidate pairs with size filter...\n",
      "Processing 34,480 clusters\n",
      "Max size ratio filter: 2.67x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building candidates:   0%|          | 0/34480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warning: Large cluster 565542 with 148947 poems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building candidates:   0%|          | 7/34480 [00:00<28:25, 20.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warning: Large cluster 624 with 15275 poems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building candidates: 100%|██████████| 34480/34480 [00:00<00:00, 35282.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 474,862 pairs by size\n",
      "Found 839,749 candidate pairs\n",
      "Computing similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering: 100%|██████████| 839749/839749 [00:03<00:00, 252998.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 10,810 poem pairs\n",
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "Time:                  0.1 minutes\n",
      "Poem clusters:         153,855\n",
      "Candidate pairs:       10,810\n",
      "Cluster size mean:     1.07\n",
      "Cluster size median:   1\n",
      "Cluster size max:      957\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from numba import njit\n",
    "import gc\n",
    "\n",
    "class FastPoemClusterer:\n",
    "    def __init__(self, threshold: float = 0.6):\n",
    "        self.threshold = threshold\n",
    "        self.poem_clusters = {}\n",
    "        self.similarities = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def reconstruct_poems_vectorized(df):\n",
    "        \"\"\"Vectorized poem reconstruction - 10x faster\"\"\"\n",
    "        # Filter out unclustered verses once\n",
    "        valid_mask = df['cluster_id'] != -1\n",
    "        df_valid = df[valid_mask]\n",
    "        \n",
    "        # Group by poem efficiently\n",
    "        grouped = df_valid.groupby('idoriginal_poem')['cluster_id'].apply(set)\n",
    "        return grouped.to_dict()\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_inverted_index_fast(poem_to_clusters):\n",
    "        \"\"\"Fast inverted index with list comprehension\"\"\"\n",
    "        cluster_to_poems = defaultdict(set)\n",
    "        \n",
    "        # Flatten and build in one pass\n",
    "        for poem_id, clusters in poem_to_clusters.items():\n",
    "            for c in clusters:\n",
    "                cluster_to_poems[c].add(poem_id)\n",
    "        \n",
    "        return cluster_to_poems\n",
    "    \n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def jaccard_numba(a_arr, b_arr):\n",
    "        \"\"\"Ultra-fast Jaccard using Numba\"\"\"\n",
    "        intersection = 0\n",
    "        a_set = set(a_arr)\n",
    "        b_set = set(b_arr)\n",
    "        \n",
    "        for item in a_set:\n",
    "            if item in b_set:\n",
    "                intersection += 1\n",
    "        \n",
    "        union = len(a_set) + len(b_set) - intersection\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        return intersection / union\n",
    "    \n",
    "    def find_candidate_pairs_optimized(self, poem_to_clusters, cluster_to_poems):\n",
    "        \"\"\"\n",
    "        Optimized candidate generation:\n",
    "        - Skip clusters with only 1 poem\n",
    "        - Pre-allocate set\n",
    "        - Use batch processing\n",
    "        \"\"\"\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        # Filter out singleton clusters immediately\n",
    "        multi_poem_clusters = {\n",
    "            c: poems for c, poems in cluster_to_poems.items() \n",
    "            if len(poems) > 1\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(multi_poem_clusters):,} clusters (filtered from {len(cluster_to_poems):,})\")\n",
    "        \n",
    "        # Batch process large clusters differently\n",
    "        small_threshold = 1000\n",
    "        \n",
    "        for cluster_id, poems in tqdm(multi_poem_clusters.items(), desc=\"Building candidates\"):\n",
    "            poems_list = list(poems)\n",
    "            n_poems = len(poems_list)\n",
    "            \n",
    "            if n_poems <= small_threshold:\n",
    "                # Small cluster: all pairs\n",
    "                for i in range(n_poems):\n",
    "                    for j in range(i + 1, n_poems):\n",
    "                        candidate_pairs.add(tuple(sorted((poems_list[i], poems_list[j]))))\n",
    "            else:\n",
    "                # Large cluster: sample or warn\n",
    "                print(f\"  Warning: Large cluster {cluster_id} with {n_poems} poems - sampling pairs\")\n",
    "                # Sample pairs from large clusters to avoid memory explosion\n",
    "                sample_size = min(10000, n_poems * (n_poems - 1) // 2)\n",
    "                indices = np.random.choice(n_poems, size=min(2000, n_poems), replace=False)\n",
    "                for i in range(len(indices)):\n",
    "                    for j in range(i + 1, len(indices)):\n",
    "                        candidate_pairs.add(tuple(sorted((poems_list[indices[i]], poems_list[indices[j]]))))\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def cluster_poems_fast(self, df):\n",
    "        \"\"\"\n",
    "        Fast poem clustering with multiple optimizations:\n",
    "        1. Vectorized data loading\n",
    "        2. Filtered candidate generation\n",
    "        3. Numpy-based similarity computation\n",
    "        4. Optimized Union-Find\n",
    "        \"\"\"\n",
    "        print(\"Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        print(\"Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"Found {len(cluster_to_poems):,} clusters\")\n",
    "        \n",
    "        print(\"Finding candidate pairs...\")\n",
    "        candidate_pairs = self.find_candidate_pairs_optimized(poem_to_clusters, cluster_to_poems)\n",
    "        print(f\"Found {len(candidate_pairs):,} candidate pairs\")\n",
    "        \n",
    "        # Convert sets to sorted arrays for faster Jaccard\n",
    "        print(\"Converting to arrays...\")\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(clusters), dtype=np.int32) \n",
    "            for p, clusters in poem_to_clusters.items()\n",
    "        }\n",
    "        \n",
    "        # Optimized Union-Find with path compression and union by rank\n",
    "        poem_ids = list(poem_to_clusters.keys())\n",
    "        parent = {p: p for p in poem_ids}\n",
    "        rank = {p: 0 for p in poem_ids}\n",
    "        \n",
    "        def find(x):\n",
    "            if parent[x] != x:\n",
    "                parent[x] = find(parent[x])  # Path compression\n",
    "            return parent[x]\n",
    "        \n",
    "        def union(x, y):\n",
    "            px, py = find(x), find(y)\n",
    "            if px == py:\n",
    "                return False\n",
    "            \n",
    "            # Union by rank\n",
    "            if rank[px] < rank[py]:\n",
    "                px, py = py, px\n",
    "            parent[py] = px\n",
    "            if rank[px] == rank[py]:\n",
    "                rank[px] += 1\n",
    "            return True\n",
    "        \n",
    "        # Batch process similarities\n",
    "        print(\"Computing similarities and clustering...\")\n",
    "        similarities = []\n",
    "        merges = 0\n",
    "        \n",
    "        # Process in batches for better memory management\n",
    "        batch_size = 100000\n",
    "        pairs_list = list(candidate_pairs)\n",
    "        \n",
    "        for batch_start in tqdm(range(0, len(pairs_list), batch_size), desc=\"Processing batches\"):\n",
    "            batch_end = min(batch_start + batch_size, len(pairs_list))\n",
    "            batch_pairs = pairs_list[batch_start:batch_end]\n",
    "            \n",
    "            for p1, p2 in batch_pairs:\n",
    "                # Skip if already in same cluster\n",
    "                if find(p1) == find(p2):\n",
    "                    continue\n",
    "                \n",
    "                # Compute Jaccard with Numba-accelerated function\n",
    "                sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "                \n",
    "                if sim >= self.threshold:\n",
    "                    if union(p1, p2):\n",
    "                        merges += 1\n",
    "                    similarities.append((p1, p2, sim))\n",
    "            \n",
    "            # Periodic garbage collection\n",
    "            if batch_start % (batch_size * 5) == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"Merged {merges:,} poem pairs\")\n",
    "        \n",
    "        # Final cluster assignment\n",
    "        self.poem_clusters = {p: find(p) for p in poem_ids}\n",
    "        self.similarities = similarities\n",
    "        \n",
    "        return self.poem_clusters, self.similarities\n",
    "    \n",
    "    def cluster_sizes(self):\n",
    "        \"\"\"Return distribution of poem cluster sizes\"\"\"\n",
    "        cluster_count = defaultdict(int)\n",
    "        for cluster in self.poem_clusters.values():\n",
    "            cluster_count[cluster] += 1\n",
    "        sizes = np.array(list(cluster_count.values()))\n",
    "        return sizes\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EVEN FASTER: Pre-filter with size heuristic\n",
    "# =========================\n",
    "class UltraFastPoemClusterer(FastPoemClusterer):\n",
    "    \"\"\"\n",
    "    Additional optimization: Pre-filter candidate pairs by size difference.\n",
    "    If two poems have very different numbers of verse clusters,\n",
    "    they can't have high Jaccard similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_candidate_pairs_with_size_filter(self, poem_to_clusters, cluster_to_poems):\n",
    "        \"\"\"\n",
    "        Size-aware candidate filtering:\n",
    "        - Only consider pairs where size ratio is within reasonable bounds\n",
    "        - For threshold=0.6, max size ratio is ~2.5x\n",
    "        \"\"\"\n",
    "        # Calculate size bounds based on threshold\n",
    "        # For Jaccard >= threshold, max size ratio is (1+threshold)/threshold\n",
    "        max_ratio = (1 + self.threshold) / self.threshold\n",
    "        \n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        # Get poem sizes\n",
    "        poem_sizes = {p: len(clusters) for p, clusters in poem_to_clusters.items()}\n",
    "        \n",
    "        # Filter singleton clusters\n",
    "        multi_poem_clusters = {\n",
    "            c: poems for c, poems in cluster_to_poems.items() \n",
    "            if len(poems) > 1\n",
    "        }\n",
    "        \n",
    "        print(f\"Processing {len(multi_poem_clusters):,} clusters\")\n",
    "        print(f\"Max size ratio filter: {max_ratio:.2f}x\")\n",
    "        \n",
    "        filtered_by_size = 0\n",
    "        \n",
    "        for cluster_id, poems in tqdm(multi_poem_clusters.items(), desc=\"Building candidates\"):\n",
    "            poems_list = list(poems)\n",
    "            n_poems = len(poems_list)\n",
    "            \n",
    "            if n_poems > 1000:\n",
    "                print(f\"  Warning: Large cluster {cluster_id} with {n_poems} poems\")\n",
    "                # Sample from large clusters\n",
    "                poems_list = list(np.random.choice(poems_list, size=min(1000, n_poems), replace=False))\n",
    "                n_poems = len(poems_list)\n",
    "            \n",
    "            for i in range(n_poems):\n",
    "                p1 = poems_list[i]\n",
    "                size1 = poem_sizes[p1]\n",
    "                \n",
    "                for j in range(i + 1, n_poems):\n",
    "                    p2 = poems_list[j]\n",
    "                    size2 = poem_sizes[p2]\n",
    "                    \n",
    "                    # Size filter: skip if sizes too different\n",
    "                    ratio = max(size1, size2) / max(min(size1, size2), 1)\n",
    "                    if ratio > max_ratio:\n",
    "                        filtered_by_size += 1\n",
    "                        continue\n",
    "                    \n",
    "                    candidate_pairs.add(tuple(sorted((p1, p2))))\n",
    "        \n",
    "        print(f\"Filtered {filtered_by_size:,} pairs by size\")\n",
    "        \n",
    "        return candidate_pairs\n",
    "    \n",
    "    def cluster_poems_ultrafast(self, df):\n",
    "        \"\"\"Ultra-fast clustering with size-based pre-filtering\"\"\"\n",
    "        print(\"Reconstructing poems...\")\n",
    "        poem_to_clusters = self.reconstruct_poems_vectorized(df)\n",
    "        print(f\"Found {len(poem_to_clusters):,} poems\")\n",
    "        \n",
    "        print(\"Building inverted index...\")\n",
    "        cluster_to_poems = self.build_inverted_index_fast(poem_to_clusters)\n",
    "        print(f\"Found {len(cluster_to_poems):,} clusters\")\n",
    "        \n",
    "        print(\"Finding candidate pairs with size filter...\")\n",
    "        candidate_pairs = self.find_candidate_pairs_with_size_filter(\n",
    "            poem_to_clusters, cluster_to_poems\n",
    "        )\n",
    "        print(f\"Found {len(candidate_pairs):,} candidate pairs\")\n",
    "        \n",
    "        # Use the fast clustering from parent class\n",
    "        return self._cluster_with_pairs(poem_to_clusters, candidate_pairs)\n",
    "    \n",
    "    def _cluster_with_pairs(self, poem_to_clusters, candidate_pairs):\n",
    "        \"\"\"Internal method to cluster given candidate pairs\"\"\"\n",
    "        # Convert to arrays\n",
    "        poem_to_array = {\n",
    "            p: np.array(sorted(clusters), dtype=np.int32) \n",
    "            for p, clusters in poem_to_clusters.items()\n",
    "        }\n",
    "        \n",
    "        # Union-Find\n",
    "        poem_ids = list(poem_to_clusters.keys())\n",
    "        parent = {p: p for p in poem_ids}\n",
    "        rank = {p: 0 for p in poem_ids}\n",
    "        \n",
    "        def find(x):\n",
    "            if parent[x] != x:\n",
    "                parent[x] = find(parent[x])\n",
    "            return parent[x]\n",
    "        \n",
    "        def union(x, y):\n",
    "            px, py = find(x), find(y)\n",
    "            if px == py:\n",
    "                return False\n",
    "            if rank[px] < rank[py]:\n",
    "                px, py = py, px\n",
    "            parent[py] = px\n",
    "            if rank[px] == rank[py]:\n",
    "                rank[px] += 1\n",
    "            return True\n",
    "        \n",
    "        # Compute similarities\n",
    "        print(\"Computing similarities...\")\n",
    "        similarities = []\n",
    "        merges = 0\n",
    "        \n",
    "        pairs_list = list(candidate_pairs)\n",
    "        \n",
    "        for p1, p2 in tqdm(pairs_list, desc=\"Clustering\"):\n",
    "            if find(p1) == find(p2):\n",
    "                continue\n",
    "            \n",
    "            sim = self.jaccard_numba(poem_to_array[p1], poem_to_array[p2])\n",
    "            \n",
    "            if sim >= self.threshold:\n",
    "                if union(p1, p2):\n",
    "                    merges += 1\n",
    "                similarities.append((p1, p2, sim))\n",
    "        \n",
    "        print(f\"Merged {merges:,} poem pairs\")\n",
    "        \n",
    "        self.poem_clusters = {p: find(p) for p in poem_ids}\n",
    "        self.similarities = similarities\n",
    "        \n",
    "        return self.poem_clusters, self.similarities\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USAGE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\"full_verse_level_ortho.csv\")\n",
    "    df['cluster_id'] = pd.to_numeric(df['cluster_id'], errors='coerce').fillna(-1).astype(int)\n",
    "    df['idoriginal_poem'] = df['idoriginal_poem'].astype(str)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} verses\")\n",
    "    \n",
    "    # Choose clusterer based on data size\n",
    "    n_unique_poems = df['idoriginal_poem'].nunique()\n",
    "    \n",
    "    if n_unique_poems > 100000:\n",
    "        print(\"\\nUsing UltraFastPoemClusterer (size-filtered)\")\n",
    "        clusterer = UltraFastPoemClusterer(threshold=0.6)\n",
    "        start = time.time()\n",
    "        poem_clusters, similarities = clusterer.cluster_poems_ultrafast(df)\n",
    "    else:\n",
    "        print(\"\\nUsing FastPoemClusterer\")\n",
    "        clusterer = FastPoemClusterer(threshold=0.6)\n",
    "        start = time.time()\n",
    "        poem_clusters, similarities = clusterer.cluster_poems_fast(df)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    sizes = clusterer.cluster_sizes()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Time:                  {elapsed/60:.1f} minutes\")\n",
    "    print(f\"Poem clusters:         {len(set(poem_clusters.values())):,}\")\n",
    "    print(f\"Candidate pairs:       {len(similarities):,}\")\n",
    "    print(f\"Cluster size mean:     {sizes.mean():.2f}\")\n",
    "    print(f\"Cluster size median:   {np.median(sizes):.0f}\")\n",
    "    print(f\"Cluster size max:      {sizes.max()}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ab11d-593c-4388-9ba2-05bda355ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Invert the mapping: cluster_id -> list of poem_ids\n",
    "clusters_dict = defaultdict(list)\n",
    "for poem_id, cluster_id in poem_clusters.items():\n",
    "    clusters_dict[cluster_id].append(poem_id)\n",
    "\n",
    "for cluster_id, poems in list(clusters_dict.items())[:3]:\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    verses = df[df['idoriginal_poem'].isin(poems)]['verse'].tolist()\n",
    "    for v in verses[:5]:  # show first 5 verses in cluster\n",
    "        print(f\"  - {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231d7fd4-3f83-4667-9be0-5c12af6584bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Average similarity in first 5 clusters\n",
      "  Cluster 28467: avg similarity 0.750\n",
      "  Cluster 396556: avg similarity 1.000\n",
      "  Cluster 178269: avg similarity 0.803\n",
      "  Cluster 835623: avg similarity 0.789\n",
      "  Cluster 19956: avg similarity 0.766\n"
     ]
    }
   ],
   "source": [
    "# Average Jaccard similarity per cluster\n",
    "cluster_sim_avgs = defaultdict(list)\n",
    "for p1, p2, sim in similarities:\n",
    "    cluster_id = poem_clusters[p1]\n",
    "    cluster_sim_avgs[cluster_id].append(sim)\n",
    "\n",
    "avg_sim_per_cluster = {cid: np.mean(sims) for cid, sims in cluster_sim_avgs.items()}\n",
    "print(f\"Example: Average similarity in first 5 clusters\")\n",
    "for cid, avg in list(avg_sim_per_cluster.items())[:5]:\n",
    "    print(f\"  Cluster {cid}: avg similarity {avg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bb076-85d6-4fcc-99fc-8461e5137f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter clusters with more than one poem AND at least two different sources\n",
    "multi_source_clusters = {}\n",
    "for cid, poems in multi_poem_clusters.items():\n",
    "    sources = {poem_sources.get(p, 'unknown') for p in poems}\n",
    "    if len(sources) > 1:\n",
    "        multi_source_clusters[cid] = poems\n",
    "\n",
    "for cluster_id, poems in multi_source_clusters.items():\n",
    "    print(f\"\\n=== Cluster {cluster_id} ({len(poems)} poems) ===\")\n",
    "    for poem_id in poems:\n",
    "        verses = poem_texts.get(poem_id, [])\n",
    "        source = poem_sources.get(poem_id, 'unknown')\n",
    "        print(f\"\\nPoem ID: {poem_id} | Source: {source}\")\n",
    "        verses_str = [str(v) for v in verses if pd.notna(v)]  # optional: skip NaN\n",
    "        print(\"\\n\".join(verses_str))\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03657303-e4f8-47f8-b01c-e91933734f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

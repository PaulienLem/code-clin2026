{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e40a5-11f1-48e5-9e4a-ca06fde042a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84db782c-0a4f-4731-9b46-ecc96d8a8122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings:   0%|          | 0/1295 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Embeddings: 100%|██████████| 1295/1295 [00:40<00:00, 32.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Threshold: 0.70\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph (t=0.70): 100%|██████████| 41424/41424 [00:04<00:00, 8996.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs: 1,608,598 (avg degree: 77.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge weights: 100%|██████████| 1608598/1608598 [00:03<00:00, 526930.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 41424 nodes, 1608598 edges\n",
      "Leiden resolution sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolutions: 100%|██████████| 20/20 [08:40<00:00, 26.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution: 0.1833\n",
      "Best ARI: 0.6554\n",
      "Best V-measure: 0.9397\n",
      "Clusters: 7081\n",
      "\n",
      "================================================================================\n",
      "Threshold: 0.75\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph (t=0.75): 100%|██████████| 41424/41424 [00:04<00:00, 8839.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs: 1,607,928 (avg degree: 77.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge weights: 100%|██████████| 1607928/1607928 [00:03<00:00, 534008.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 41424 nodes, 1607928 edges\n",
      "Leiden resolution sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolutions: 100%|██████████| 20/20 [08:47<00:00, 26.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution: 0.1833\n",
      "Best ARI: 0.6847\n",
      "Best V-measure: 0.9546\n",
      "Clusters: 9218\n",
      "\n",
      "================================================================================\n",
      "Threshold: 0.80\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph (t=0.80): 100%|██████████| 41424/41424 [00:04<00:00, 8965.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs: 1,584,859 (avg degree: 76.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge weights: 100%|██████████| 1584859/1584859 [00:02<00:00, 538834.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 41424 nodes, 1584859 edges\n",
      "Leiden resolution sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolutions: 100%|██████████| 20/20 [07:48<00:00, 23.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution: 0.1274\n",
      "Best ARI: 0.7062\n",
      "Best V-measure: 0.9593\n",
      "Clusters: 10088\n",
      "\n",
      "================================================================================\n",
      "Threshold: 0.85\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph (t=0.85): 100%|██████████| 41424/41424 [00:04<00:00, 9279.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs: 1,350,979 (avg degree: 65.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge weights: 100%|██████████| 1350979/1350979 [00:02<00:00, 539321.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 41424 nodes, 1350979 edges\n",
      "Leiden resolution sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolutions: 100%|██████████| 20/20 [05:18<00:00, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution: 0.0428\n",
      "Best ARI: 0.7239\n",
      "Best V-measure: 0.9581\n",
      "Clusters: 9869\n",
      "\n",
      "================================================================================\n",
      "Threshold: 0.90\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graph (t=0.90): 100%|██████████| 41424/41424 [00:03<00:00, 10597.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs: 652,215 (avg degree: 31.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge weights: 100%|██████████| 652215/652215 [00:01<00:00, 553975.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 41424 nodes, 652215 edges\n",
      "Leiden resolution sweep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolutions:  65%|██████▌   | 13/20 [01:49<00:57,  8.22s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  70%|███████   | 14/20 [01:50<00:36,  6.01s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  75%|███████▌  | 15/20 [01:51<00:22,  4.47s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  80%|████████  | 16/20 [01:52<00:13,  3.40s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  85%|████████▌ | 17/20 [01:52<00:07,  2.65s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  90%|█████████ | 18/20 [01:53<00:04,  2.13s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions:  95%|█████████▌| 19/20 [01:54<00:01,  1.76s/it]/tmp/ipykernel_305272/3241684344.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = labels\n",
      "Resolutions: 100%|██████████| 20/20 [01:55<00:00,  5.79s/it]\n",
      "/tmp/ipykernel_305272/3241684344.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'cluster_t{int(sim_threshold*100)}_best'] = best_labels\n",
      "/tmp/ipykernel_305272/3241684344.py:204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['cluster_best'] = df[f'cluster_t{int(best_threshold*100)}_best']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best resolution: 0.0100\n",
      "Best ARI: 0.7847\n",
      "Best V-measure: 0.9694\n",
      "Clusters: 12030\n",
      "\n",
      "================================================================================\n",
      "GRID SEARCH RESULTS (BEST PER THRESHOLD)\n",
      "================================================================================\n",
      " threshold  n_pairs  avg_degree  best_resolution  best_ari  best_v_measure  n_clusters\n",
      "      0.70  1608598   77.665025         0.183298  0.655353        0.939715        7081\n",
      "      0.75  1607928   77.632677         0.183298  0.684695        0.954551        9218\n",
      "      0.80  1584859   76.518878         0.127427  0.706170        0.959314       10088\n",
      "      0.85  1350979   65.226873         0.042813  0.723863        0.958053        9869\n",
      "      0.90   652215   31.489716         0.010000  0.784729        0.969419       12030\n",
      "\n",
      "================================================================================\n",
      "OVERALL BEST THRESHOLD: 0.90\n",
      "================================================================================\n",
      "ARI: 0.7847\n",
      "V-measure: 0.9694\n",
      "Resolution: 0.0100\n",
      "Clusters: 12030\n",
      "\n",
      "Files saved:\n",
      "  - dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\n",
      "  - dbbe_semantic_results/threshold_gridsearch_summary.csv\n",
      "  - dbbe_semantic_results/all_threshold_resolution_results.csv\n",
      "\n",
      "Total clustering columns created: 100\n",
      "Thresholds tested: 5\n",
      "Resolutions per threshold: 20\n",
      "\n",
      "Reconstructed 10821 poems\n",
      "\n",
      "Threshold 50%...\n",
      "\n",
      "Threshold 60%...\n",
      "\n",
      "Threshold 70%...\n",
      "\n",
      "Threshold 80%...\n",
      "\n",
      "All outputs saved to dbbe_semantic_results/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import faiss\n",
    "import os\n",
    "from typing import Dict\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "os.makedirs('dbbe_semantic_results', exist_ok=True)\n",
    "\n",
    "csv_path = 'dbbe_full.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.dropna()\n",
    "\n",
    "model_name = 'kevinkrahn/shlm-grc-en'\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "verses = df['verse'].tolist()\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(verses), batch_size), desc=\"Embeddings\"):\n",
    "    batch = verses[i:i+batch_size]\n",
    "    encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "nlist = 500\n",
    "k = 50\n",
    "nprobe = 10\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "index.train(embeddings.astype('float32'))\n",
    "index.add(embeddings.astype('float32'))\n",
    "index.nprobe = nprobe\n",
    "distances, indices = index.search(embeddings.astype('float32'), k)\n",
    "\n",
    "similarity_thresholds = [0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "threshold_results = []\n",
    "all_resolution_results = []\n",
    "\n",
    "for sim_threshold in similarity_thresholds:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Threshold: {sim_threshold:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    for i in tqdm(range(len(embeddings)), desc=f\"Building graph (t={sim_threshold:.2f})\"):\n",
    "        for j_idx, distance in zip(indices[i], distances[i]):\n",
    "            if j_idx != i and j_idx != -1:\n",
    "                similarity = distance\n",
    "                if similarity >= sim_threshold:\n",
    "                    idx1, idx2 = min(i, j_idx), max(i, j_idx)\n",
    "                    candidate_pairs.add((idx1, idx2))\n",
    "    \n",
    "    n_pairs = len(candidate_pairs)\n",
    "    avg_degree = n_pairs * 2 / len(embeddings)\n",
    "    \n",
    "    print(f\"Candidate pairs: {n_pairs:,} (avg degree: {avg_degree:.1f})\")\n",
    "    \n",
    "    if n_pairs == 0:\n",
    "        print(\"No pairs found - skipping\")\n",
    "        threshold_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'n_pairs': 0,\n",
    "            'best_resolution': None,\n",
    "            'best_ari': 0,\n",
    "            'best_v_measure': 0,\n",
    "            'n_clusters': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for i, j in tqdm(candidate_pairs, desc=\"Edge weights\"):\n",
    "        sim = float(np.dot(embeddings[i], embeddings[j]))\n",
    "        edges.append((i, j))\n",
    "        weights.append(sim)\n",
    "    \n",
    "    g = ig.Graph(n=len(embeddings), edges=edges, directed=False)\n",
    "    g.es['weight'] = weights\n",
    "    \n",
    "    print(f\"Graph: {g.vcount()} nodes, {g.ecount()} edges\")\n",
    "    \n",
    "    w = np.array(weights)\n",
    "    w_scaled = ((w - w.min()) / (w.max() - w.min())) ** 3\n",
    "    g.es['weight'] = w_scaled.tolist()\n",
    "    \n",
    "    hub_thresh = 500\n",
    "    for v in range(g.vcount()):\n",
    "        if g.degree(v) > hub_thresh:\n",
    "            for e in g.incident(v):\n",
    "                g.es[e]['weight'] *= 0.5\n",
    "    \n",
    "    print(\"Leiden resolution sweep...\")\n",
    "    resolutions = np.logspace(-2, 1, 20)\n",
    "    \n",
    "    best_ari = -1\n",
    "    best_labels = None\n",
    "    best_res = None\n",
    "    best_v = None\n",
    "    \n",
    "    for res in tqdm(resolutions, desc=\"Resolutions\"):\n",
    "        partition = la.find_partition(\n",
    "            g,\n",
    "            la.CPMVertexPartition,\n",
    "            weights='weight',\n",
    "            resolution_parameter=res,\n",
    "            n_iterations=-1\n",
    "        )\n",
    "        labels = np.array(partition.membership)\n",
    "        ari = adjusted_rand_score(df['idgroup'], labels)\n",
    "        v_measure = v_measure_score(df['idgroup'], labels)\n",
    "        n_clusters = len(set(labels))\n",
    "        \n",
    "        col_name = f'cluster_t{int(sim_threshold*100)}_r{res:.6f}'\n",
    "        df[col_name] = labels\n",
    "        \n",
    "        all_resolution_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'resolution': res,\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'n_clusters': n_clusters,\n",
    "            'column_name': col_name\n",
    "        })\n",
    "        \n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_labels = labels\n",
    "            best_res = res\n",
    "            best_v = v_measure\n",
    "    \n",
    "    n_clusters = len(set(best_labels))\n",
    "    \n",
    "    print(f\"Best resolution: {best_res:.4f}\")\n",
    "    print(f\"Best ARI: {best_ari:.4f}\")\n",
    "    print(f\"Best V-measure: {best_v:.4f}\")\n",
    "    print(f\"Clusters: {n_clusters}\")\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': sim_threshold,\n",
    "        'n_pairs': n_pairs,\n",
    "        'avg_degree': avg_degree,\n",
    "        'best_resolution': best_res,\n",
    "        'best_ari': best_ari,\n",
    "        'best_v_measure': best_v,\n",
    "        'n_clusters': n_clusters\n",
    "    })\n",
    "    \n",
    "    df[f'cluster_t{int(sim_threshold*100)}_best'] = best_labels\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH RESULTS (BEST PER THRESHOLD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(threshold_results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_threshold_row = results_df.loc[results_df['best_ari'].idxmax()]\n",
    "best_threshold = best_threshold_row['threshold']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL BEST THRESHOLD: {best_threshold:.2f}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ARI: {best_threshold_row['best_ari']:.4f}\")\n",
    "print(f\"V-measure: {best_threshold_row['best_v_measure']:.4f}\")\n",
    "print(f\"Resolution: {best_threshold_row['best_resolution']:.4f}\")\n",
    "print(f\"Clusters: {int(best_threshold_row['n_clusters'])}\")\n",
    "\n",
    "df['cluster_best'] = df[f'cluster_t{int(best_threshold*100)}_best']\n",
    "\n",
    "df.to_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\", index=False)\n",
    "results_df.to_csv(\"dbbe_semantic_results/threshold_gridsearch_summary.csv\", index=False)\n",
    "\n",
    "all_resolution_df = pd.DataFrame(all_resolution_results)\n",
    "all_resolution_df.to_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "print(\"  - dbbe_semantic_results/threshold_gridsearch_summary.csv\")\n",
    "print(\"  - dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "print(f\"\\nTotal clustering columns created: {len(all_resolution_results)}\")\n",
    "print(f\"Thresholds tested: {len(similarity_thresholds)}\")\n",
    "print(f\"Resolutions per threshold: {len(resolutions)}\")\n",
    "\n",
    "def calculate_jaccard_similarity(clusters_a, clusters_b):\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_leiden_fixed']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "def evaluate_against_ground_truth(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for poem_id, predicted_cluster in poem_clusters.items():\n",
    "        if poem_id in poem_to_type:\n",
    "            y_true.append(poem_to_type[poem_id])\n",
    "            y_pred.append(predicted_cluster)\n",
    "\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "\n",
    "    return ari, v_measure, y_true, y_pred\n",
    "\n",
    "def cluster_poems_jaccard(poem_to_clusters, similarity_threshold=0.66):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            pid_a, pid_b = poem_ids[i], poem_ids[j]\n",
    "            sim = calculate_jaccard_similarity(poem_to_clusters[pid_a], poem_to_clusters[pid_b])\n",
    "            if sim >= similarity_threshold:\n",
    "                edges.append((pid_a, pid_b))\n",
    "\n",
    "    class UF:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py: return\n",
    "            if self.rank[px] < self.rank[py]: px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]: self.rank[px] += 1\n",
    "\n",
    "    uf = UF(poem_ids)\n",
    "    for a, b in edges:\n",
    "        uf.union(a, b)\n",
    "\n",
    "    poem_clusters = {pid: uf.find(pid) for pid in poem_ids}\n",
    "    return poem_clusters, edges\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "\n",
    "if 'cluster_best' in df.columns:\n",
    "    df['cluster_leiden_fixed'] = df['cluster_best']\n",
    "else:\n",
    "    raise ValueError(\"Column 'cluster_best' not found in CSV\")\n",
    "\n",
    "poem_to_clusters, _ = reconstruct_poems(df)\n",
    "\n",
    "thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nThreshold {thresh:.0%}...\")\n",
    "    poem_clusters, edges = cluster_poems_jaccard(poem_to_clusters, thresh)\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].map(poem_clusters)\n",
    "\n",
    "    ari, v_measure, _, _ = evaluate_against_ground_truth(df, poem_clusters)\n",
    "    reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "\n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'n_poem_clusters': len(set(poem_clusters.values())),\n",
    "        'n_edges': len(edges),\n",
    "        'ari': ari,\n",
    "        'v_measure': v_measure,\n",
    "        'perfect_reconstruction_rate': reconstruction_rate,\n",
    "        'n_perfect_clusters': n_perfect,\n",
    "        'n_total_gt_clusters': n_total_gt\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "df.to_csv('dbbe_semantic_results/dbbe_poems_semantic.csv')\n",
    "results_df.to_csv('dbbe_semantic_results/dbbe_poems_semantic_stats.csv')\n",
    "\n",
    "results_df = results_df.sort_values('threshold')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(results_df['threshold'], results_df['ari'], marker='o', linestyle='-')\n",
    "plt.xticks(results_df['threshold'])\n",
    "plt.xlabel(\"Jaccard Similarity Threshold\")\n",
    "plt.ylabel(\"Adjusted Rand Index (ARI)\")\n",
    "plt.title(\"ARI of Poem Clustering vs Threshold\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dbbe_semantic_results/ari_poemlevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "df = df[~df['threshold'].isin([0.85, 0.75])]\n",
    "df = df[df['resolution'] <= 1.0]\n",
    "\n",
    "df['resolution'] = df['resolution'].round(4)\n",
    "df['threshold'] = df['threshold'].round(4)\n",
    "df['ari'] = df['ari'].round(4)\n",
    "\n",
    "heatmap_data = df.pivot(index='resolution', columns='threshold', values='ari')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    fmt=\".4f\", \n",
    "    cmap=\"viridis\", \n",
    "    cbar_kws={'label': 'ARI'},\n",
    "    annot_kws={\"fontsize\":14}\n",
    ")\n",
    "plt.ylabel(\"Resolution\")\n",
    "plt.xlabel(\"Similarity Threshold\")\n",
    "plt.title(\"ARI heatmap across Threshold and Resolution\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"dbbe_semantic_results/ari_verselevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll outputs saved to dbbe_semantic_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b35f747-5b29-4d43-9afa-968c7c5db010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 31705 | Poems in cluster <5 | Verses 2 | Distance 0.858 ===\n",
      "\n",
      "Poem 31679:\n",
      "   : ἰώνι(ον) μ(ὲν) ἀττ(ι)(κ)(ῶς) μέγα γράφε\n",
      "   ἄλλ(ως) (δὲ), μικρ(ὸν) οὐχ ἁμαρτήσ(εις) γράφ(ων):-\n",
      "\n",
      "Poem 31705:\n",
      "   τ(ὸν) παιονισμ(ὸν) βουλγαρισμ(ὸν) | δ’ εἰ γρ(άφ)(εις), :\n",
      "   γρ(άφ)(ειν) μι(κ)(ρ)(ὸν) γίνωσκε: | μηδαμῶς μέγα:-\n",
      "\n",
      "=== Cluster 36240 | Poems in cluster <5 | Verses 2 | Distance 0.741 ===\n",
      "\n",
      "Poem 36240:\n",
      "   ⁘ ὁ τῆς ἐνεδρ(ας) τω(ν) πονηρ(ων) δαιμο(νων)\n",
      "   ῥυσθεις ἀνυμνεῖ τὸν λυτρωτ(ην) ἐκ πόθου ⁘\n",
      "\n",
      "Poem 36419:\n",
      "   ⁘ ἐχθρῶν πονηρω(ν) δαιμο(νων) τυραννι(...)\n",
      "   αιτει λυτρω(...) (...) ⁘\n",
      "\n",
      "=== Cluster 17726 | Poems in cluster <5 | Verses 3 | Distance 0.643 ===\n",
      "\n",
      "Poem 17726:\n",
      "   + ὧσπερ ξένη χέροντες, ἡδῆν π(ατ)ρίδα·\n",
      "   καὶ η θαλατ(τέ)βοντες του ευρῆν λημέ(νας?).\n",
      "   οῦτος και ἡ γρά|φοντες εὔρην βηβλῆου τελος.\n",
      "\n",
      "Poem 18561:\n",
      "   + ὧσπερ ξένοι χάιρουσιν εὑρεῖν π(ατ)ρῖδα.\n",
      "   καὶ οἱ θαλαττεύοντες εὑρεῖν λημ(έν)α.\n",
      "   οὗτω καὶ οἰ γράφοντες βλϊβλίου τέλος.\n",
      "\n",
      "=== Cluster 17996 | Poems in cluster <5 | Verses 3 | Distance 0.594 ===\n",
      "\n",
      "Poem 19149:\n",
      "   ὥσπερ ξένοι χαίρουσιν ἰδεῖν π(ατ)ρίδα· |\n",
      "   (καὶ) οἱ θαλατεύοντες εὑρεῖν καλὸν | λοιμένα,\n",
      "   οὕτος (καὶ) οἱ γράφοντες:- | ἰδεῖν βιβλίου τέλος:-\n",
      "\n",
      "Poem 23901:\n",
      "   †ώσπερ ξένι χέρουσιν υδῆν π(ατ)ρίδα \n",
      "   (καὶ) ἠ θαλατέβ(ων)τες υδ(ῖν) | λυμένα\n",
      "   ούτως (καὶ) ἠ γράφοντ(ες) ὐδῆν βιβλίου τέ(λως)·\n",
      "\n",
      "=== Cluster 21703 | Poems in cluster <5 | Verses 2 | Distance 0.556 ===\n",
      "\n",
      "Poem 21703:\n",
      "   Ματθαῖος οἰκτρὸς (...)\n",
      "   (...) ὠργάνωσε κανόνων\n",
      "\n",
      "Poem 22028:\n",
      "   Ματθαῖος οἰκτρὸς ἐν μονασταῖς καὶ θύτης\n",
      "   πίνακα τούτων ὠργάνωσε κανόνων.\n",
      "\n",
      "=== Cluster 21685 | Poems in cluster <5 | Verses 2 | Distance 0.515 ===\n",
      "\n",
      "Poem 23413:\n",
      "   Τῶν εὐτυχούντων πάνταις ἄνθρωποι φοίλοι\n",
      "   τω<ν> δέ <δυ>στηχοῦντων οὐδέ αὐτῶς ὠ γενοίτῶρ.\n",
      "\n",
      "Poem 23426:\n",
      "   + Τῶν μὲν εὐτυχούντ(ων) πάντες ἄν(θρωπ)οι φίλοι·\n",
      "   τῶν δέ διστυχούντ(ων), οὐδ᾽ αὐτὸς ὀ γεννήτωρ\n",
      "\n",
      "=== Cluster 21685 | Poems in cluster <5 | Verses 2 | Distance 0.512 ===\n",
      "\n",
      "Poem 23413:\n",
      "   Τῶν εὐτυχούντων πάνταις ἄνθρωποι φοίλοι\n",
      "   τω<ν> δέ <δυ>στηχοῦντων οὐδέ αὐτῶς ὠ γενοίτῶρ.\n",
      "\n",
      "Poem 24503:\n",
      "   + τῶν μὲν εὐτυχούντ(ων) πάντες ἄν(θρωπ)οι φίλ(οι)·\n",
      "   τῶν δε δηστυ|χούντ(ων), οὐδ’ αὐτὸς ὁ γενήτωρ:+\n",
      "\n",
      "=== Cluster 17996 | Poems in cluster <5 | Verses 3 | Distance 0.495 ===\n",
      "\n",
      "Poem 17996:\n",
      "   + ὥσπερ ξένοι χέρ(ουσιν) ηδῦν π(ατ)ρίδαν·\n",
      "   καὶ ὲ θαλατέβοντ(ες) ἡδ(ὶν) λημ(έ)ν(α)·\n",
      "   ούτος (καὶ) η | γράφοντες βιβλίου τέλος\n",
      "\n",
      "Poem 19149:\n",
      "   ὥσπερ ξένοι χαίρουσιν ἰδεῖν π(ατ)ρίδα· |\n",
      "   (καὶ) οἱ θαλατεύοντες εὑρεῖν καλὸν | λοιμένα,\n",
      "   οὕτος (καὶ) οἱ γράφοντες:- | ἰδεῖν βιβλίου τέλος:-\n",
      "\n",
      "=== Cluster 18285 | Poems in cluster <5 | Verses 2 | Distance 0.494 ===\n",
      "\n",
      "Poem 18285:\n",
      "   δέχου ὦ Χ(ριστ)έ μου τῶν ἐμῶν χειρῶν κόπον\n",
      "   λύσιν παράσχου τῶν πολλῶν μου σφαλμάτ(ων)\n",
      "\n",
      "Poem 25928:\n",
      "   δέχιο χ(ριστ)ὲ τῶν ἐμῶν χειρῶν κόπον:\n",
      "   λύσιν | βραβεύων τῶν πολλῶν μου πταισμάτων :-\n",
      "\n",
      "=== Cluster 30617 | Poems in cluster <5 | Verses 2 | Distance 0.488 ===\n",
      "\n",
      "Poem 22495:\n",
      "   + ἄρξου χήραν αγαθὴν\n",
      "   γράφε γράμ|ματα καλά\n",
      "\n",
      "Poem 23154:\n",
      "   ἄρξου χῆρ μοῦ ἀγαθῆ\n",
      "   γρα´ γραφί γράματα καλά\n",
      "\n",
      "=== Cluster 30617 | Poems in cluster <5 | Verses 2 | Distance 0.488 ===\n",
      "\n",
      "Poem 23154:\n",
      "   ἄρξου χῆρ μοῦ ἀγαθῆ\n",
      "   γρα´ γραφί γράματα καλά\n",
      "\n",
      "Poem 30617:\n",
      "   + ἄρξου χήραν αγαθην\n",
      "   γράφε γράματα κα(...)\n",
      "\n",
      "=== Cluster 20758 | Poems in cluster <5 | Verses 2 | Distance 0.482 ===\n",
      "\n",
      "Poem 20758:\n",
      "   ὥσπερ ξένη χέρουσιν ἠδεὶν π(ατ)ρίδα |\n",
      "   οὗτως καὶ ἡ γράφονταις βιβλίου | τέλος·\n",
      "\n",
      "Poem 36041:\n",
      "   ωσπερ ξένη χερουσαν ιδων π(α)τριδα\n",
      "   ουτος και η γραφοντες βυβληου τελος. \n",
      "\n",
      "=== Cluster 21685 | Poems in cluster <5 | Verses 2 | Distance 0.477 ===\n",
      "\n",
      "Poem 21685:\n",
      "   + τῶν εὐτυχούντ(ων) | πάντες ἄν(θρωπ)οι φίλοι· |\n",
      "   τῶν δε δυστυχύντ(ων) (δε) | οὐδ᾽ αὐτός ὁ γενήτωρ·\n",
      "\n",
      "Poem 23413:\n",
      "   Τῶν εὐτυχούντων πάνταις ἄνθρωποι φοίλοι\n",
      "   τω<ν> δέ <δυ>στηχοῦντων οὐδέ αὐτῶς ὠ γενοίτῶρ.\n",
      "\n",
      "=== Cluster 17996 | Poems in cluster <5 | Verses 3 | Distance 0.466 ===\n",
      "\n",
      "Poem 17996:\n",
      "   + ὥσπερ ξένοι χέρ(ουσιν) ηδῦν π(ατ)ρίδαν·\n",
      "   καὶ ὲ θαλατέβοντ(ες) ἡδ(ὶν) λημ(έ)ν(α)·\n",
      "   ούτος (καὶ) η | γράφοντες βιβλίου τέλος\n",
      "\n",
      "Poem 23901:\n",
      "   †ώσπερ ξένι χέρουσιν υδῆν π(ατ)ρίδα \n",
      "   (καὶ) ἠ θαλατέβ(ων)τες υδ(ῖν) | λυμένα\n",
      "   ούτως (καὶ) ἠ γράφοντ(ες) ὐδῆν βιβλίου τέ(λως)·\n",
      "\n",
      "=== Cluster 18385 | Poems in cluster <5 | Verses 2 | Distance 0.462 ===\n",
      "\n",
      "Poem 18627:\n",
      "   Μακαρίου ταῦτεστι φιλαδεφείας·\n",
      "   τοῦ οἰκέτου τῆς μακαρίας Τριάδος +\n",
      "\n",
      "Poem 19866:\n",
      "   ταπεινὸς μακάριος φιλαδελφείας·\n",
      "   ὁ οἰκέτης τῆς μακαρίας τριάδος. |\n",
      "\n",
      "=== Cluster 18385 | Poems in cluster <5 | Verses 2 | Distance 0.462 ===\n",
      "\n",
      "Poem 19866:\n",
      "   ταπεινὸς μακάριος φιλαδελφείας·\n",
      "   ὁ οἰκέτης τῆς μακαρίας τριάδος. |\n",
      "\n",
      "Poem 31113:\n",
      "   Μακαρίου ταῦτεστι φιλαδεφείας,\n",
      "   τοῦ οἰκέτου τῆς μακαρίας Τριάδος +\n",
      "\n",
      "=== Cluster 18385 | Poems in cluster <5 | Verses 2 | Distance 0.462 ===\n",
      "\n",
      "Poem 19866:\n",
      "   ταπεινὸς μακάριος φιλαδελφείας·\n",
      "   ὁ οἰκέτης τῆς μακαρίας τριάδος. |\n",
      "\n",
      "Poem 31116:\n",
      "   Μακαρίου ταῦτεστι φιλαδεφείας,\n",
      "   τοῦ οἰκέτου τῆς μακαρίας Τριάδος +\n",
      "\n",
      "=== Cluster 18362 | Poems in cluster <5 | Verses 2 | Distance 0.437 ===\n",
      "\n",
      "Poem 18362:\n",
      "   + τὸν ἀχρεῖον νικόλαον τοῦ θ(εο)ῦ δούλε·\n",
      "   μιμνήσκου χαίρων, διερχόμ(εν)ος τοῦτο:·​\n",
      "\n",
      "Poem 24962:\n",
      "   + τὸν ἀχρεῖον οἰάκιθνον θ(εο)ῦ δοῦλε:\n",
      "   μέμνησο χαίρ(ων) διερχόμενος τούτ(ου) +\n",
      "\n",
      "=== Cluster 18385 | Poems in cluster <5 | Verses 2 | Distance 0.421 ===\n",
      "\n",
      "Poem 18385:\n",
      "   Μακαρίου ταῦτ᾽ ἐστι φιλαδελφεί(ας)·\n",
      "   τοῦ οἰκέτου τῆς μακαρί(ας) τριάδος+\n",
      "\n",
      "Poem 19866:\n",
      "   ταπεινὸς μακάριος φιλαδελφείας·\n",
      "   ὁ οἰκέτης τῆς μακαρίας τριάδος. |\n",
      "\n",
      "=== Cluster 25680 | Poems in cluster <5 | Verses 3 | Distance 0.419 ===\n",
      "\n",
      "Poem 19728:\n",
      "   Ὡσπὲρ ξένοι χαίρουσιν εἰδ(εῖν) πατρίδα\n",
      "   καὶ οἱ θαλαττευῶντ' εὑρ(εῖν) λειμένα· οὔτ(ως)\n",
      "   καὶ οἱ γράφοντες· εἶδ(εῖν) βιβλί(ως) τέλος.\n",
      "\n",
      "Poem 25680:\n",
      "   ὥσπερ ξένοι χαίροντες οἱδεῖν πατρίδα·\n",
      "   οὕτῳ καὶ οἱ θαλαττεύοντες εὑρεῖν λοιμέν(α)·\n",
      "   οὕτω καὶ οἱ γράφοντες οἱδεῖν βιβλίου τέλος\n"
     ]
    }
   ],
   "source": [
    "# Helper printing verses that are orthographically dissimilar but share a cluster\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "poems = (\n",
    "    df.sort_values([\"idoriginal_poem\", \"order\"])\n",
    "      .groupby(\"idoriginal_poem\")[\"verse\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "poem_clusters = (\n",
    "    df.groupby(\"idoriginal_poem\")[\"poem_cluster_id\"]\n",
    "      .first()\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "cluster_to_poems = defaultdict(list)\n",
    "for pid, cid in poem_clusters.items():\n",
    "    cluster_to_poems[cid].append(pid)\n",
    "def versewise_min_distance(v1, v2):\n",
    "    v1_proc = [preprocess_text(v) for v in v1]\n",
    "    v2_proc = [preprocess_text(v) for v in v2]\n",
    "    \n",
    "    unmatched = set(range(len(v2_proc)))\n",
    "    total_dist = 0\n",
    "    \n",
    "    for va in v1_proc:\n",
    "        if not unmatched:\n",
    "            break\n",
    "        best_idx = min(unmatched, key=lambda i: jaccard_distance(va, v2_proc[i]))\n",
    "        total_dist += jaccard_distance(va, v2_proc[best_idx])\n",
    "        unmatched.remove(best_idx)\n",
    "        \n",
    "    return total_dist / max(len(v1_proc), len(v2_proc))\n",
    "\n",
    "def shingles(s, k=3):\n",
    "    s = preprocess_text(s)\n",
    "    return {s[i:i+k] for i in range(len(s)-k+1)} if len(s) >= k else {s}\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "\n",
    "    A, B = shingles(a), shingles(b)\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return 1 - (inter / union if union else 0)\n",
    "\n",
    "results = [] \n",
    "\n",
    "MAX_VERSES = 4\n",
    "MAX_CLUSTER_SIZE = 5  \n",
    "\n",
    "for cid, poem_ids in cluster_to_poems.items():\n",
    "    if len(poem_ids) > MAX_CLUSTER_SIZE:\n",
    "        continue\n",
    "\n",
    "    if len(poem_ids) < 2:\n",
    "        continue\n",
    "\n",
    "    for p1, p2 in combinations(poem_ids, 2):\n",
    "        v1, v2 = poems[p1], poems[p2]\n",
    "\n",
    "        verse_count = len(v1)\n",
    "\n",
    "        if verse_count > MAX_VERSES:\n",
    "            continue\n",
    "\n",
    "        dist = versewise_min_distance(v1, v2)\n",
    "\n",
    "        results.append((cid, p1, p2, dist, verse_count))\n",
    "\n",
    "results.sort(key=lambda x: x[3], reverse=True)  \n",
    "\n",
    "filtered_results = [r for r in results if r[4] > 1]\n",
    "\n",
    "TOP = 20\n",
    "for cid, p1, p2, dist, vcount in filtered_results[:TOP]:\n",
    "    print(f\"\\n=== Cluster {cid} | Poems in cluster <5 | Verses {vcount} | Distance {dist:.3f} ===\")\n",
    "\n",
    "    print(f\"\\nPoem {p1}:\")\n",
    "    for line in poems[p1]:\n",
    "        print(\"  \", line)\n",
    "\n",
    "    print(f\"\\nPoem {p2}:\")\n",
    "    for line in poems[p2]:\n",
    "        print(\"  \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6e6cd0e-cdf6-4b7b-88b3-2cf453928e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 19026: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​\n",
      "\n",
      "=== cluster_t70 | Cluster 2427 | 4 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 21229 | Verse: ναυκρατίου μεγάλου γλυ|κερὸς πόνος ὅν ποτε κεῖνος: |  | Poem Cluster: 17481 | Type ID: 2019 | Verse Group 19072.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n",
      "=== cluster_t75 | Cluster 3465 | 3 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n",
      "=== cluster_t80 | Cluster 3425 | 3 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n",
      "=== cluster_t85 | Cluster 2754 | 3 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n",
      "=== cluster_t90 | Cluster 3134 | 3 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n",
      "=== cluster_leiden_fixed | Cluster 3134 | 3 poems ===\n",
      "  Poem ID: 18986 | Verse: ποντωπλο|οῦσι παῦλα λιμὴν | τῶν πόνων·​ | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 22021 | Verse: Ποντοπλωοῦσι παῦλα· λιμὶν τῶν πόνων· | Poem Cluster: 22021 | Type ID: 3096 | Verse Group 19026.0\n",
      "  Poem ID: 35833 | Verse: ποντοποοῦσι παῦλα· λυμὴν τῶ(ν) πόνων· | Poem Cluster: 18986 | Type ID: 3096 | Verse Group 19026.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper printing all verses that were clustered together with a given target idgroup\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "\n",
    "target_idgroup = 831\n",
    "threshold_cols = ['cluster_t70', 'cluster_t75', 'cluster_t80', 'cluster_t85', 'cluster_t90', 'cluster_leiden_fixed']\n",
    "poem_to_cluster = df.groupby('idoriginal_poem')['poem_cluster_id'].first().to_dict()\n",
    "\n",
    "\n",
    "verse_row = df[df['idgroup'] == target_idgroup]\n",
    "if verse_row.empty:\n",
    "    print(\"idgroup not found\")\n",
    "else:\n",
    "    print(f\"Verse {target_idgroup}: {verse_row['verse'].iloc[0]}\\n\")dbbe_semantic_results\n",
    "\n",
    "    for col in threshold_cols:\n",
    "        cluster_id = verse_row[col].iloc[0]\n",
    "        same_cluster = df[df[col] == cluster_id]\n",
    "        same_cluster_sorted = same_cluster.sort_values(['idoriginal_poem', 'order'])\n",
    "\n",
    "        poem_ids_in_cluster = same_cluster_sorted['idoriginal_poem'].unique().tolist()\n",
    "\n",
    "        print(f\"=== {col} | Cluster {cluster_id} | {len(poem_ids_in_cluster)} poems ===\")\n",
    "\n",
    "        for _, row in same_cluster_sorted.iterrows():\n",
    "            poem_id = row['idoriginal_poem']\n",
    "            verse_text = row['verse']\n",
    "            type_id = row['type_id']\n",
    "            verse_group = row['idgroup']\n",
    "\n",
    "            poem_cluster = poem_to_cluster.get(poem_id, \"N/A\")\n",
    "            print(f\"  Poem ID: {poem_id} | Verse: {verse_text} | Poem Cluster: {poem_cluster} | Type ID: {type_id} | Verse Group {verse_group}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae4032ec-d2ec-419a-8094-306fba90fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Poem 36240:\n",
      "   ⁘ ὁ τῆς ἐνεδρ(ας) τω(ν) πονηρ(ων) δαιμο(νων)\n",
      "   ῥυσθεις ἀνυμνεῖ τὸν λυτρωτ(ην) ἐκ πόθου ⁘\n",
      "\n",
      "Poem 36419:\n",
      "   ⁘ ἐχθρῶν πονηρω(ν) δαιμο(νων) τυραννι(...)\n",
      "   αιτει λυτρω(...) (...) ⁘\n"
     ]
    }
   ],
   "source": [
    "# Helper printing all poems for a poem_cluster_id\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "target_cluster_id = 36240\n",
    "\n",
    "cluster_df = df[df['poem_cluster_id'] == target_cluster_id]\n",
    "\n",
    "poems_in_cluster = (\n",
    "    cluster_df.sort_values(['idoriginal_poem', 'order'])\n",
    "              .groupby('idoriginal_poem')['verse']\n",
    "              .apply(list)\n",
    "              .to_dict()\n",
    ")\n",
    "\n",
    "for poem_id, verses in poems_in_cluster.items():\n",
    "    print(f\"\\nPoem {poem_id}:\")\n",
    "    for line in verses:\n",
    "        print(\"  \", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06a9c9-8e39-4efe-b9a9-933fd6aa4609",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0f25ab-375f-4cdd-afcc-c5a1b62886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n",
      "✓ Checkpoint directory: /scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints\n",
      "✓ Results directory: semantic_clustering_results\n",
      "✓ Utility functions defined\n",
      "================================================================================\n",
      "📂 COMPACT CHECKPOINT FOUND - LOADING FROM DISK\n",
      "================================================================================\n",
      "\n",
      "Loading embeddings...\n",
      "  ✓ Embeddings: (1536616, 768)\n",
      "Loading candidate pairs...\n",
      "  ✓ Candidate pairs: 35,082,746\n",
      "Loading DataFrame...\n",
      "  ✓ DataFrame: 1,536,616 rows\n",
      "Loading metadata...\n",
      "\n",
      "✅ Checkpoint loaded! Size: 4358.6 MB\n",
      "  Documents: 1,536,616\n",
      "  Candidate pairs: 35,082,746\n",
      "  Source datasets: ['rhoby', 'dbbe', 'phi', 'papyri']\n",
      "\n",
      "================================================================================\n",
      "✅ READY TO PROCEED WITH THRESHOLD/RESOLUTION SWEEP\n",
      "================================================================================\n",
      "⏭️  Skipping data loading and embedding computation (loaded from checkpoint)\n",
      "⏭️  Skipping FAISS stage (loaded from checkpoint)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2: THRESHOLD/RESOLUTION SWEEP ON SAMPLE\n",
      "================================================================================\n",
      "Sample size: 9,998\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_305272/1553198463.py\", line -1, in <module>\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/apps/gent/RHEL9/zen3-ampere-ib/software/IPython/8.14.0-GCCcore-12.3.0/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import unicodedata\n",
    "# from typing import Dict\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# from collections import defaultdict, Counter\n",
    "# import time\n",
    "# import os\n",
    "# import pickle\n",
    "# import gzip\n",
    "# from pathlib import Path\n",
    "\n",
    "# CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints\")\n",
    "# CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# print(\"✓ Imports loaded\")\n",
    "# print(f\"✓ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "# WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "# def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "#     if options is None:\n",
    "#         options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "#     text = str(text)\n",
    "#     if options.get('lowercase', True):\n",
    "#         text = text.lower()\n",
    "#     if options.get('remove_diacritics', True):\n",
    "#         text = unicodedata.normalize('NFD', text)\n",
    "#         text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "#         text = unicodedata.normalize('NFC', text)\n",
    "#     else:\n",
    "#         text = unicodedata.normalize('NFC', text)\n",
    "#     if options.get('remove_punctuation', True):\n",
    "#         text = CLEAN_PATTERN.sub('', text)\n",
    "#     text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "#     return text.strip()\n",
    "\n",
    "# def cls_pooling(model_output):\n",
    "#     return model_output[0][:, 0]\n",
    "\n",
    "# print(\"✓ Utility functions defined\")\n",
    "\n",
    "# embeddings_file = CHECKPOINT_DIR / 'embeddings.npz'\n",
    "# metadata_file = CHECKPOINT_DIR / 'metadata.pkl.gz'\n",
    "\n",
    "# if embeddings_file.exists() and metadata_file.exists():\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"📂 COMPACT CHECKPOINT FOUND - LOADING FROM DISK\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     print(\"\\nLoading embeddings...\")\n",
    "#     embeddings_data = np.load(embeddings_file)\n",
    "#     embeddings = embeddings_data['embeddings']\n",
    "#     print(f\"  ✓ Embeddings: {embeddings.shape}\")\n",
    "    \n",
    "#     pairs_file = CHECKPOINT_DIR / 'candidate_pairs.npz'\n",
    "#     if pairs_file.exists():\n",
    "#         print(\"Loading candidate pairs...\")\n",
    "#         pairs_data = np.load(pairs_file)\n",
    "#         pairs_array = pairs_data['pairs']\n",
    "#         candidate_pairs = set((int(i), int(j)) for i, j in pairs_array)\n",
    "#         print(f\"  ✓ Candidate pairs: {len(candidate_pairs):,}\")\n",
    "#     else:\n",
    "#         candidate_pairs = set()\n",
    "    \n",
    "#     print(\"Loading DataFrame...\")\n",
    "#     df = pd.read_parquet(CHECKPOINT_DIR / 'df_minimal.parquet')\n",
    "#     print(f\"  ✓ DataFrame: {len(df):,} rows\")\n",
    "    \n",
    "#     print(\"Loading metadata...\")\n",
    "#     with gzip.open(metadata_file, 'rb') as f:\n",
    "#         metadata = pickle.load(f)\n",
    "    \n",
    "#     source_datasets = metadata['source_datasets']\n",
    "#     dataset_to_indices = metadata['dataset_to_indices']\n",
    "#     similarity_threshold = metadata['similarity_threshold']\n",
    "#     faiss_config = metadata['faiss_config']\n",
    "    \n",
    "#     dimension = faiss_config['dimension']\n",
    "#     nlist = faiss_config['nlist']\n",
    "#     k = faiss_config['k']\n",
    "#     nprobe = faiss_config['nprobe']\n",
    "    \n",
    "#     total_size = sum(os.path.getsize(f) / 1024 / 1024 \n",
    "#                      for f in CHECKPOINT_DIR.glob('*') \n",
    "#                      if f.is_file() and f.suffix in ['.npz', '.parquet', '.gz'])\n",
    "    \n",
    "#     print(f\"\\n✅ Checkpoint loaded! Size: {total_size:.1f} MB\")\n",
    "#     print(f\"  Documents: {len(df):,}\")\n",
    "#     print(f\"  Candidate pairs: {len(candidate_pairs):,}\")\n",
    "#     print(f\"  Source datasets: {list(dataset_to_indices.keys())}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"✅ READY TO PROCEED WITH LEIDEN STAGE\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     LOADED_FROM_CHECKPOINT = True\n",
    "#     faiss_available = True\n",
    "    \n",
    "# else:\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"ℹ️  NO CHECKPOINT FOUND\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"Will run pipeline from the beginning.\\n\")\n",
    "#     LOADED_FROM_CHECKPOINT = False\n",
    "\n",
    "\n",
    "# if not LOADED_FROM_CHECKPOINT:\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"LOADING DATA\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     csv_path = 'concatenated.csv'\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     df = df.dropna(subset=['verse', 'source_dataset'])\n",
    "#     df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "#     df = df[df['verse'].fillna('').astype(str).str.len() < 256]\n",
    "#     df['verse'] = df['verse'].apply(preprocess_text)\n",
    "#     df = df.reset_index(drop=True)\n",
    "    \n",
    "#     print(f\"Total verses: {len(df):,}\")\n",
    "#     print(f\"Dataset distribution:\")\n",
    "#     print(df['source_dataset'].value_counts())\n",
    "    \n",
    "#     # Load model\n",
    "#     model_name = 'kevinkrahn/shlm-grc-en'\n",
    "#     print(f\"\\nLoading model: {model_name}\")\n",
    "#     model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Compute embeddings\n",
    "#     print(\"\\nComputing embeddings...\")\n",
    "#     verses = df['verse'].tolist()\n",
    "#     embeddings = []\n",
    "#     batch_size = 32\n",
    "    \n",
    "#     for i in tqdm(range(0, len(verses), batch_size), desc=\"Computing embeddings\"):\n",
    "#         try:\n",
    "#             batch = verses[i:i+batch_size]\n",
    "#             encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "#             encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 model_output = model(**encoded_input)\n",
    "            \n",
    "#             batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "#             embeddings.append(batch_embeddings)\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n⚠️ Error at batch {i}: {e}\")\n",
    "#             embeddings.append(np.zeros((len(batch), model.config.hidden_size), dtype=np.float32))\n",
    "    \n",
    "#     embeddings = np.vstack(embeddings)\n",
    "#     embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "#     # Create dataset mapping\n",
    "#     source_datasets = df['source_dataset'].values\n",
    "#     dataset_to_indices = defaultdict(list)\n",
    "#     for idx, dataset in enumerate(source_datasets):\n",
    "#         dataset_to_indices[dataset].append(idx)\n",
    "\n",
    "# else:\n",
    "#     print(\"⏭️  Skipping data loading and embedding computation (loaded from checkpoint)\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # NOTEBOOK CELL 4: FAISS IVF + SAVE COMPACT CHECKPOINT (only if no checkpoint)\n",
    "# # ============================================================================\n",
    "# if not LOADED_FROM_CHECKPOINT:\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"STAGE 1: FAISS IVF PREFILTERING\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     try:\n",
    "#         import faiss\n",
    "        \n",
    "#         dimension = embeddings.shape[1]\n",
    "#         nlist = 500\n",
    "#         k = 100\n",
    "#         nprobe = 20\n",
    "        \n",
    "#         print(f\"\\nFAISS: dimension={dimension}, nlist={nlist}, k={k}, nprobe={nprobe}\")\n",
    "        \n",
    "#         quantizer = faiss.IndexFlatIP(dimension)\n",
    "#         index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        \n",
    "#         print(\"Training...\")\n",
    "#         index.train(embeddings.astype('float32'))\n",
    "        \n",
    "#         print(\"Adding vectors...\")\n",
    "#         index.add(embeddings.astype('float32'))\n",
    "        \n",
    "#         print(\"Searching...\")\n",
    "#         index.nprobe = nprobe\n",
    "#         distances, indices = index.search(embeddings.astype('float32'), k)\n",
    "        \n",
    "#         print(\"Filtering for cross-dataset pairs...\")\n",
    "#         candidate_pairs = set()\n",
    "#         similarity_threshold = 0.75\n",
    "        \n",
    "#         for i in tqdm(range(len(embeddings)), desc=\"Building candidate graph\"):\n",
    "#             dataset_i = source_datasets[i]\n",
    "#             for j_idx, distance in zip(indices[i], distances[i]):\n",
    "#                 if j_idx == i or j_idx == -1:\n",
    "#                     continue\n",
    "#                 dataset_j = source_datasets[j_idx]\n",
    "#                 if dataset_i != dataset_j and distance >= similarity_threshold:\n",
    "#                     idx1, idx2 = min(i, j_idx), max(i, j_idx)\n",
    "#                     candidate_pairs.add((idx1, idx2))\n",
    "        \n",
    "#         print(f\"Cross-dataset candidate pairs: {len(candidate_pairs):,}\")\n",
    "#         faiss_available = True\n",
    "        \n",
    "#     except ImportError:\n",
    "#         print(\"❌ FAISS not installed!\")\n",
    "#         faiss_available = False\n",
    "#         candidate_pairs = set()\n",
    "\n",
    "#     if faiss_available and len(candidate_pairs) > 0:\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"💾 SAVING COMPACT CHECKPOINT\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         # Embeddings (compressed numpy)\n",
    "#         np.savez_compressed(CHECKPOINT_DIR / 'embeddings.npz', embeddings=embeddings)\n",
    "        \n",
    "#         # Candidate pairs (compressed numpy)\n",
    "#         pairs_array = np.array(list(candidate_pairs), dtype=np.int32)\n",
    "#         np.savez_compressed(CHECKPOINT_DIR / 'candidate_pairs.npz', pairs=pairs_array)\n",
    "        \n",
    "#         # DataFrame (parquet with compression)\n",
    "#         essential_cols = ['verse', 'source_dataset']\n",
    "#         for col in ['idoriginal_poem', 'idgroup']:\n",
    "#             if col in df.columns:\n",
    "#                 essential_cols.append(col)\n",
    "        \n",
    "#         df_minimal = df[essential_cols].copy()\n",
    "        \n",
    "#         # Fix data types for parquet compatibility\n",
    "#         for col in df_minimal.columns:\n",
    "#             if df_minimal[col].dtype == 'object':\n",
    "#                 df_minimal[col] = df_minimal[col].astype(str)\n",
    "#                 df_minimal[col] = df_minimal[col].replace('nan', None).replace('None', None)\n",
    "        \n",
    "#         df_minimal.to_parquet(CHECKPOINT_DIR / 'df_minimal.parquet', \n",
    "#                               compression='gzip', index=True)\n",
    "        \n",
    "#         # Metadata (gzip pickle)\n",
    "#         metadata = {\n",
    "#             'source_datasets': source_datasets,\n",
    "#             'dataset_to_indices': dataset_to_indices,\n",
    "#             'similarity_threshold': similarity_threshold,\n",
    "#             'faiss_config': {'dimension': dimension, 'nlist': nlist, 'k': k, 'nprobe': nprobe}\n",
    "#         }\n",
    "#         with gzip.open(CHECKPOINT_DIR / 'metadata.pkl.gz', 'wb') as f:\n",
    "#             pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#         # Calculate size\n",
    "#         total_size = sum(os.path.getsize(f) / 1024 / 1024 \n",
    "#                         for f in CHECKPOINT_DIR.glob('*') \n",
    "#                         if f.is_file() and f.suffix in ['.npz', '.parquet', '.gz'])\n",
    "        \n",
    "#         print(f\"\\n✅ Compact checkpoint saved: {total_size:.1f} MB\")\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"YOU CAN NOW STOP THE NOTEBOOK\")\n",
    "#         print(\"=\"*80)\n",
    "\n",
    "# else:\n",
    "#     print(\"⏭️  Skipping FAISS stage (loaded from checkpoint)\")\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # NOTEBOOK CELL 6: SAVE FINAL RESULTS\n",
    "# # ============================================================================\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"SAVING RESULTS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# df.to_csv(\"concatenated_cross_dataset_clusters.csv\", index=False)\n",
    "# print(\"✓ Full results saved\")\n",
    "\n",
    "# # Save cross-dataset verses only\n",
    "# cross_mask = df['cluster_id'].map(lambda cid: len(cluster_datasets[cid]) > 1)\n",
    "# df_cross = df[cross_mask].copy()\n",
    "# df_cross.to_csv(\"cross_dataset_verses_only.csv\", index=False)\n",
    "# print(f\"✓ Cross-dataset verses: {len(df_cross):,}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"✅ COMPLETE\")\n",
    "# print(\"=\"*80)\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints\")\n",
    "RESULTS_DIR = Path(\"semantic_clustering_results\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"✓ Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"✓ Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "def stratified_sample(df, n_sample=10000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "        sample_indices.extend(sampled)\n",
    "    \n",
    "    return sorted(sample_indices)\n",
    "\n",
    "print(\"✓ Utility functions defined\")\n",
    "\n",
    "embeddings_file = CHECKPOINT_DIR / 'embeddings.npz'\n",
    "metadata_file = CHECKPOINT_DIR / 'metadata.pkl.gz'\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    print(\"=\"*80)\n",
    "    print(\"📂 COMPACT CHECKPOINT FOUND - LOADING FROM DISK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nLoading embeddings...\")\n",
    "    embeddings_data = np.load(embeddings_file)\n",
    "    embeddings = embeddings_data['embeddings']\n",
    "    print(f\"  ✓ Embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    pairs_file = CHECKPOINT_DIR / 'candidate_pairs.npz'\n",
    "    if pairs_file.exists():\n",
    "        print(\"Loading candidate pairs...\")\n",
    "        pairs_data = np.load(pairs_file)\n",
    "        pairs_array = pairs_data['pairs']\n",
    "        candidate_pairs = set((int(i), int(j)) for i, j in pairs_array)\n",
    "        print(f\"  ✓ Candidate pairs: {len(candidate_pairs):,}\")\n",
    "    else:\n",
    "        candidate_pairs = set()\n",
    "    \n",
    "    print(\"Loading DataFrame...\")\n",
    "    df = pd.read_parquet(CHECKPOINT_DIR / 'df_minimal.parquet')\n",
    "    print(f\"  ✓ DataFrame: {len(df):,} rows\")\n",
    "    \n",
    "    print(\"Loading metadata...\")\n",
    "    with gzip.open(metadata_file, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    source_datasets = metadata['source_datasets']\n",
    "    dataset_to_indices = metadata['dataset_to_indices']\n",
    "    similarity_threshold = metadata['similarity_threshold']\n",
    "    faiss_config = metadata['faiss_config']\n",
    "    \n",
    "    dimension = faiss_config['dimension']\n",
    "    nlist = faiss_config['nlist']\n",
    "    k = faiss_config['k']\n",
    "    nprobe = faiss_config['nprobe']\n",
    "    \n",
    "    total_size = sum(os.path.getsize(f) / 1024 / 1024 \n",
    "                     for f in CHECKPOINT_DIR.glob('*') \n",
    "                     if f.is_file() and f.suffix in ['.npz', '.parquet', '.gz'])\n",
    "    \n",
    "    print(f\"\\n✅ Checkpoint loaded! Size: {total_size:.1f} MB\")\n",
    "    print(f\"  Documents: {len(df):,}\")\n",
    "    print(f\"  Candidate pairs: {len(candidate_pairs):,}\")\n",
    "    print(f\"  Source datasets: {list(dataset_to_indices.keys())}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ READY TO PROCEED WITH THRESHOLD/RESOLUTION SWEEP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    LOADED_FROM_CHECKPOINT = True\n",
    "    faiss_available = True\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ℹ️  NO CHECKPOINT FOUND\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Will run pipeline from the beginning.\\n\")\n",
    "    LOADED_FROM_CHECKPOINT = False\n",
    "\n",
    "if not LOADED_FROM_CHECKPOINT:\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    csv_path = 'concatenated.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['verse', 'source_dataset'])\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() < 256]\n",
    "    df['verse'] = df['verse'].apply(preprocess_text)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Total verses: {len(df):,}\")\n",
    "    print(f\"Dataset distribution:\")\n",
    "    print(df['source_dataset'].value_counts())\n",
    "    \n",
    "    model_name = 'kevinkrahn/shlm-grc-en'\n",
    "    print(f\"\\nLoading model: {model_name}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    try:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    except RuntimeError as e:\n",
    "        print(f\"CUDA error: {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "        device = torch.device('cpu')\n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    print(\"\\nComputing embeddings...\")\n",
    "    verses = df['verse'].tolist()\n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(verses), batch_size), desc=\"Computing embeddings\"):\n",
    "        try:\n",
    "            batch = verses[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            \n",
    "            batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error at batch {i}: {e}\")\n",
    "            embeddings.append(np.zeros((len(batch), model.config.hidden_size), dtype=np.float32))\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    source_datasets = df['source_dataset'].values\n",
    "    dataset_to_indices = defaultdict(list)\n",
    "    for idx, dataset in enumerate(source_datasets):\n",
    "        dataset_to_indices[dataset].append(idx)\n",
    "\n",
    "else:\n",
    "    print(\"⏭️  Skipping data loading and embedding computation (loaded from checkpoint)\")\n",
    "\n",
    "if not LOADED_FROM_CHECKPOINT:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 1: FAISS IVF PREFILTERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        import faiss\n",
    "        \n",
    "        dimension = embeddings.shape[1]\n",
    "        nlist = 500\n",
    "        k = 100\n",
    "        nprobe = 20\n",
    "        \n",
    "        print(f\"\\nFAISS: dimension={dimension}, nlist={nlist}, k={k}, nprobe={nprobe}\")\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        \n",
    "        print(\"Training...\")\n",
    "        index.train(embeddings.astype('float32'))\n",
    "        \n",
    "        print(\"Adding vectors...\")\n",
    "        index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(\"Searching...\")\n",
    "        index.nprobe = nprobe\n",
    "        distances, indices = index.search(embeddings.astype('float32'), k)\n",
    "        \n",
    "        print(\"Filtering for cross-dataset pairs...\")\n",
    "        candidate_pairs = set()\n",
    "        similarity_threshold = 0.75\n",
    "        \n",
    "        for i in tqdm(range(len(embeddings)), desc=\"Building candidate graph\"):\n",
    "            dataset_i = source_datasets[i]\n",
    "            for j_idx, distance in zip(indices[i], distances[i]):\n",
    "                if j_idx == i or j_idx == -1:\n",
    "                    continue\n",
    "                dataset_j = source_datasets[j_idx]\n",
    "                if dataset_i != dataset_j and distance >= similarity_threshold:\n",
    "                    idx1, idx2 = min(i, j_idx), max(i, j_idx)\n",
    "                    candidate_pairs.add((idx1, idx2))\n",
    "        \n",
    "        print(f\"Cross-dataset candidate pairs: {len(candidate_pairs):,}\")\n",
    "        faiss_available = True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ FAISS not installed!\")\n",
    "        faiss_available = False\n",
    "        candidate_pairs = set()\n",
    "\n",
    "    if faiss_available and len(candidate_pairs) > 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"💾 SAVING COMPACT CHECKPOINT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        np.savez_compressed(CHECKPOINT_DIR / 'embeddings.npz', embeddings=embeddings)\n",
    "        \n",
    "        pairs_array = np.array(list(candidate_pairs), dtype=np.int32)\n",
    "        np.savez_compressed(CHECKPOINT_DIR / 'candidate_pairs.npz', pairs=pairs_array)\n",
    "        \n",
    "        essential_cols = ['verse', 'source_dataset']\n",
    "        for col in ['idoriginal_poem', 'idgroup']:\n",
    "            if col in df.columns:\n",
    "                essential_cols.append(col)\n",
    "        \n",
    "        df_minimal = df[essential_cols].copy()\n",
    "        \n",
    "        for col in df_minimal.columns:\n",
    "            if df_minimal[col].dtype == 'object':\n",
    "                df_minimal[col] = df_minimal[col].astype(str)\n",
    "                df_minimal[col] = df_minimal[col].replace('nan', None).replace('None', None)\n",
    "        \n",
    "        df_minimal.to_parquet(CHECKPOINT_DIR / 'df_minimal.parquet', \n",
    "                              compression='gzip', index=True)\n",
    "        \n",
    "        metadata = {\n",
    "            'source_datasets': source_datasets,\n",
    "            'dataset_to_indices': dataset_to_indices,\n",
    "            'similarity_threshold': similarity_threshold,\n",
    "            'faiss_config': {'dimension': dimension, 'nlist': nlist, 'k': k, 'nprobe': nprobe}\n",
    "        }\n",
    "        with gzip.open(CHECKPOINT_DIR / 'metadata.pkl.gz', 'wb') as f:\n",
    "            pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        total_size = sum(os.path.getsize(f) / 1024 / 1024 \n",
    "                        for f in CHECKPOINT_DIR.glob('*') \n",
    "                        if f.is_file() and f.suffix in ['.npz', '.parquet', '.gz'])\n",
    "        \n",
    "        print(f\"\\n✅ Compact checkpoint saved: {total_size:.1f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"⏭️  Skipping FAISS stage (loaded from checkpoint)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 2: THRESHOLD/RESOLUTION SWEEP ON SAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_indices = stratified_sample(df, n_sample=10000)\n",
    "print(f\"Sample size: {len(sample_indices):,}\")\n",
    "\n",
    "sample_pairs = set()\n",
    "for i, j in candidate_pairs:\n",
    "    if i in sample_indices and j in sample_indices:\n",
    "        sample_pairs.add((i, j))\n",
    "\n",
    "print(f\"Sample pairs: {len(sample_pairs):,}\")\n",
    "\n",
    "similarity_thresholds = [0.80, 0.85, 0.90]\n",
    "all_results = []\n",
    "\n",
    "for sim_threshold in similarity_thresholds:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Threshold: {sim_threshold:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    threshold_pairs = set()\n",
    "    for i, j in tqdm(sample_pairs, desc=\"Filtering by threshold\"):\n",
    "        similarity = float(np.dot(embeddings[i], embeddings[j]))\n",
    "        if similarity >= sim_threshold:\n",
    "            threshold_pairs.add((i, j))\n",
    "    \n",
    "    n_pairs = len(threshold_pairs)\n",
    "    avg_degree = n_pairs * 2 / len(sample_indices) if len(sample_indices) > 0 else 0\n",
    "    \n",
    "    print(f\"Pairs at threshold: {n_pairs:,} (avg degree: {avg_degree:.1f})\")\n",
    "    \n",
    "    if n_pairs == 0:\n",
    "        print(\"No pairs found - skipping\")\n",
    "        continue\n",
    "    \n",
    "    edges = [(i, j) for i, j in threshold_pairs]\n",
    "    weights = [float(np.dot(embeddings[i], embeddings[j])) for i, j in edges]\n",
    "    \n",
    "    g = ig.Graph(n=len(df), edges=edges, directed=False)\n",
    "    g.es['weight'] = weights\n",
    "    \n",
    "    print(f\"Graph: {g.vcount()} nodes, {g.ecount()} edges\")\n",
    "    \n",
    "    w = np.array(weights)\n",
    "    w_scaled = ((w - w.min()) / (w.max() - w.min())) ** 3\n",
    "    g.es['weight'] = w_scaled.tolist()\n",
    "    \n",
    "    hub_thresh = 500\n",
    "    for v in range(g.vcount()):\n",
    "        if g.degree(v) > hub_thresh:\n",
    "            for e in g.incident(v):\n",
    "                g.es[e]['weight'] *= 0.5\n",
    "    \n",
    "    print(\"Leiden resolution sweep...\")\n",
    "    resolutions = np.logspace(-2, 1, 15)\n",
    "    \n",
    "    for res in tqdm(resolutions, desc=\"Resolutions\"):\n",
    "        partition = la.find_partition(\n",
    "            g,\n",
    "            la.CPMVertexPartition,\n",
    "            weights='weight',\n",
    "            resolution_parameter=res,\n",
    "            n_iterations=-1\n",
    "        )\n",
    "        labels = np.array(partition.membership)\n",
    "        \n",
    "        cluster_datasets = defaultdict(set)\n",
    "        for idx in sample_indices:\n",
    "            cluster_id = labels[idx]\n",
    "            cluster_datasets[cluster_id].add(source_datasets[idx])\n",
    "        \n",
    "        n_cross_dataset = sum(1 for datasets in cluster_datasets.values() if len(datasets) > 1)\n",
    "        n_clusters = len(set(labels))\n",
    "        cluster_sizes = np.bincount(labels)\n",
    "        avg_cluster_size = cluster_sizes.mean()\n",
    "        \n",
    "        all_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'resolution': res,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_cross_dataset_clusters': n_cross_dataset,\n",
    "            'avg_cluster_size': avg_cluster_size,\n",
    "            'n_pairs': n_pairs\n",
    "        })\n",
    "        \n",
    "        print(f\"  res={res:.4f}: clusters={n_clusters}, cross-dataset={n_cross_dataset}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(RESULTS_DIR / \"threshold_resolution_sweep.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD/RESOLUTION SWEEP RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "pivot_clusters = results_df.pivot(index='resolution', columns='threshold', values='n_cross_dataset_clusters')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_clusters, annot=True, fmt='.0f', cmap='viridis', \n",
    "            cbar_kws={'label': 'Cross-dataset Clusters'},\n",
    "            annot_kws={\"fontsize\":12})\n",
    "plt.ylabel(\"Resolution\", fontsize=12)\n",
    "plt.xlabel(\"Similarity Threshold\", fontsize=12)\n",
    "plt.title(\"Cross-dataset Clusters by Threshold and Resolution\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"threshold_resolution_heatmap.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "best_row = results_df[results_df['threshold'] >= 0.85].sort_values('n_cross_dataset_clusters', ascending=False).iloc[0]\n",
    "best_threshold = best_row['threshold']\n",
    "best_resolution = best_row['resolution']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST CONFIGURATION (threshold >= 0.85):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Resolution: {best_resolution:.4f}\")\n",
    "print(f\"Cross-dataset clusters: {int(best_row['n_cross_dataset_clusters'])}\")\n",
    "print(f\"Total clusters: {int(best_row['n_clusters'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STAGE 3: FULL CLUSTERING WITH BEST PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "threshold_pairs = set()\n",
    "for i, j in tqdm(candidate_pairs, desc=\"Filtering by best threshold\"):\n",
    "    similarity = float(np.dot(embeddings[i], embeddings[j]))\n",
    "    if similarity >= best_threshold:\n",
    "        threshold_pairs.add((i, j))\n",
    "\n",
    "print(f\"Pairs at threshold {best_threshold:.2f}: {len(threshold_pairs):,}\")\n",
    "\n",
    "edges = [(i, j) for i, j in threshold_pairs]\n",
    "weights = [float(np.dot(embeddings[i], embeddings[j])) for i, j in edges]\n",
    "\n",
    "g = ig.Graph(n=len(df), edges=edges, directed=False)\n",
    "g.es['weight'] = weights\n",
    "\n",
    "w = np.array(weights)\n",
    "w_scaled = ((w - w.min()) / (w.max() - w.min())) ** 3\n",
    "g.es['weight'] = w_scaled.tolist()\n",
    "\n",
    "hub_thresh = 500\n",
    "for v in range(g.vcount()):\n",
    "    if g.degree(v) > hub_thresh:\n",
    "        for e in g.incident(v):\n",
    "            g.es[e]['weight'] *= 0.5\n",
    "\n",
    "print(f\"Running Leiden with resolution={best_resolution:.4f}...\")\n",
    "partition = la.find_partition(\n",
    "    g,\n",
    "    la.CPMVertexPartition,\n",
    "    weights='weight',\n",
    "    resolution_parameter=best_resolution,\n",
    "    n_iterations=-1\n",
    ")\n",
    "\n",
    "cluster_labels = np.array(partition.membership)\n",
    "df['cluster_id'] = cluster_labels\n",
    "\n",
    "cluster_datasets = defaultdict(set)\n",
    "for idx, cluster_id in enumerate(cluster_labels):\n",
    "    cluster_datasets[cluster_id].add(source_datasets[idx])\n",
    "\n",
    "n_cross_dataset = sum(1 for datasets in cluster_datasets.values() if len(datasets) > 1)\n",
    "print(f\"Cross-dataset clusters: {n_cross_dataset:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df.to_csv(RESULTS_DIR / \"concatenated_cross_dataset_clusters.csv\", index=False)\n",
    "print(\"✓ Full results saved\")\n",
    "\n",
    "cross_mask = df['cluster_id'].map(lambda cid: len(cluster_datasets[cid]) > 1)\n",
    "df_cross = df[cross_mask].copy()\n",
    "df_cross.to_csv(RESULTS_DIR / \"cross_dataset_verses_only.csv\", index=False)\n",
    "print(f\"✓ Cross-dataset verses: {len(df_cross):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d743a-b492-42a8-98af-5cefbe86d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_jaccard_similarity(clusters_a: Set[int], clusters_b: Set[int]) -> float:\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union\n",
    "\n",
    "def reconstruct_poems_unsupervised(df, cluster_col='cluster_id'):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row[cluster_col]\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"Reconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "def evaluate_against_ground_truth(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for poem_id, predicted_cluster in poem_clusters.items():\n",
    "        if poem_id in poem_to_type:\n",
    "            y_true.append(poem_to_type[poem_id])\n",
    "            y_pred.append(predicted_cluster)\n",
    "\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "\n",
    "    return ari, v_measure, y_true, y_pred\n",
    "\n",
    "def cluster_poems_jaccard_unsupervised(poem_to_clusters: dict, similarity_threshold: float = 0.66):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "    \n",
    "    edges = []\n",
    "\n",
    "    for i in tqdm(range(n_poems), desc=\"Comparing poems\"):\n",
    "        for j in range(i+1, n_poems):\n",
    "            pid_a, pid_b = poem_ids[i], poem_ids[j]\n",
    "            set_a, set_b = poem_to_clusters[pid_a], poem_to_clusters[pid_b]\n",
    "            if not set_a or not set_b:\n",
    "                continue\n",
    "            sim = len(set_a & set_b) / len(set_a | set_b)\n",
    "            if sim >= similarity_threshold:\n",
    "                edges.append((pid_a, pid_b))\n",
    "    \n",
    "    class UF:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py: return\n",
    "            if self.rank[px] < self.rank[py]: px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]: self.rank[px] += 1\n",
    "    \n",
    "    uf = UF(poem_ids)\n",
    "    for a, b in edges:\n",
    "        uf.union(a, b)\n",
    "    \n",
    "    poem_clusters = {pid: uf.find(pid) for pid in poem_ids}\n",
    "    return poem_clusters, edges\n",
    "\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "\n",
    "poem_to_clusters, _ = reconstruct_poems_unsupervised(df)\n",
    "\n",
    "thresholds = [0.50, 0.60, 0.66, 0.70]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nThreshold {thresh:.0%}...\")\n",
    "    poem_clusters, edges = cluster_poems_jaccard_unsupervised(poem_to_clusters, thresh)\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].map(poem_clusters)\n",
    "\n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'n_poem_clusters': len(set(poem_clusters.values())),\n",
    "        'n_edges': len(edges),\n",
    "    })\n",
    "    display(results)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04294943-6cd2-422b-8ed5-ff1b1ebad9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (161000975.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[81], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pd.read_csv('\"concatenated_cross_dataset_clusters.csv\")\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv('\"concatenated_cross_dataset_clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e33e87-0036-4de8-baa7-0243e7861549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e40a5-11f1-48e5-9e4a-ca06fde042a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "id": "84db782c-0a4f-4731-9b46-ecc96d8a8122",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import faiss\n",
    "import os\n",
    "from typing import Dict\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "os.makedirs('dbbe_semantic_results', exist_ok=True)\n",
    "\n",
    "csv_path = 'paper_verses.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['idgroup'] = df['idgroup'].astype('float32')\n",
    "df = df.dropna(subset=['text', 'idgroup'])  # Drop if text OR idgroup is missing\n",
    "\n",
    "\n",
    "model_name = 'kevinkrahn/shlm-grc-en'\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "verses = df['text'].tolist()\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(verses), batch_size), desc=\"Embeddings\"):\n",
    "    batch = verses[i:i+batch_size]\n",
    "    encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "n_vectors, dimension = embeddings.shape\n",
    "nlist = int(np.sqrt(n_vectors))\n",
    "nlist = max(1, nlist)\n",
    "nlist = min(nlist, 1024)        # cap to avoid too many lists\n",
    "nlist = min(nlist, n_vectors)\n",
    "k = 50\n",
    "nprobe = 10\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "index.train(embeddings.astype('float32'))\n",
    "index.add(embeddings.astype('float32'))\n",
    "index.nprobe = nprobe\n",
    "distances, indices = index.search(embeddings.astype('float32'), k)\n",
    "\n",
    "similarity_thresholds = [0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "threshold_results = []\n",
    "all_resolution_results = []\n",
    "\n",
    "for sim_threshold in similarity_thresholds:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Threshold: {sim_threshold:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    for i in tqdm(range(len(embeddings)), desc=f\"Building graph (t={sim_threshold:.2f})\"):\n",
    "        for j_idx, distance in zip(indices[i], distances[i]):\n",
    "            if j_idx != i and j_idx != -1:\n",
    "                similarity = distance\n",
    "                if similarity >= sim_threshold:\n",
    "                    idx1, idx2 = min(i, j_idx), max(i, j_idx)\n",
    "                    candidate_pairs.add((idx1, idx2))\n",
    "    \n",
    "    n_pairs = len(candidate_pairs)\n",
    "    avg_degree = n_pairs * 2 / len(embeddings)\n",
    "    \n",
    "    print(f\"Candidate pairs: {n_pairs:,} (avg degree: {avg_degree:.1f})\")\n",
    "    \n",
    "    if n_pairs == 0:\n",
    "        print(\"No pairs found - skipping\")\n",
    "        threshold_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'n_pairs': 0,\n",
    "            'best_resolution': None,\n",
    "            'best_ari': 0,\n",
    "            'best_v_measure': 0,\n",
    "            'n_clusters': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for i, j in tqdm(candidate_pairs, desc=\"Edge weights\"):\n",
    "        sim = float(np.dot(embeddings[i], embeddings[j]))\n",
    "        edges.append((i, j))\n",
    "        weights.append(sim)\n",
    "    \n",
    "    g = ig.Graph(n=len(embeddings), edges=edges, directed=False)\n",
    "    g.es['weight'] = weights\n",
    "    \n",
    "    print(f\"Graph: {g.vcount()} nodes, {g.ecount()} edges\")\n",
    "    \n",
    "    w = np.array(weights)\n",
    "    w_scaled = ((w - w.min()) / (w.max() - w.min())) ** 3\n",
    "    g.es['weight'] = w_scaled.tolist()\n",
    "    \n",
    "    hub_thresh = 500\n",
    "    for v in range(g.vcount()):\n",
    "        if g.degree(v) > hub_thresh:\n",
    "            for e in g.incident(v):\n",
    "                g.es[e]['weight'] *= 0.5\n",
    "    \n",
    "    print(\"Leiden resolution sweep...\")\n",
    "    resolutions = np.logspace(-2, 1, 20)\n",
    "    \n",
    "    best_ari = -1\n",
    "    best_labels = None\n",
    "    best_res = None\n",
    "    best_v = None\n",
    "    \n",
    "    for res in tqdm(resolutions, desc=\"Resolutions\"):\n",
    "        partition = la.find_partition(\n",
    "            g,\n",
    "            la.CPMVertexPartition,\n",
    "            weights='weight',\n",
    "            resolution_parameter=res,\n",
    "            n_iterations=-1\n",
    "        )\n",
    "        labels = np.array(partition.membership)\n",
    "        ari = adjusted_rand_score(df['idgroup'], labels)\n",
    "        v_measure = v_measure_score(df['idgroup'], labels)\n",
    "        n_clusters = len(set(labels))\n",
    "        \n",
    "        col_name = f'cluster_t{int(sim_threshold*100)}_r{res:.6f}'\n",
    "        df[col_name] = labels\n",
    "        \n",
    "        all_resolution_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'resolution': res,\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'n_clusters': n_clusters,\n",
    "            'column_name': col_name\n",
    "        })\n",
    "        \n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_labels = labels\n",
    "            best_res = res\n",
    "            best_v = v_measure\n",
    "    \n",
    "    n_clusters = len(set(best_labels))\n",
    "    \n",
    "    print(f\"Best resolution: {best_res:.4f}\")\n",
    "    print(f\"Best ARI: {best_ari:.4f}\")\n",
    "    print(f\"Best V-measure: {best_v:.4f}\")\n",
    "    print(f\"Clusters: {n_clusters}\")\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': sim_threshold,\n",
    "        'n_pairs': n_pairs,\n",
    "        'avg_degree': avg_degree,\n",
    "        'best_resolution': best_res,\n",
    "        'best_ari': best_ari,\n",
    "        'best_v_measure': best_v,\n",
    "        'n_clusters': n_clusters\n",
    "    })\n",
    "    \n",
    "    df[f'cluster_t{int(sim_threshold*100)}_best'] = best_labels\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH RESULTS (BEST PER THRESHOLD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(threshold_results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_threshold_row = results_df.loc[results_df['best_ari'].idxmax()]\n",
    "best_threshold = best_threshold_row['threshold']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL BEST THRESHOLD: {best_threshold:.2f}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ARI: {best_threshold_row['best_ari']:.4f}\")\n",
    "print(f\"V-measure: {best_threshold_row['best_v_measure']:.4f}\")\n",
    "print(f\"Resolution: {best_threshold_row['best_resolution']:.4f}\")\n",
    "print(f\"Clusters: {int(best_threshold_row['n_clusters'])}\")\n",
    "\n",
    "df['cluster_best'] = df[f'cluster_t{int(best_threshold*100)}_best']\n",
    "\n",
    "df.to_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\", index=False)\n",
    "results_df.to_csv(\"dbbe_semantic_results/threshold_gridsearch_summary.csv\", index=False)\n",
    "\n",
    "all_resolution_df = pd.DataFrame(all_resolution_results)\n",
    "all_resolution_df.to_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "print(\"  - dbbe_semantic_results/threshold_gridsearch_summary.csv\")\n",
    "print(\"  - dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "print(f\"\\nTotal clustering columns created: {len(all_resolution_results)}\")\n",
    "print(f\"Thresholds tested: {len(similarity_thresholds)}\")\n",
    "print(f\"Resolutions per threshold: {len(resolutions)}\")\n",
    "\n",
    "def calculate_jaccard_similarity(clusters_a, clusters_b):\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "    all_poem_ids = set()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_leiden_fixed']\n",
    "        all_poem_ids.add(poem_id)\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    for poem_id in all_poem_ids:\n",
    "        if poem_id not in poem_to_clusters:\n",
    "            poem_to_clusters[poem_id] = set()  # Empty set for poems with no verse clusters\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    print(f\"  - Poems with verse clusters: {sum(1 for clusters in poem_to_clusters.values() if clusters)}\")\n",
    "    print(f\"  - Poems without verse clusters: {sum(1 for clusters in poem_to_clusters.values() if not clusters)}\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "def evaluate_against_ground_truth(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for poem_id, predicted_cluster in poem_clusters.items():\n",
    "        if poem_id in poem_to_type:\n",
    "            y_true.append(poem_to_type[poem_id])\n",
    "            y_pred.append(predicted_cluster)\n",
    "\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "\n",
    "    return ari, v_measure, y_true, y_pred\n",
    "\n",
    "def cluster_poems_jaccard(poem_to_clusters, similarity_threshold=0.66):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            pid_a, pid_b = poem_ids[i], poem_ids[j]\n",
    "            sim = calculate_jaccard_similarity(poem_to_clusters[pid_a], poem_to_clusters[pid_b])\n",
    "            if sim >= similarity_threshold:\n",
    "                edges.append((pid_a, pid_b))\n",
    "\n",
    "    class UF:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py: return\n",
    "            if self.rank[px] < self.rank[py]: px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]: self.rank[px] += 1\n",
    "\n",
    "    uf = UF(poem_ids)\n",
    "    for a, b in edges:\n",
    "        uf.union(a, b)\n",
    "\n",
    "    poem_clusters = {pid: uf.find(pid) for pid in poem_ids}\n",
    "    return poem_clusters, edges\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "\n",
    "if 'cluster_best' in df.columns:\n",
    "    df['cluster_leiden_fixed'] = df['cluster_best']\n",
    "else:\n",
    "    raise ValueError(\"Column 'cluster_best' not found in CSV\")\n",
    "\n",
    "poem_to_clusters, _ = reconstruct_poems(df)\n",
    "\n",
    "thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nThreshold {thresh:.0%}...\")\n",
    "    poem_clusters, edges = cluster_poems_jaccard(poem_to_clusters, thresh)\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].map(poem_clusters)\n",
    "\n",
    "    ari, v_measure, _, _ = evaluate_against_ground_truth(df, poem_clusters)\n",
    "    reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "\n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'n_poem_clusters': len(set(poem_clusters.values())),\n",
    "        'n_edges': len(edges),\n",
    "        'ari': ari,\n",
    "        'v_measure': v_measure,\n",
    "        'perfect_reconstruction_rate': reconstruction_rate,\n",
    "        'n_perfect_clusters': n_perfect,\n",
    "        'n_total_gt_clusters': n_total_gt\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "df.to_csv('dbbe_semantic_results/dbbe_poems_semantic.csv')\n",
    "results_df.to_csv('dbbe_semantic_results/dbbe_poems_semantic_stats.csv')\n",
    "\n",
    "results_df = results_df.sort_values('threshold')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(results_df['threshold'], results_df['ari'], marker='o', linestyle='-')\n",
    "plt.xticks(results_df['threshold'])\n",
    "plt.xlabel(\"Jaccard Similarity Threshold\")\n",
    "plt.ylabel(\"Adjusted Rand Index (ARI)\")\n",
    "plt.title(\"ARI of Poem Clustering vs Threshold\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dbbe_semantic_results/ari_poemlevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "df = df[~df['threshold'].isin([0.85, 0.75])]\n",
    "df = df[df['resolution'] <= 1.0]\n",
    "\n",
    "df['resolution'] = df['resolution'].round(4)\n",
    "df['threshold'] = df['threshold'].round(4)\n",
    "df['ari'] = df['ari'].round(4)\n",
    "\n",
    "heatmap_data = df.pivot(index='resolution', columns='threshold', values='ari')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    fmt=\".4f\", \n",
    "    cmap=\"viridis\", \n",
    "    cbar_kws={'label': 'ARI'},\n",
    "    annot_kws={\"fontsize\":14}\n",
    ")\n",
    "plt.ylabel(\"Resolution\")\n",
    "plt.xlabel(\"Similarity Threshold\")\n",
    "plt.title(\"ARI heatmap across Threshold and Resolution\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"dbbe_semantic_results/ari_verselevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll outputs saved to dbbe_semantic_results/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6e6cd0e-cdf6-4b7b-88b3-2cf453928e2b",
   "metadata": {},
   "source": [
    "# Helper printing all verses that were clustered together with a given target idgroup\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "\n",
    "target_idgroup = 831\n",
    "threshold_cols = ['cluster_t70', 'cluster_t75', 'cluster_t80', 'cluster_t85', 'cluster_t90', 'cluster_leiden_fixed']\n",
    "poem_to_cluster = df.groupby('idoriginal_poem')['poem_cluster_id'].first().to_dict()\n",
    "\n",
    "\n",
    "verse_row = df[df['idgroup'] == target_idgroup]\n",
    "if verse_row.empty:\n",
    "    print(\"idgroup not found\")\n",
    "else:\n",
    "    print(f\"Verse {target_idgroup}: {verse_row['text'].iloc[0]}\\n\")\n",
    "\n",
    "    for col in threshold_cols:\n",
    "        cluster_id = verse_row[col].iloc[0]\n",
    "        same_cluster = df[df[col] == cluster_id]\n",
    "        same_cluster_sorted = same_cluster.sort_values(['idoriginal_poem', 'order'])\n",
    "\n",
    "        poem_ids_in_cluster = same_cluster_sorted['idoriginal_poem'].unique().tolist()\n",
    "\n",
    "        print(f\"=== {col} | Cluster {cluster_id} | {len(poem_ids_in_cluster)} poems ===\")\n",
    "\n",
    "        for _, row in same_cluster_sorted.iterrows():\n",
    "            poem_id = row['idoriginal_poem']\n",
    "            verse_text = row['text']\n",
    "            type_id = row['type_id']\n",
    "            verse_group = row['idgroup']\n",
    "\n",
    "            poem_cluster = poem_to_cluster.get(poem_id, \"N/A\")\n",
    "            print(f\"  Poem ID: {poem_id} | Verse: {verse_text} | Poem Cluster: {poem_cluster} | Type ID: {type_id} | Verse Group {verse_group}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f8e9a40-6568-468a-82b0-fb3088773efc",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "a50f3de6-bf60-4965-8fba-82fcef2f268f",
   "metadata": {},
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import faiss\n",
    "from functools import partial\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "import platform\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import sys\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "csv_path = 'concatenated.csv'\n",
    "RESULTS_DIR = Path(\"full_semantic_results\")\n",
    "CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints_tmp\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self):\n",
    "        self.monitoring = False\n",
    "        self.thread = None\n",
    "        self.peak_ram_gb = 0\n",
    "        self.peak_gpu_mem_gb = 0\n",
    "        self.ram_samples = []\n",
    "        self.gpu_samples = []\n",
    "        self.process = psutil.Process()\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        while self.monitoring:\n",
    "            ram_gb = self.process.memory_info().rss / (1024**3)\n",
    "            self.ram_samples.append(ram_gb)\n",
    "            self.peak_ram_gb = max(self.peak_ram_gb, ram_gb)\n",
    "\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_mem_gb = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "                    self.gpu_samples.append(gpu_mem_gb)\n",
    "                    self.peak_gpu_mem_gb = max(self.peak_gpu_mem_gb, gpu_mem_gb)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    def start(self):\n",
    "        self.monitoring = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.monitoring = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'peak_ram_gb': self.peak_ram_gb,\n",
    "            'avg_ram_gb': np.mean(self.ram_samples) if self.ram_samples else 0,\n",
    "            'peak_gpu_mem_gb': self.peak_gpu_mem_gb,\n",
    "            'avg_gpu_mem_gb': np.mean(self.gpu_samples) if self.gpu_samples else 0\n",
    "        }\n",
    "\n",
    "class TimingLogger:\n",
    "    def __init__(self):\n",
    "        self.stages = {}\n",
    "        self.current_stage = None\n",
    "        self.stage_start = None\n",
    "\n",
    "    def start_stage(self, name):\n",
    "        self.current_stage = name\n",
    "        self.stage_start = time.time()\n",
    "\n",
    "    def end_stage(self):\n",
    "        if self.current_stage and self.stage_start:\n",
    "            duration = time.time() - self.stage_start\n",
    "            self.stages[self.current_stage] = duration\n",
    "            self.current_stage = None\n",
    "            self.stage_start = None\n",
    "\n",
    "    def get_summary(self):\n",
    "        return self.stages.copy()\n",
    "\n",
    "resource_monitor = ResourceMonitor()\n",
    "timing_logger = TimingLogger()\n",
    "\n",
    "def get_system_info():\n",
    "    info = {\n",
    "        'hostname': socket.gethostname(),\n",
    "        'platform': platform.platform(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'processor': platform.processor(),\n",
    "        'cpu_count_physical': psutil.cpu_count(logical=False),\n",
    "        'cpu_count_logical': psutil.cpu_count(logical=True),\n",
    "        'total_ram_gb': psutil.virtual_memory().total / (1024**3),\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        info['gpu_available'] = True\n",
    "        info['gpu_count'] = torch.cuda.device_count()\n",
    "        info['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n",
    "        info['gpu_total_memory_gb'] = [torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                                       for i in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        info['gpu_available'] = False\n",
    "        info['gpu_count'] = 0\n",
    "        info['gpu_names'] = []\n",
    "        info['gpu_total_memory_gb'] = []\n",
    "\n",
    "    return info\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def detect_optimal_resources():\n",
    "    \"\"\"Detect and configure optimal parameters based on available hardware.\"\"\"\n",
    "    cpu_count_physical = psutil.cpu_count(logical=False) or 1\n",
    "    cpu_count_logical = psutil.cpu_count(logical=True) or 1\n",
    "    total_ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    gpu_count = torch.cuda.device_count() if gpu_available else 0\n",
    "    gpu_memory_gb = []\n",
    "    gpu_names = []\n",
    "    if gpu_available and gpu_count > 0:\n",
    "        for i in range(gpu_count):\n",
    "            gpu_memory_gb.append(torch.cuda.get_device_properties(i).total_memory / (1024**3))\n",
    "            gpu_names.append(torch.cuda.get_device_name(i))\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"DYNAMIC RESOURCE DETECTION FOR HPC\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Physical CPU cores: {cpu_count_physical}\")\n",
    "    logger.info(f\"Logical CPU cores:  {cpu_count_logical}\")\n",
    "    logger.info(f\"Total RAM:          {total_ram_gb:.2f} GB\")\n",
    "    logger.info(f\"GPU available:      {gpu_available}\")\n",
    "    if gpu_available:\n",
    "        logger.info(f\"GPU count:          {gpu_count}\")\n",
    "        for i, (name, mem) in enumerate(zip(gpu_names, gpu_memory_gb)):\n",
    "            logger.info(f\"  GPU {i}: {name} ({mem:.2f} GB)\")\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Scale parameters based on resources - AGGRESSIVE HPC SCALING\n",
    "    # Use up to 90% of logical cores for parallel processing on HPC\n",
    "    max_workers = max(8, int(cpu_count_logical * 0.90))\n",
    "    max_workers = min(max_workers, 128)  # Cap at 128 for safety\n",
    "\n",
    "    # Scale FAISS threads - use more on HPC\n",
    "    n_threads = max(8, int(cpu_count_physical * 0.8))\n",
    "    n_threads = min(n_threads, 64)  # Cap at 64\n",
    "\n",
    "    # Scale sample size based on RAM - aggressive for HPC\n",
    "    # Assume ~0.5GB per 10k samples (embeddings are small)\n",
    "    sample_size = min(50000, int(total_ram_gb * 250))\n",
    "    sample_size = max(15000, sample_size)\n",
    "\n",
    "    # Scale FAISS nprobe based on CPU power - more thorough search on HPC\n",
    "    faiss_nprobe = min(64, max(16, cpu_count_physical))\n",
    "\n",
    "    # Scale N_NEIGHBORS based on RAM and CPU\n",
    "    n_neighbors = min(300, max(100, int(total_ram_gb * 3)))\n",
    "\n",
    "    # Bootstrap and stability pairs scale with workers\n",
    "    n_bootstrap = max(2, min(10, max_workers // 12))\n",
    "    stability_pairs = min(10000, max(2000, max_workers * 75))\n",
    "\n",
    "    # Batch sizes scale with RAM - larger on HPC\n",
    "    batch_size = min(50000, max(10000, int(total_ram_gb * 200)))\n",
    "\n",
    "    # Search batch size for FAISS\n",
    "    search_batch_size = min(100000, max(30000, int(total_ram_gb * 300)))\n",
    "\n",
    "    # Edge batch size\n",
    "    edge_batch_size = min(50000, max(20000, int(total_ram_gb * 150)))\n",
    "\n",
    "    # Poem-level workers\n",
    "    poem_max_workers = max(16, int(cpu_count_logical * 0.85))\n",
    "    poem_max_workers = min(poem_max_workers, 128)\n",
    "\n",
    "    # Poem batch size\n",
    "    poem_batch_size = min(100000, max(50000, int(total_ram_gb * 300)))\n",
    "\n",
    "    config = {\n",
    "        'max_workers': max_workers,\n",
    "        'n_threads': n_threads,\n",
    "        'sample_size': sample_size,\n",
    "        'faiss_nprobe': faiss_nprobe,\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'n_bootstrap': n_bootstrap,\n",
    "        'stability_pairs': stability_pairs,\n",
    "        'batch_size': batch_size,\n",
    "        'search_batch_size': search_batch_size,\n",
    "        'edge_batch_size': edge_batch_size,\n",
    "        'poem_max_workers': poem_max_workers,\n",
    "        'poem_batch_size': poem_batch_size,\n",
    "        'cpu_physical': cpu_count_physical,\n",
    "        'cpu_logical': cpu_count_logical,\n",
    "        'ram_gb': total_ram_gb,\n",
    "        'gpu_available': gpu_available,\n",
    "        'gpu_count': gpu_count,\n",
    "        'gpu_memory_gb': gpu_memory_gb,\n",
    "        'gpu_names': gpu_names\n",
    "    }\n",
    "\n",
    "    logger.info(\"OPTIMIZED CONFIGURATION FOR HPC\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Max workers (parallel):   {max_workers}\")\n",
    "    logger.info(f\"FAISS threads:            {n_threads}\")\n",
    "    logger.info(f\"Sample size:              {sample_size:,}\")\n",
    "    logger.info(f\"FAISS nprobe:             {faiss_nprobe}\")\n",
    "    logger.info(f\"N neighbors:              {n_neighbors}\")\n",
    "    logger.info(f\"N bootstrap:              {n_bootstrap}\")\n",
    "    logger.info(f\"Stability pairs:          {stability_pairs:,}\")\n",
    "    logger.info(f\"Batch size:               {batch_size:,}\")\n",
    "    logger.info(f\"Search batch size:        {search_batch_size:,}\")\n",
    "    logger.info(f\"Edge batch size:          {edge_batch_size:,}\")\n",
    "    logger.info(f\"Poem max workers:         {poem_max_workers}\")\n",
    "    logger.info(f\"Poem batch size:          {poem_batch_size:,}\")\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Save resource config for reference\n",
    "    resource_info_path = CHECKPOINT_DIR / 'resource_config.pkl'\n",
    "    with open(resource_info_path, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    logger.info(f\"Resource configuration saved to: {resource_info_path}\\n\")\n",
    "\n",
    "    return config\n",
    "\n",
    "# Get optimal configuration\n",
    "resource_config = detect_optimal_resources()\n",
    "\n",
    "N_NEIGHBORS = resource_config['n_neighbors']\n",
    "BATCH_SIZE = resource_config['batch_size']\n",
    "N_THREADS = resource_config['n_threads']\n",
    "FAISS_NPROBE = resource_config['faiss_nprobe']\n",
    "\n",
    "SAMPLE_SIZE = resource_config['sample_size']\n",
    "N_BOOTSTRAP = resource_config['n_bootstrap']\n",
    "STABILITY_PAIRS = resource_config['stability_pairs']\n",
    "MAX_WORKERS = resource_config['max_workers']\n",
    "\n",
    "SEARCH_BATCH_SIZE = resource_config['search_batch_size']\n",
    "EDGE_BATCH_SIZE = resource_config['edge_batch_size']\n",
    "POEM_MAX_WORKERS = resource_config['poem_max_workers']\n",
    "POEM_BATCH_SIZE = resource_config['poem_batch_size']\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"HPC-OPTIMIZED SEMANTIC CLUSTERING PIPELINE\")\n",
    "logger.info(f\"Sample: {SAMPLE_SIZE:,}, Bootstrap: {N_BOOTSTRAP}, Workers: {MAX_WORKERS}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "resource_monitor.start()\n",
    "script_start_time = time.time()\n",
    "\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    return text.strip()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "timing_logger.start_stage(\"01_loading_embeddings\")\n",
    "\n",
    "embeddings_file = CHECKPOINT_DIR / 'embeddings.npz'\n",
    "metadata_file = CHECKPOINT_DIR / 'metadata.pkl.gz'\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"Loading checkpoint\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    embeddings_data = np.load(embeddings_file)\n",
    "    embeddings = embeddings_data['embeddings'].astype('float32')\n",
    "\n",
    "    with gzip.open(metadata_file, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    df = pd.read_parquet(CHECKPOINT_DIR / 'df_minimal.parquet')\n",
    "    source_datasets = metadata['source_datasets']\n",
    "    dataset_to_indices = metadata['dataset_to_indices']\n",
    "\n",
    "    logger.info(f\"Loaded {len(embeddings):,} embeddings in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "    # Load previous embedding timing if available\n",
    "    timing_file = CHECKPOINT_DIR / 'timing_metadata.pkl'\n",
    "    if timing_file.exists():\n",
    "        with open(timing_file, 'rb') as f:\n",
    "            embed_timing = pickle.load(f)\n",
    "            if 'gpu_used' in embed_timing:\n",
    "                logger.info(f\"Previous embedding GPU: {embed_timing.get('gpu_used', 'N/A')}\")\n",
    "\n",
    "    LOADED_FROM_CHECKPOINT = True\n",
    "\n",
    "else:\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"No checkpoint found - creating embeddings\")\n",
    "    logger.info(\"=\"*80)\n",
    "    embedding_start_time = time.time()\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['verse', 'source_dataset'])\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() < 256]\n",
    "    df['verse'] = df['verse'].apply(preprocess_text)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    logger.info(f\"Total verses: {len(df):,}\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    model_name = 'kevinkrahn/shlm-grc-en'\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Log GPU details for embeddings\n",
    "    gpu_info_str = \"CPU only\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_idx = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_idx)\n",
    "        gpu_mem = torch.cuda.get_device_properties(gpu_idx).total_memory / (1024**3)\n",
    "        gpu_info_str = f\"GPU {gpu_idx}: {gpu_name} ({gpu_mem:.2f} GB)\"\n",
    "        logger.info(f\"GPU for embeddings: {gpu_info_str}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    logger.info(\"Computing embeddings...\")\n",
    "    verses = df['verse'].tolist()\n",
    "    embeddings = []\n",
    "\n",
    "    # Scale embedding batch size based on GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        embed_batch_size = min(128, max(32, int(gpu_mem_gb * 8)))\n",
    "    else:\n",
    "        embed_batch_size = 16\n",
    "\n",
    "    logger.info(f\"Embedding batch size: {embed_batch_size}\")\n",
    "\n",
    "    for i in tqdm(range(0, len(verses), embed_batch_size), desc=\"Embedding\"):\n",
    "        try:\n",
    "            batch = verses[i:i+embed_batch_size]\n",
    "            encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "\n",
    "            batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error at batch {i}: {e}\")\n",
    "            embeddings.append(np.zeros((len(batch), model.config.hidden_size), dtype=np.float32))\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype('float32')\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    source_datasets = df['source_dataset'].values\n",
    "    dataset_to_indices = defaultdict(list)\n",
    "    for idx, dataset in enumerate(source_datasets):\n",
    "        dataset_to_indices[dataset].append(idx)\n",
    "\n",
    "    logger.info(\"\\nSaving checkpoint...\")\n",
    "\n",
    "    np.savez_compressed(CHECKPOINT_DIR / 'embeddings.npz', embeddings=embeddings)\n",
    "    embedding_total_time = time.time() - embedding_start_time\n",
    "\n",
    "    resource_monitor.stop()\n",
    "    resource_stats = resource_monitor.get_stats()\n",
    "    resource_monitor.start()  # Restart monitoring\n",
    "\n",
    "    checkpoint_timing = {\n",
    "        'embedding_generation_time': embedding_total_time,\n",
    "        'total_verses_processed': len(df),\n",
    "        'embedding_batch_size': embed_batch_size,\n",
    "        'device_used': str(device),\n",
    "        'gpu_used': gpu_info_str,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'peak_ram_gb': resource_stats['peak_ram_gb'],\n",
    "        'avg_ram_gb': resource_stats['avg_ram_gb'],\n",
    "        'peak_gpu_mem_gb': resource_stats['peak_gpu_mem_gb'],\n",
    "        'avg_gpu_mem_gb': resource_stats['avg_gpu_mem_gb']\n",
    "    }\n",
    "\n",
    "    with open(CHECKPOINT_DIR / 'timing_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(checkpoint_timing, f)\n",
    "\n",
    "    essential_cols = ['verse', 'source_dataset']\n",
    "    for col in ['idoriginal_poem', 'idgroup', 'order']:\n",
    "        if col in df.columns:\n",
    "            essential_cols.append(col)\n",
    "\n",
    "    df_minimal = df[essential_cols].copy()\n",
    "    for col in df_minimal.columns:\n",
    "        if df_minimal[col].dtype == 'object':\n",
    "            df_minimal[col] = df_minimal[col].astype(str)\n",
    "            df_minimal[col] = df_minimal[col].replace('nan', None).replace('None', None)\n",
    "\n",
    "    df_minimal.to_parquet(CHECKPOINT_DIR / 'df_minimal.parquet',\n",
    "                          compression='gzip', index=True)\n",
    "\n",
    "    metadata = {\n",
    "        'source_datasets': source_datasets,\n",
    "        'dataset_to_indices': dataset_to_indices\n",
    "    }\n",
    "    with gzip.open(CHECKPOINT_DIR / 'metadata.pkl.gz', 'wb') as f:\n",
    "        pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    logger.info(\"Checkpoint saved\")\n",
    "    logger.info(f\"GPU used for embeddings: {gpu_info_str}\")\n",
    "    LOADED_FROM_CHECKPOINT = False\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "# [REST OF THE SCRIPT CONTINUES WITH THE SAME LOGIC BUT USING DYNAMIC PARAMETERS]\n",
    "# I'll include key sections with modifications...\n",
    "\n",
    "timing_logger.start_stage(\"02_sample_preparation\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Fast sample preparation\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        if n_from_dataset > 0:\n",
    "            sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "            sample_indices.extend(sampled)\n",
    "\n",
    "    return np.array(sorted(sample_indices))\n",
    "\n",
    "sample_indices = stratified_sample(df, n_sample=SAMPLE_SIZE)\n",
    "sample_embeddings = embeddings[sample_indices].copy()\n",
    "sample_dataset_map = np.array([source_datasets[i] for i in sample_indices])\n",
    "\n",
    "logger.info(f\"Sample size: {len(sample_indices):,}\")\n",
    "\n",
    "logger.info(\"Building FAISS index...\")\n",
    "logger.info(f\"Using {N_THREADS} threads for FAISS operations\")\n",
    "faiss.omp_set_num_threads(N_THREADS)\n",
    "\n",
    "start_time = time.time()\n",
    "dimension = embeddings.shape[1]\n",
    "index_sample = faiss.IndexFlatIP(dimension)\n",
    "faiss.normalize_L2(sample_embeddings)\n",
    "index_sample.add(sample_embeddings)\n",
    "\n",
    "k = min(200, len(sample_embeddings) - 1)\n",
    "similarities, indices = index_sample.search(sample_embeddings, k)\n",
    "\n",
    "logger.info(f\"Neighbor search complete in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "\n",
    "timing_logger.start_stage(\"03_parameter_grid_setup\")\n",
    "\n",
    "threshold_percentiles_coarse = [96, 97, 98, 99]\n",
    "resolutions_coarse = np.logspace(-6, -1, 8)\n",
    "\n",
    "logger.info(f\"\\nCoarse parameter grid:\")\n",
    "logger.info(f\"  Thresholds: {threshold_percentiles_coarse}\")\n",
    "logger.info(f\"  Resolutions: {len(resolutions_coarse)} values\")\n",
    "logger.info(f\"  Total: {len(threshold_percentiles_coarse) * len(resolutions_coarse)} combinations\")\n",
    "\n",
    "logger.info(\"\\nPrecomputing cross-dataset similarities...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cross_similarities = []\n",
    "for i in range(len(sample_embeddings)):\n",
    "    neighbor_datasets = sample_dataset_map[indices[i, 1:]]\n",
    "    cross_mask = neighbor_datasets != sample_dataset_map[i]\n",
    "    cross_similarities.extend(similarities[i, 1:][cross_mask])\n",
    "\n",
    "cross_similarities = np.array(cross_similarities)\n",
    "logger.info(f\"Collected {len(cross_similarities):,} pairs in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "all_percentiles = list(range(50, 100, 5))\n",
    "threshold_lookup = {p: np.percentile(cross_similarities, p) for p in all_percentiles}\n",
    "\n",
    "logger.info(f\"Threshold range: {threshold_lookup[50]:.4f} (P50) to {threshold_lookup[95]:.4f} (P95)\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "def compute_stability_fast(partitions_list, n_nodes, sample_size=STABILITY_PAIRS):\n",
    "    if n_nodes < 100:\n",
    "        return 0.0\n",
    "\n",
    "    n_partitions = len(partitions_list)\n",
    "    n_sample_pairs = min(sample_size, n_nodes * (n_nodes - 1) // 2)\n",
    "\n",
    "    pairs_i = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    pairs_j = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    valid = pairs_i != pairs_j\n",
    "    pairs_i = pairs_i[valid][:n_sample_pairs]\n",
    "    pairs_j = pairs_j[valid][:n_sample_pairs]\n",
    "\n",
    "    coclustering = 0\n",
    "    for membership in partitions_list:\n",
    "        membership_arr = np.array(membership)\n",
    "        matches = membership_arr[pairs_i] == membership_arr[pairs_j]\n",
    "        coclustering += np.sum(matches)\n",
    "\n",
    "    stability = coclustering / (len(pairs_i) * n_partitions)\n",
    "    return stability\n",
    "\n",
    "def evaluate_single_combination(args):\n",
    "    threshold_pct, threshold, resolution, edges_data, weights_data, dataset_map, n_nodes = args\n",
    "\n",
    "    try:\n",
    "        edge_mask = weights_data >= threshold\n",
    "        edges = edges_data[edge_mask]\n",
    "        weights = weights_data[edge_mask]\n",
    "\n",
    "        if len(edges) == 0:\n",
    "            return None\n",
    "\n",
    "        g = ig.Graph(n=n_nodes, edges=edges.tolist(), directed=False)\n",
    "        g.es['weight'] = weights.tolist()\n",
    "\n",
    "        bootstrap_memberships = []\n",
    "        bootstrap_qualities = []\n",
    "\n",
    "        for seed in range(N_BOOTSTRAP):\n",
    "            partition = la.find_partition(\n",
    "                g,\n",
    "                la.CPMVertexPartition,\n",
    "                weights='weight',\n",
    "                resolution_parameter=resolution,\n",
    "                n_iterations=5,\n",
    "                seed=seed\n",
    "            )\n",
    "            bootstrap_memberships.append(partition.membership)\n",
    "            bootstrap_qualities.append(partition.quality())\n",
    "\n",
    "        membership = bootstrap_memberships[0]\n",
    "        n_clusters = len(set(membership))\n",
    "\n",
    "        if n_clusters == 0 or n_clusters == n_nodes:\n",
    "            return None\n",
    "\n",
    "        stability = compute_stability_fast(bootstrap_memberships, n_nodes)\n",
    "        avg_size = n_nodes / n_clusters\n",
    "\n",
    "        cluster_datasets = defaultdict(set)\n",
    "        cluster_sizes = defaultdict(int)\n",
    "\n",
    "        for idx, cid in enumerate(membership):\n",
    "            cluster_datasets[cid].add(dataset_map[idx])\n",
    "            cluster_sizes[cid] += 1\n",
    "\n",
    "        n_cross_clusters = sum(1 for datasets in cluster_datasets.values()\n",
    "                              if len(datasets) > 1)\n",
    "        pct_cross_clusters = (n_cross_clusters / n_clusters * 100) if n_clusters > 0 else 0\n",
    "        n_singleton = sum(1 for size in cluster_sizes.values() if size == 1)\n",
    "\n",
    "        avg_quality = np.mean(bootstrap_qualities)\n",
    "        modularity = g.modularity(membership, weights='weight')\n",
    "\n",
    "        return {\n",
    "            'threshold_percentile': threshold_pct,\n",
    "            'threshold_value': threshold,\n",
    "            'resolution': resolution,\n",
    "            'n_edges': g.ecount(),\n",
    "            'graph_density': 2*g.ecount()/(n_nodes*(n_nodes-1)) if n_nodes > 1 else 0,\n",
    "            'stability': stability,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singleton': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'n_cross_clusters': n_cross_clusters,\n",
    "            'pct_cross_clusters': pct_cross_clusters,\n",
    "            'avg_quality': avg_quality,\n",
    "            'modularity': modularity\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error at P{threshold_pct}, res={resolution:.2e}: {e}\")\n",
    "        return None\n",
    "\n",
    "timing_logger.start_stage(\"04_edge_precomputation\")\n",
    "\n",
    "logger.info(\"\\nPrecomputing edge structures...\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_edges_list = []\n",
    "all_weights_list = []\n",
    "\n",
    "for i in range(len(sample_embeddings)):\n",
    "    dataset_i = sample_dataset_map[i]\n",
    "    neighbors = indices[i, 1:]\n",
    "    sims = similarities[i, 1:]\n",
    "\n",
    "    valid_mask = neighbors > i\n",
    "    valid_neighbors = neighbors[valid_mask]\n",
    "    valid_sims = sims[valid_mask]\n",
    "\n",
    "    if len(valid_neighbors) > 0:\n",
    "        neighbor_datasets = sample_dataset_map[valid_neighbors]\n",
    "        cross_mask = neighbor_datasets != dataset_i\n",
    "\n",
    "        final_neighbors = valid_neighbors[cross_mask]\n",
    "        final_sims = valid_sims[cross_mask]\n",
    "\n",
    "        for j, sim in zip(final_neighbors, final_sims):\n",
    "            all_edges_list.append([i, j])\n",
    "            all_weights_list.append(sim)\n",
    "\n",
    "all_edges = np.array(all_edges_list, dtype=np.int32)\n",
    "all_weights = np.array(all_weights_list, dtype=np.float32)\n",
    "\n",
    "logger.info(f\"Precomputed {len(all_edges):,} edges in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"05_coarse_sweep\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Stage 1: Coarse parallel sweep\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "coarse_args = []\n",
    "for threshold_pct in threshold_percentiles_coarse:\n",
    "    if threshold_pct in threshold_lookup:\n",
    "        threshold = threshold_lookup[threshold_pct]\n",
    "    else:\n",
    "        threshold = np.percentile(cross_similarities, threshold_pct)\n",
    "        threshold_lookup[threshold_pct] = threshold\n",
    "\n",
    "    for resolution in resolutions_coarse:\n",
    "        coarse_args.append((\n",
    "            threshold_pct,\n",
    "            threshold,\n",
    "            resolution,\n",
    "            all_edges,\n",
    "            all_weights,\n",
    "            sample_dataset_map,\n",
    "            len(sample_embeddings)\n",
    "        ))\n",
    "\n",
    "logger.info(f\"Testing {len(coarse_args)} combinations with {MAX_WORKERS} workers...\")\n",
    "start_time = time.time()\n",
    "\n",
    "coarse_results = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(evaluate_single_combination, args): args\n",
    "               for args in coarse_args}\n",
    "\n",
    "    with tqdm(total=len(futures), desc=\"Coarse sweep\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                coarse_results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "logger.info(f\"Coarse sweep complete in {time.time()-start_time:.1f}s\")\n",
    "logger.info(f\"  Valid results: {len(coarse_results)} / {len(coarse_args)}\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"06_fine_sweep\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Stage 2: Fine sweep around best region\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "if len(coarse_results) == 0:\n",
    "    logger.error(\"No valid coarse results! Cannot proceed with fine sweep.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "coarse_df = pd.DataFrame(coarse_results)\n",
    "\n",
    "best_coarse = coarse_df.loc[coarse_df['stability'].idxmax()]\n",
    "best_thresh_pct = best_coarse['threshold_percentile']\n",
    "best_res_coarse = best_coarse['resolution']\n",
    "\n",
    "logger.info(f\"Best coarse result (by stability): P{best_thresh_pct}, res={best_res_coarse:.2e}\")\n",
    "logger.info(f\"  Stability: {best_coarse['stability']:.3f}\")\n",
    "logger.info(f\"  Clusters: {best_coarse['n_clusters']:,.0f}\")\n",
    "logger.info(f\"  Cross-dataset: {best_coarse['pct_cross_clusters']:.1f}%\")\n",
    "\n",
    "thresh_fine = [max(50, best_thresh_pct - 10),\n",
    "               max(50, best_thresh_pct - 5),\n",
    "               best_thresh_pct,\n",
    "               min(95, best_thresh_pct + 5),\n",
    "               min(95, best_thresh_pct + 10)]\n",
    "thresh_fine = sorted(list(set(thresh_fine)))\n",
    "\n",
    "log_res = np.log10(best_res_coarse)\n",
    "res_fine = np.logspace(log_res - 0.5, log_res + 0.5, 7)\n",
    "\n",
    "logger.info(f\"\\nFine grid:\")\n",
    "logger.info(f\"  Thresholds: {thresh_fine}\")\n",
    "logger.info(f\"  Resolutions: {len(res_fine)} values around {best_res_coarse:.2e}\")\n",
    "logger.info(f\"  Total: {len(thresh_fine) * len(res_fine)} combinations\")\n",
    "\n",
    "fine_args = []\n",
    "for threshold_pct in thresh_fine:\n",
    "    if threshold_pct in threshold_lookup:\n",
    "        threshold = threshold_lookup[threshold_pct]\n",
    "    else:\n",
    "        threshold = np.percentile(cross_similarities, threshold_pct)\n",
    "        threshold_lookup[threshold_pct] = threshold\n",
    "\n",
    "    for resolution in res_fine:\n",
    "        already_tested = any(\n",
    "            np.isclose(r['threshold_percentile'], threshold_pct) and\n",
    "            np.isclose(r['resolution'], resolution, rtol=0.1)\n",
    "            for r in coarse_results\n",
    "        )\n",
    "        if not already_tested:\n",
    "            fine_args.append((\n",
    "                threshold_pct,\n",
    "                threshold,\n",
    "                resolution,\n",
    "                all_edges,\n",
    "                all_weights,\n",
    "                sample_dataset_map,\n",
    "                len(sample_embeddings)\n",
    "            ))\n",
    "\n",
    "logger.info(f\"Testing {len(fine_args)} new combinations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "fine_results = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(evaluate_single_combination, args): args\n",
    "               for args in fine_args}\n",
    "\n",
    "    with tqdm(total=len(futures), desc=\"Fine sweep\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                fine_results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "logger.info(f\"Fine sweep complete in {time.time()-start_time:.1f}s\")\n",
    "logger.info(f\"  Valid results: {len(fine_results)} / {len(fine_args)}\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"07_parameter_analysis\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Final analysis - selection by stability\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "all_results = coarse_results + fine_results\n",
    "sweep_df = pd.DataFrame(all_results)\n",
    "\n",
    "sweep_df = sweep_df.sort_values('stability', ascending=False)\n",
    "sweep_df.to_csv(RESULTS_DIR / 'joint_parameter_sweep_results.csv', index=False)\n",
    "\n",
    "best_params = sweep_df.iloc[0]\n",
    "best_threshold = best_params['threshold_value']\n",
    "best_resolution = best_params['resolution']\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Top 5 parameter combinations (by stability)\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "for idx, (i, row) in enumerate(sweep_df.head(10).iterrows(), 1):\n",
    "    logger.info(f\"\\n#{idx}. Threshold: P{row['threshold_percentile']} ({row['threshold_value']:.4f}), \"\n",
    "               f\"Resolution: {row['resolution']:.2e}\")\n",
    "    logger.info(f\"     Stability: {row['stability']:.3f}\")\n",
    "    logger.info(f\"     Clusters: {row['n_clusters']:,.0f}, Singletons: {row['n_singleton']:,.0f}\")\n",
    "    logger.info(f\"     Cross-dataset: {row['n_cross_clusters']:,.0f} ({row['pct_cross_clusters']:.1f}%)\")\n",
    "    logger.info(f\"     Modularity: {row['modularity']:.3f}, Quality: {row['avg_quality']:.3f}\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Selected parameters (highest stability)\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Threshold: P{best_params['threshold_percentile']} = {best_threshold:.4f}\")\n",
    "logger.info(f\"Resolution: {best_resolution:.6e}\")\n",
    "logger.info(f\"Stability: {best_params['stability']:.3f}\")\n",
    "logger.info(f\"Clusters: {best_params['n_clusters']:,.0f}\")\n",
    "logger.info(f\"Cross-dataset: {best_params['pct_cross_clusters']:.1f}%\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"08_visualization\")\n",
    "\n",
    "logger.info(\"\\nCreating visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "pivot_data = sweep_df.pivot_table(\n",
    "    values='stability',\n",
    "    index='resolution',\n",
    "    columns='threshold_percentile',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "pivot_data.index = [f\"{res:.3e}\" for res in pivot_data.index]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax,\n",
    "           cbar_kws={'label': 'Stability'})\n",
    "ax.set_ylabel('Resolution', fontweight='bold')\n",
    "ax.set_xlabel('Threshold Percentile', fontweight='bold')\n",
    "ax.set_title('Stability Heatmap', fontweight='bold', fontsize=14)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.hist(sweep_df['stability'], bins=30, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(best_params['stability'], color='red', linestyle='--', linewidth=2,\n",
    "          label=f\"Best: {best_params['stability']:.3f}\")\n",
    "ax.set_xlabel('Stability', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Stability Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "for thresh_pct in sorted(sweep_df['threshold_percentile'].unique()):\n",
    "    data = sweep_df[sweep_df['threshold_percentile'] == thresh_pct]\n",
    "    ax.plot(data['resolution'], data['stability'], 'o-',\n",
    "           label=f'P{thresh_pct}', alpha=0.7, markersize=4)\n",
    "ax.axhline(best_params['stability'], color='red', linestyle='--',\n",
    "          linewidth=1, alpha=0.5, label=f'Best: {best_params[\"stability\"]:.3f}')\n",
    "ax.set_xlabel('Resolution', fontweight='bold')\n",
    "ax.set_ylabel('Stability', fontweight='bold')\n",
    "ax.set_title('Stability vs Resolution', fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(sweep_df['n_clusters'], sweep_df['stability'],\n",
    "                    c=sweep_df['threshold_value'], cmap='viridis',\n",
    "                    s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax.scatter(best_params['n_clusters'], best_params['stability'],\n",
    "          color='red', s=200, marker='*', edgecolors='black', linewidth=2,\n",
    "          label='Best', zorder=10)\n",
    "ax.set_xlabel('Number of Clusters', fontweight='bold')\n",
    "ax.set_ylabel('Stability', fontweight='bold')\n",
    "ax.set_title('Stability vs Cluster Count (colored by threshold)', fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "plt.colorbar(scatter, ax=ax, label='Threshold Value')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "fig.suptitle(f'Fast Joint Parameter Sweep - Stability-Based Selection (n={len(sweep_df)} combinations)',\n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'fast_parameter_sweep_summary.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "logger.info(f\"Plot saved: {plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "summary = {\n",
    "    'best_threshold_percentile': best_params['threshold_percentile'],\n",
    "    'best_threshold_value': best_threshold,\n",
    "    'best_resolution': best_resolution,\n",
    "    'stability': best_params['stability'],\n",
    "    'n_clusters': best_params['n_clusters'],\n",
    "    'n_singleton': best_params['n_singleton'],\n",
    "    'avg_cluster_size': best_params['avg_cluster_size'],\n",
    "    'n_cross_clusters': best_params['n_cross_clusters'],\n",
    "    'pct_cross_clusters': best_params['pct_cross_clusters'],\n",
    "    'modularity': best_params['modularity'],\n",
    "    'n_combinations_tested': len(sweep_df),\n",
    "    'sample_size': SAMPLE_SIZE,\n",
    "    'n_bootstrap': N_BOOTSTRAP,\n",
    "    'selection_criterion': 'stability'\n",
    "}\n",
    "\n",
    "pd.DataFrame([summary]).to_csv(RESULTS_DIR / 'best_parameters_summary.csv', index=False)\n",
    "\n",
    "params_for_clustering = {\n",
    "    'threshold': best_threshold,\n",
    "    'resolution': best_resolution\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'optimal_parameters.pkl', 'wb') as f:\n",
    "    pickle.dump(params_for_clustering, f)\n",
    "\n",
    "logger.info(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"Fast joint parameter sweep complete\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "timing_logger.start_stage(\"09_graph_construction\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Stage 3: Fast approximate graph construction\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(\"Normalizing embeddings...\")\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "logger.info(\"Building FAISS IVF index...\")\n",
    "logger.info(f\"Using {N_THREADS} threads for FAISS\")\n",
    "faiss.omp_set_num_threads(N_THREADS)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nlist = min(int(4 * np.sqrt(len(embeddings))), 16384)\n",
    "nprobe = FAISS_NPROBE\n",
    "\n",
    "logger.info(f\"FAISS params: nlist={nlist}, nprobe={nprobe}\")\n",
    "logger.info(f\"Targeting {N_NEIGHBORS} neighbors per node\")\n",
    "\n",
    "\n",
    "on_gpu = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        logger.info(\"Attempting GPU setup for FAISS...\")\n",
    "\n",
    "        res = faiss.StandardGpuResources()\n",
    "\n",
    "        cpu_quantizer = faiss.IndexFlatIP(dimension)\n",
    "        cpu_index = faiss.IndexIVFFlat(cpu_quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        index_full = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "\n",
    "        logger.info(\"Successfully created GPU index\")\n",
    "        on_gpu = True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"GPU setup failed: {str(e)}\")\n",
    "        logger.info(\"Falling back to CPU (this is fine, just slower)\")\n",
    "\n",
    "        quantizer = faiss.IndexFlatIP(dimension)\n",
    "        index_full = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "        on_gpu = False\n",
    "else:\n",
    "    logger.info(\"CUDA not available, using CPU\")\n",
    "    quantizer = faiss.IndexFlatIP(dimension)\n",
    "    index_full = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "    on_gpu = False\n",
    "\n",
    "logger.info(f\"Training index on {'GPU' if on_gpu else 'CPU'}...\")\n",
    "\n",
    "if len(embeddings) > 1000000:\n",
    "    train_sample_size = min(500000, len(embeddings))\n",
    "    train_indices = np.random.choice(len(embeddings), train_sample_size, replace=False)\n",
    "    train_data = embeddings[train_indices].copy()\n",
    "    logger.info(f\"Training on sample of {train_sample_size:,} vectors...\")\n",
    "    index_full.train(train_data)\n",
    "else:\n",
    "    index_full.train(embeddings)\n",
    "\n",
    "logger.info(\"Adding vectors to index...\")\n",
    "index_full.add(embeddings)\n",
    "index_full.nprobe = nprobe\n",
    "\n",
    "logger.info(f\"FAISS index built in {time.time()-start_time:.1f}s (mode: {'GPU' if on_gpu else 'CPU'})\")\n",
    "\n",
    "logger.info(f\"Searching for {N_NEIGHBORS} nearest neighbors...\")\n",
    "SEARCH_BATCH_SIZE = 50000\n",
    "\n",
    "start_time = time.time()\n",
    "all_similarities = []\n",
    "all_indices = []\n",
    "\n",
    "n_search_batches = (len(embeddings) + SEARCH_BATCH_SIZE - 1) // SEARCH_BATCH_SIZE\n",
    "logger.info(f\"Processing {n_search_batches} search batches of size {SEARCH_BATCH_SIZE:,}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(embeddings), SEARCH_BATCH_SIZE), desc=\"Neighbor search\", total=n_search_batches):\n",
    "    batch_end = min(i + SEARCH_BATCH_SIZE, len(embeddings))\n",
    "    batch_emb = embeddings[i:batch_end]\n",
    "\n",
    "    D, I = index_full.search(batch_emb, N_NEIGHBORS)\n",
    "    all_similarities.append(D)\n",
    "    all_indices.append(I)\n",
    "\n",
    "all_similarities = np.vstack(all_similarities)\n",
    "all_indices = np.vstack(all_indices)\n",
    "\n",
    "logger.info(f\"Neighbor search complete in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"10_edge_construction\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Stage 4: Memory-efficient edge construction\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "edge_checkpoint = CHECKPOINT_DIR / 'edges_checkpoint.npz'\n",
    "threshold = best_threshold\n",
    "\n",
    "if edge_checkpoint.exists():\n",
    "    logger.info(\"Found edge checkpoint - loading...\")\n",
    "    edge_data = np.load(edge_checkpoint)\n",
    "    all_edges = [(int(i), int(j)) for i, j in edge_data['edges']]\n",
    "    all_weights = edge_data['weights'].tolist()\n",
    "    logger.info(f\"Loaded {len(all_edges):,} edges from checkpoint\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"Building edge list with cross-dataset filtering...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_edges = []\n",
    "    all_weights = []\n",
    "\n",
    "    EDGE_BATCH_SIZE = 20000\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(embeddings), EDGE_BATCH_SIZE), desc=\"Building edges\"):\n",
    "        batch_end = min(batch_start + EDGE_BATCH_SIZE, len(embeddings))\n",
    "\n",
    "        batch_size = batch_end - batch_start\n",
    "\n",
    "        for local_idx in range(batch_size):\n",
    "            node_idx = batch_start + local_idx\n",
    "            dataset_i = source_datasets[node_idx]\n",
    "\n",
    "            neighbors = all_indices[node_idx, 1:]\n",
    "            sims = all_similarities[node_idx, 1:]\n",
    "\n",
    "            valid_mask = (neighbors > node_idx) & (sims >= threshold)\n",
    "            valid_neighbors = neighbors[valid_mask]\n",
    "            valid_sims = sims[valid_mask]\n",
    "\n",
    "            if len(valid_neighbors) > 0:\n",
    "                neighbor_datasets = np.array([source_datasets[n] for n in valid_neighbors])\n",
    "                cross_dataset_mask = neighbor_datasets != dataset_i\n",
    "\n",
    "                final_neighbors = valid_neighbors[cross_dataset_mask]\n",
    "                final_sims = valid_sims[cross_dataset_mask]\n",
    "\n",
    "                for neighbor, sim in zip(final_neighbors, final_sims):\n",
    "                    all_edges.append((node_idx, int(neighbor)))\n",
    "                    all_weights.append(float(sim))\n",
    "\n",
    "        if (batch_start // EDGE_BATCH_SIZE) % 10 == 0 and batch_start > 0:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "    logger.info(f\"{len(all_edges):,} cross-dataset edges in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "    logger.info(\"Saving edge checkpoint...\")\n",
    "    np.savez_compressed(\n",
    "        CHECKPOINT_DIR / 'edges_checkpoint.npz',\n",
    "        edges=np.array(all_edges, dtype=np.int32),\n",
    "        weights=np.array(all_weights, dtype=np.float32)\n",
    "    )\n",
    "    logger.info(\"Edge checkpoint saved\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"11_verse_clustering\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Stage 5: Hierarchical Leiden clustering\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "logger.info(\"Building graph...\")\n",
    "g = ig.Graph(n=len(embeddings), edges=all_edges, directed=False)\n",
    "g.es['weight'] = all_weights\n",
    "\n",
    "logger.info(f\"Full graph: {g.vcount():,} nodes, {g.ecount():,} edges\")\n",
    "\n",
    "logger.info(\"\\nStep 1: Coarse clustering (fast)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "coarse_partition = la.find_partition(\n",
    "    g,\n",
    "    la.CPMVertexPartition,\n",
    "    weights='weight',\n",
    "    resolution_parameter=0.01,\n",
    "    n_iterations=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "coarse_labels = np.array(coarse_partition.membership)\n",
    "n_coarse = len(set(coarse_labels))\n",
    "\n",
    "logger.info(f\"{n_coarse:,} coarse clusters in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "coarse_cluster_info = defaultdict(lambda: {'datasets': set(), 'nodes': []})\n",
    "\n",
    "for idx, cid in enumerate(coarse_labels):\n",
    "    coarse_cluster_info[cid]['datasets'].add(source_datasets[idx])\n",
    "    coarse_cluster_info[cid]['nodes'].append(idx)\n",
    "\n",
    "cross_dataset_clusters = [cid for cid, info in coarse_cluster_info.items()\n",
    "                          if len(info['datasets']) > 1]\n",
    "\n",
    "logger.info(f\"Cross-dataset coarse clusters: {len(cross_dataset_clusters):,}\")\n",
    "\n",
    "logger.info(f\"\\nStep 2: Refining with resolution {best_resolution:.4f}...\")\n",
    "\n",
    "final_labels = coarse_labels.copy()\n",
    "next_cluster_id = n_coarse\n",
    "\n",
    "cluster_sizes = [(cid, len(coarse_cluster_info[cid]['nodes']))\n",
    "                 for cid in cross_dataset_clusters]\n",
    "cluster_sizes.sort(key=lambda x: x[1])\n",
    "\n",
    "refined_count = 0\n",
    "skipped_small = 0\n",
    "skipped_large = 0\n",
    "\n",
    "for coarse_cid, size in tqdm(cluster_sizes, desc=\"Refining\"):\n",
    "    if size < 10:\n",
    "        skipped_small += 1\n",
    "        continue\n",
    "\n",
    "    if size > 100000:\n",
    "        skipped_large += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cluster_nodes = coarse_cluster_info[coarse_cid]['nodes']\n",
    "        subg = g.subgraph(cluster_nodes)\n",
    "\n",
    "        if subg.ecount() > 0:\n",
    "            sub_partition = la.find_partition(\n",
    "                subg,\n",
    "                la.CPMVertexPartition,\n",
    "                weights='weight',\n",
    "                resolution_parameter=best_resolution,\n",
    "                n_iterations=10,\n",
    "                seed=42\n",
    "            )\n",
    "\n",
    "            sub_labels = np.array(sub_partition.membership)\n",
    "\n",
    "            if len(set(sub_labels)) > 1:\n",
    "                for sub_idx, sub_cid in enumerate(sub_labels):\n",
    "                    global_idx = cluster_nodes[sub_idx]\n",
    "                    final_labels[global_idx] = next_cluster_id + sub_cid\n",
    "\n",
    "                next_cluster_id += len(set(sub_labels))\n",
    "                refined_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error refining cluster {coarse_cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "logger.info(f\"Refined: {refined_count:,}, Skipped small: {skipped_small:,}, \"\n",
    "            f\"Skipped large: {skipped_large:,}\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "timing_logger.start_stage(\"12_verse_results\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Analyzing verse clustering results\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "cluster_datasets = defaultdict(set)\n",
    "cluster_verses = defaultdict(list)\n",
    "\n",
    "for idx, cid in enumerate(final_labels):\n",
    "    cluster_datasets[cid].add(source_datasets[idx])\n",
    "    cluster_verses[cid].append(idx)\n",
    "\n",
    "n_total_clusters = len(set(final_labels))\n",
    "n_cross = sum(1 for ds in cluster_datasets.values() if len(ds) > 1)\n",
    "cross_verses = sum(len(cluster_verses[cid]) for cid in set(final_labels)\n",
    "                  if len(cluster_datasets[cid]) > 1)\n",
    "\n",
    "logger.info(f\"\\nVerse-level clustering results:\")\n",
    "logger.info(f\"  Total clusters: {n_total_clusters:,}\")\n",
    "logger.info(f\"  Cross-dataset clusters: {n_cross:,} ({n_cross/n_total_clusters*100:.1f}%)\")\n",
    "logger.info(f\"  Cross-dataset verses: {cross_verses:,} ({cross_verses/len(final_labels)*100:.1f}%)\")\n",
    "\n",
    "df['cluster_id'] = final_labels\n",
    "\n",
    "cluster_info = []\n",
    "for cid, datasets in cluster_datasets.items():\n",
    "    if len(datasets) > 1:\n",
    "        verses = cluster_verses[cid]\n",
    "        counts = Counter(source_datasets[i] for i in verses)\n",
    "        cluster_info.append({\n",
    "            'cluster_id': cid,\n",
    "            'size': len(verses),\n",
    "            'n_datasets': len(datasets),\n",
    "            'datasets': ', '.join(sorted(datasets)),\n",
    "            'dataset_counts': dict(counts)\n",
    "        })\n",
    "\n",
    "cluster_info_df = pd.DataFrame(cluster_info).sort_values('size', ascending=False)\n",
    "\n",
    "logger.info(f\"\\nTop 20 cross-dataset clusters:\")\n",
    "if len(cluster_info_df) > 0:\n",
    "    print(cluster_info_df.head(20).to_string(index=False))\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Saving verse clustering results\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "df.to_csv(RESULTS_DIR / \"concatenated_cross_dataset_clusters.csv\", index=False)\n",
    "logger.info(f\"Full results: {RESULTS_DIR / 'concatenated_cross_dataset_clusters.csv'}\")\n",
    "\n",
    "cross_mask = df['cluster_id'].map(lambda cid: len(cluster_datasets[cid]) > 1)\n",
    "df_cross = df[cross_mask].copy()\n",
    "\n",
    "df_cross = df_cross[['verse', 'source_dataset', 'cluster_id']]\n",
    "df_cross.to_csv(RESULTS_DIR / \"cross_dataset_verses_only.csv\", index=False)\n",
    "logger.info(f\"Cross-dataset only: {RESULTS_DIR / 'cross_dataset_verses_only.csv'} ({len(df_cross):,} verses)\")\n",
    "\n",
    "cluster_info_df.to_csv(RESULTS_DIR / 'cross_dataset_clusters_summary.csv', index=False)\n",
    "logger.info(f\"Cluster summary: {RESULTS_DIR / 'cross_dataset_clusters_summary.csv'}\")\n",
    "\n",
    "verse_summary = {\n",
    "    'n_verses': len(df),\n",
    "    'n_datasets': len(set(source_datasets)),\n",
    "    'threshold': threshold,\n",
    "    'resolution': best_resolution,\n",
    "    'n_total_clusters': n_total_clusters,\n",
    "    'n_cross_clusters': n_cross,\n",
    "    'pct_cross_clusters': n_cross/n_total_clusters*100,\n",
    "    'n_cross_verses': cross_verses,\n",
    "    'pct_cross_verses': cross_verses/len(df)*100\n",
    "}\n",
    "\n",
    "pd.DataFrame([verse_summary]).to_csv(RESULTS_DIR / 'verse_clustering_summary.csv', index=False)\n",
    "logger.info(f\"Summary: {RESULTS_DIR / 'verse_clustering_summary.csv'}\")\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "INPUT_CSV = RESULTS_DIR / \"concatenated_cross_dataset_clusters.csv\"\n",
    "\n",
    "POEM_SAMPLE_SIZE = 15000\n",
    "JACCARD_THRESHOLD_RANGE = (0.8, 0.9, 2)\n",
    "POEM_MAX_WORKERS = 32\n",
    "POEM_BATCH_SIZE = 50000\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Poem-level clustering based on verse cluster Jaccard similarity\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Input: {INPUT_CSV}\")\n",
    "logger.info(f\"Sample size: {POEM_SAMPLE_SIZE:,}\")\n",
    "logger.info(f\"Jaccard threshold range: {JACCARD_THRESHOLD_RANGE}\")\n",
    "logger.info(f\"Workers: {POEM_MAX_WORKERS}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "@njit\n",
    "def compute_jaccard_similarity(a_clusters, b_clusters):\n",
    "    a_set = set(a_clusters)\n",
    "    b_set = set(b_clusters)\n",
    "\n",
    "    intersection = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "class UnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "\n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 0 for e in elements}\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "\n",
    "    def get_clusters(self):\n",
    "        clusters = defaultdict(set)\n",
    "        for elem in self.parent.keys():\n",
    "            clusters[self.find(elem)].add(elem)\n",
    "        return dict(clusters)\n",
    "\n",
    "def reconstruct_poems_from_verses(df):\n",
    "    timing_logger.start_stage(\"13_reconstruct_poems\")\n",
    "\n",
    "    logger.info(\"\\nReconstructing poems from verses...\")\n",
    "\n",
    "    required_cols = ['idoriginal_poem', 'cluster_id', 'source_dataset']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Error: '{col}' column not found in input CSV.\")\n",
    "\n",
    "    valid_mask = df['cluster_id'] != -1\n",
    "    df_valid = df[valid_mask].copy()\n",
    "\n",
    "    logger.info(f\"  Total verses: {len(df):,}\")\n",
    "    logger.info(f\"  Valid verses (cluster_id != -1): {len(df_valid):,}\")\n",
    "\n",
    "    df_valid['poem_composite_id'] = df_valid['idoriginal_poem'].astype(str) + '___' + df_valid['source_dataset'].astype(str)\n",
    "\n",
    "    if 'order' in df_valid.columns:\n",
    "        df_valid = df_valid.sort_values(['poem_composite_id', 'order'])\n",
    "    else:\n",
    "        df_valid = df_valid.sort_values('poem_composite_id')\n",
    "\n",
    "    poem_to_clusters = {}\n",
    "    poem_to_dataset = {}\n",
    "    poem_to_size = {}\n",
    "\n",
    "    for composite_id, group in df_valid.groupby('poem_composite_id'):\n",
    "        cluster_sequence = group['cluster_id'].values\n",
    "        poem_to_clusters[composite_id] = np.unique(cluster_sequence).astype(np.int32)\n",
    "        poem_to_dataset[composite_id] = group['source_dataset'].iloc[0]\n",
    "        poem_to_size[composite_id] = len(poem_to_clusters[composite_id])\n",
    "\n",
    "    logger.info(f\"  Reconstructed {len(poem_to_clusters):,} poems\")\n",
    "    logger.info(f\"  Average unique clusters per poem: {np.mean(list(poem_to_size.values())):.1f}\")\n",
    "\n",
    "    dataset_counts = defaultdict(int)\n",
    "    for dataset in poem_to_dataset.values():\n",
    "        dataset_counts[dataset] += 1\n",
    "\n",
    "    logger.info(f\"  Poems by dataset:\")\n",
    "    for dataset, count in sorted(dataset_counts.items()):\n",
    "        logger.info(f\"      {dataset}: {count:,}\")\n",
    "\n",
    "    poems_by_dataset = defaultdict(list)\n",
    "    for poem_id, dataset in poem_to_dataset.items():\n",
    "        poems_by_dataset[dataset].append(poem_id)\n",
    "\n",
    "    poem_metadata = {\n",
    "        'poem_to_size': poem_to_size,\n",
    "        'poems_by_dataset': dict(poems_by_dataset)\n",
    "    }\n",
    "\n",
    "    timing_logger.end_stage()\n",
    "    return poem_to_clusters, poem_to_dataset, poem_metadata\n",
    "\n",
    "def build_cluster_to_poems_index(poem_to_clusters):\n",
    "    logger.info(\"\\nBuilding cluster-to-poems inverted index...\")\n",
    "    cluster_to_poems = defaultdict(list)\n",
    "    for poem_id, cluster_array in tqdm(poem_to_clusters.items(), desc=\"Indexing\"):\n",
    "        for cluster_id in cluster_array:\n",
    "            cluster_to_poems[int(cluster_id)].append(poem_id)\n",
    "\n",
    "    for cluster_id in cluster_to_poems:\n",
    "        cluster_to_poems[cluster_id] = np.array(cluster_to_poems[cluster_id])\n",
    "\n",
    "    return dict(cluster_to_poems)\n",
    "\n",
    "def stratified_sample_poems(poem_to_clusters, poem_to_dataset, poem_to_size, n_sample=10000):\n",
    "    logger.info(f\"\\nStratified sampling of {n_sample:,} poems...\")\n",
    "\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    poem_metadata = []\n",
    "    for poem_id in poem_to_clusters.keys():\n",
    "        metadata = {\n",
    "            'poem_id': poem_id,\n",
    "            'n_verses': poem_to_size[poem_id],\n",
    "            'source': poem_to_dataset[poem_id]\n",
    "        }\n",
    "        poem_metadata.append(metadata)\n",
    "\n",
    "    poem_df = pd.DataFrame(poem_metadata)\n",
    "\n",
    "    poem_df['size_bin'] = pd.cut(poem_df['n_verses'],\n",
    "                                  bins=[0, 5, 10, 20, 50, np.inf],\n",
    "                                  labels=['tiny', 'small', 'medium', 'large', 'huge'])\n",
    "\n",
    "    sample_indices = []\n",
    "\n",
    "    for (source, size_bin), group in poem_df.groupby(['source', 'size_bin']):\n",
    "        n_in_group = len(group)\n",
    "        proportion = n_in_group / len(poem_df)\n",
    "        n_sample_group = max(1, int(n_sample * proportion))\n",
    "        n_sample_group = min(n_sample_group, n_in_group)\n",
    "\n",
    "        sampled = group.sample(n=n_sample_group, random_state=RANDOM_SEED)\n",
    "        sample_indices.extend(sampled['poem_id'].tolist())\n",
    "\n",
    "    if len(sample_indices) < n_sample:\n",
    "        remaining = n_sample - len(sample_indices)\n",
    "        available = set(poem_to_clusters.keys()) - set(sample_indices)\n",
    "        if available:\n",
    "            additional = np.random.choice(list(available),\n",
    "                                         size=min(remaining, len(available)),\n",
    "                                         replace=False)\n",
    "            sample_indices.extend(additional)\n",
    "\n",
    "    sample_indices = sample_indices[:n_sample]\n",
    "    logger.info(f\"  Sampled {len(sample_indices):,} poems\")\n",
    "\n",
    "    return sample_indices\n",
    "\n",
    "def find_cross_dataset_candidate_pairs_batched(poem_to_clusters, poem_to_dataset, cluster_to_poems, poems_by_dataset):\n",
    "    timing_logger.start_stage(\"14_find_poem_pairs\")\n",
    "\n",
    "    logger.info(\"\\nFinding cross-dataset candidate pairs (batched)...\")\n",
    "\n",
    "    datasets = list(poems_by_dataset.keys())\n",
    "    pair_file = CHECKPOINT_DIR / \"candidate_pairs.npz\"\n",
    "\n",
    "    if pair_file.exists():\n",
    "        logger.info(\"  Loading cached candidate pairs...\")\n",
    "        data = np.load(pair_file)\n",
    "        timing_logger.end_stage()\n",
    "        return set(zip(data['p1'], data['p2']))\n",
    "\n",
    "    all_pairs = set()\n",
    "\n",
    "    for i, dataset1 in enumerate(datasets):\n",
    "        for dataset2 in datasets[i+1:]:\n",
    "            logger.info(f\"  Processing {dataset1} x {dataset2}...\")\n",
    "            poems1 = poems_by_dataset[dataset1]\n",
    "            poems2 = poems_by_dataset[dataset2]\n",
    "            poems2_set = set(poems2)\n",
    "\n",
    "            batch_pairs = set()\n",
    "\n",
    "            for poem_id in tqdm(poems1, desc=f\"  {dataset1}\"):\n",
    "                clusters = poem_to_clusters[poem_id]\n",
    "\n",
    "                candidates = set()\n",
    "                for cluster_id in clusters:\n",
    "                    if int(cluster_id) in cluster_to_poems:\n",
    "                        candidates.update(cluster_to_poems[int(cluster_id)])\n",
    "\n",
    "                candidates = candidates & poems2_set\n",
    "\n",
    "                for other_poem in candidates:\n",
    "                    pair = tuple(sorted([poem_id, other_poem]))\n",
    "                    batch_pairs.add(pair)\n",
    "\n",
    "            all_pairs.update(batch_pairs)\n",
    "            logger.info(f\"    Found {len(batch_pairs):,} pairs\")\n",
    "\n",
    "    logger.info(f\"  Total candidate pairs: {len(all_pairs):,}\")\n",
    "\n",
    "    if all_pairs:\n",
    "        p1_list, p2_list = zip(*all_pairs)\n",
    "        np.savez_compressed(pair_file, p1=p1_list, p2=p2_list)\n",
    "        logger.info(f\"  Cached pairs to {pair_file}\")\n",
    "\n",
    "    timing_logger.end_stage()\n",
    "    return all_pairs\n",
    "\n",
    "def compute_cluster_cohesion(poem_to_clusters, cluster_assignments, max_sample=500):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    cohesions = []\n",
    "\n",
    "    clusters = defaultdict(list)\n",
    "    for poem_id in poem_ids:\n",
    "        cluster_id = cluster_assignments.get(poem_id)\n",
    "        if cluster_id is not None:\n",
    "            clusters[cluster_id].append(poem_id)\n",
    "\n",
    "    for cluster_id, cluster_poems in clusters.items():\n",
    "        if len(cluster_poems) < 2:\n",
    "            continue\n",
    "\n",
    "        if len(cluster_poems) > 30:\n",
    "            sampled = np.random.choice(cluster_poems, 30, replace=False)\n",
    "        else:\n",
    "            sampled = cluster_poems\n",
    "\n",
    "        overlaps = []\n",
    "        for i in range(len(sampled)):\n",
    "            for j in range(i+1, min(i+10, len(sampled))):\n",
    "                jaccard = compute_jaccard_similarity(\n",
    "                    poem_to_clusters[sampled[i]],\n",
    "                    poem_to_clusters[sampled[j]]\n",
    "                )\n",
    "                overlaps.append(jaccard)\n",
    "\n",
    "        if overlaps:\n",
    "            cohesions.append(np.mean(overlaps))\n",
    "\n",
    "        if len(cohesions) >= max_sample:\n",
    "            break\n",
    "\n",
    "    return np.mean(cohesions) if cohesions else 0.0\n",
    "\n",
    "def evaluate_single_poem_config(args):\n",
    "    jaccard_thresh, sample_poems, poem_to_clusters, candidate_pairs, poem_to_dataset = args\n",
    "\n",
    "    try:\n",
    "        sample_set = set(sample_poems)\n",
    "        cross_dataset_pairs = set()\n",
    "\n",
    "        for p1, p2 in candidate_pairs:\n",
    "            if p1 in sample_set and p2 in sample_set:\n",
    "                cross_dataset_pairs.add((p1, p2))\n",
    "\n",
    "        uf = UnionFind(sample_poems)\n",
    "        merges = 0\n",
    "\n",
    "        for p1, p2 in cross_dataset_pairs:\n",
    "            jaccard = compute_jaccard_similarity(\n",
    "                poem_to_clusters[p1],\n",
    "                poem_to_clusters[p2]\n",
    "            )\n",
    "\n",
    "            if jaccard >= jaccard_thresh:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "\n",
    "        poem_clusters = uf.get_clusters()\n",
    "        cluster_assignments = {}\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            for poem in poems:\n",
    "                cluster_assignments[poem] = cluster_id\n",
    "\n",
    "        n_clusters = len(poem_clusters)\n",
    "        cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "        n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "        avg_size = np.mean(cluster_sizes)\n",
    "        max_size = max(cluster_sizes) if cluster_sizes else 0\n",
    "\n",
    "        n_cross_dataset_clusters = 0\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "            if len(datasets) > 1:\n",
    "                n_cross_dataset_clusters += 1\n",
    "\n",
    "        cohesion = compute_cluster_cohesion(poem_to_clusters, cluster_assignments)\n",
    "\n",
    "        return {\n",
    "            'jaccard_threshold': jaccard_thresh,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singletons': n_singletons,\n",
    "            'n_cross_dataset_clusters': n_cross_dataset_clusters,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'merges': merges,\n",
    "            'n_cross_dataset_pairs': len(cross_dataset_pairs)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error at jaccard={jaccard_thresh:.2f}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_poem_parameters(poem_to_clusters, poem_to_dataset, poem_to_size, poems_by_dataset):\n",
    "    timing_logger.start_stage(\"15_poem_parameter_search\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"Grid search: Jaccard threshold\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    sample_poems = stratified_sample_poems(poem_to_clusters, poem_to_dataset,\n",
    "                                          poem_to_size, POEM_SAMPLE_SIZE)\n",
    "\n",
    "    sample_poems_set = set(sample_poems)\n",
    "    sample_poems_by_dataset = defaultdict(list)\n",
    "    for poem_id in sample_poems:\n",
    "        sample_poems_by_dataset[poem_to_dataset[poem_id]].append(poem_id)\n",
    "\n",
    "    cluster_to_poems = build_cluster_to_poems_index({p: poem_to_clusters[p] for p in sample_poems})\n",
    "\n",
    "    candidate_pairs = find_cross_dataset_candidate_pairs_batched(\n",
    "        {p: poem_to_clusters[p] for p in sample_poems},\n",
    "        poem_to_dataset,\n",
    "        cluster_to_poems,\n",
    "        sample_poems_by_dataset\n",
    "    )\n",
    "\n",
    "    jaccard_thresholds = np.linspace(JACCARD_THRESHOLD_RANGE[0],\n",
    "                                     JACCARD_THRESHOLD_RANGE[1],\n",
    "                                     int(JACCARD_THRESHOLD_RANGE[2]))\n",
    "\n",
    "    logger.info(f\"\\nParameter grid:\")\n",
    "    logger.info(f\"  Jaccard thresholds: {len(jaccard_thresholds)} values from {jaccard_thresholds[0]:.2f} to {jaccard_thresholds[-1]:.2f}\")\n",
    "\n",
    "    args_list = []\n",
    "    for jaccard_thresh in jaccard_thresholds:\n",
    "        args_list.append((\n",
    "            jaccard_thresh, sample_poems, poem_to_clusters,\n",
    "            candidate_pairs, poem_to_dataset\n",
    "        ))\n",
    "\n",
    "    logger.info(f\"\\nRunning grid search with {POEM_MAX_WORKERS} workers...\")\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=POEM_MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(evaluate_single_poem_config, args): args for args in args_list}\n",
    "        with tqdm(total=len(futures), desc=\"Grid search\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "\n",
    "    logger.info(f\"Grid search complete in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    def normalize(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val < 1e-10:\n",
    "            return pd.Series(0.5, index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "    cohesion_score = normalize(results_df['cohesion'])\n",
    "\n",
    "    cross_dataset_ratio = results_df['n_cross_dataset_clusters'] / (results_df['n_clusters'] + 1e-10)\n",
    "    cross_dataset_score = normalize(cross_dataset_ratio)\n",
    "\n",
    "    singleton_ratio = results_df['n_singletons'] / len(sample_poems)\n",
    "    balance_score = np.clip(1 - singleton_ratio, 0, 1)\n",
    "\n",
    "    results_df['quality_score'] = (\n",
    "        cohesion_score * 0.50 +\n",
    "        cross_dataset_score * 0.30 +\n",
    "        balance_score * 0.20\n",
    "    )\n",
    "\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    results_csv = RESULTS_DIR / 'poem_parameter_grid_search.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    logger.info(f\"Results saved: {results_csv}\")\n",
    "\n",
    "    logger.info(\"\\nCreating poem parameter visualizations...\")\n",
    "\n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Poem-level cross-dataset clustering: Grid search', fontsize=16, fontweight='bold')\n",
    "\n",
    "    best = results_df.iloc[0]\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['quality_score'], 'o-', linewidth=2, markersize=6)\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2, label='Best')\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Quality Score', fontweight='bold')\n",
    "    ax.set_title('Quality vs Jaccard', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['cohesion'], 'o-', linewidth=2, markersize=6, color='orange')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Cohesion', fontweight='bold')\n",
    "    ax.set_title('Cohesion vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['n_cross_dataset_clusters'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_title('Cross-Dataset Clusters vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['n_clusters'], 'o-', linewidth=2, markersize=6, color='purple')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Total Clusters', fontweight='bold')\n",
    "    ax.set_title('Total Clusters vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(results_df['cohesion'], results_df['n_cross_dataset_clusters'],\n",
    "              c=results_df['quality_score'], cmap='RdYlGn', s=100, edgecolors='black')\n",
    "    ax.scatter(best['cohesion'], best['n_cross_dataset_clusters'],\n",
    "              color='red', s=300, marker='*', edgecolors='black', linewidth=2, zorder=10)\n",
    "    ax.set_xlabel('Cohesion', fontweight='bold')\n",
    "    ax.set_ylabel('Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_title('Cohesion vs Cross-Dataset', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 2]\n",
    "    ax.hist(results_df['quality_score'], bins=15, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(best['quality_score'], color='red', linestyle='--', linewidth=2, label=f\"Best: {best['quality_score']:.3f}\")\n",
    "    ax.set_xlabel('Quality Score', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title('Score Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = RESULTS_DIR / 'poem_grid_search_comprehensive.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    logger.info(f\"Visualization saved: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    best_jaccard = float(best['jaccard_threshold'])\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"Selected configuration (highest quality)\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Jaccard threshold: {best_jaccard:.3f}\")\n",
    "    logger.info(f\"Quality score: {best['quality_score']:.3f}\")\n",
    "    logger.info(f\"Cohesion: {best['cohesion']:.3f}\")\n",
    "    logger.info(f\"Cross-dataset clusters: {best['n_cross_dataset_clusters']}\")\n",
    "\n",
    "    timing_logger.end_stage()\n",
    "    return best_jaccard, results_df\n",
    "\n",
    "def cluster_all_poems_batched(poem_to_clusters, poem_to_dataset, poems_by_dataset, jaccard_threshold):\n",
    "    timing_logger.start_stage(\"16_poem_clustering\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"Clustering all poems (batched, cross-dataset only)\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Jaccard threshold: {jaccard_threshold:.3f}\")\n",
    "\n",
    "    cluster_to_poems = build_cluster_to_poems_index(poem_to_clusters)\n",
    "\n",
    "    candidate_pairs = find_cross_dataset_candidate_pairs_batched(\n",
    "        poem_to_clusters, poem_to_dataset, cluster_to_poems, poems_by_dataset\n",
    "    )\n",
    "\n",
    "    logger.info(f\"\\nClustering {len(poem_to_clusters):,} poems...\")\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    uf = UnionFind(poem_ids)\n",
    "\n",
    "    merges = 0\n",
    "    batches = [list(candidate_pairs)[i:i+POEM_BATCH_SIZE] for i in range(0, len(candidate_pairs), POEM_BATCH_SIZE)]\n",
    "\n",
    "    for batch in tqdm(batches, desc=\"Processing batches\"):\n",
    "        for p1, p2 in batch:\n",
    "            jaccard = compute_jaccard_similarity(\n",
    "                poem_to_clusters[p1],\n",
    "                poem_to_clusters[p2]\n",
    "            )\n",
    "\n",
    "            if jaccard >= jaccard_threshold:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "\n",
    "    logger.info(f\"  Performed {merges:,} merges\")\n",
    "\n",
    "    poem_clusters = uf.get_clusters()\n",
    "    cluster_assignments = {}\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        for poem in poems:\n",
    "            cluster_assignments[poem] = cluster_id\n",
    "\n",
    "    n_clusters = len(poem_clusters)\n",
    "    cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "    n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "\n",
    "    n_cross_dataset_clusters = 0\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "        if len(datasets) > 1:\n",
    "            n_cross_dataset_clusters += 1\n",
    "\n",
    "    logger.info(f\"\\n  Total poem clusters: {n_clusters:,}\")\n",
    "    logger.info(f\"  Cross-dataset clusters: {n_cross_dataset_clusters:,}\")\n",
    "    logger.info(f\"  Singleton poems: {n_singletons:,}\")\n",
    "    logger.info(f\"  Avg cluster size: {np.mean(cluster_sizes):.2f}\")\n",
    "    logger.info(f\"  Max cluster size: {max(cluster_sizes)}\")\n",
    "\n",
    "    timing_logger.end_stage()\n",
    "    return cluster_assignments, poem_clusters\n",
    "\n",
    "poem_to_clusters, poem_to_dataset, poem_metadata = reconstruct_poems_from_verses(df)\n",
    "poem_to_size = poem_metadata['poem_to_size']\n",
    "poems_by_dataset = poem_metadata['poems_by_dataset']\n",
    "\n",
    "best_jaccard, grid_results = grid_search_poem_parameters(\n",
    "    poem_to_clusters, poem_to_dataset, poem_to_size, poems_by_dataset\n",
    ")\n",
    "\n",
    "del grid_results\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "cluster_assignments, poem_clusters = cluster_all_poems_batched(\n",
    "    poem_to_clusters, poem_to_dataset, poems_by_dataset, best_jaccard\n",
    ")\n",
    "\n",
    "timing_logger.start_stage(\"17_poem_results\")\n",
    "\n",
    "df['poem_composite_id'] = df['idoriginal_poem'].astype(str) + '___' + df['source_dataset'].astype(str)\n",
    "df['poem_cluster_id'] = df['poem_composite_id'].map(cluster_assignments)\n",
    "\n",
    "cross_dataset_cluster_ids = set()\n",
    "for cluster_id, poems in poem_clusters.items():\n",
    "    datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "    if len(datasets) > 1:\n",
    "        cross_dataset_cluster_ids.add(cluster_id)\n",
    "\n",
    "df['is_cross_dataset_poem_cluster'] = df['poem_cluster_id'].isin(cross_dataset_cluster_ids)\n",
    "\n",
    "output_csv = RESULTS_DIR / 'poems_clustered_by_verse_jaccard.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "logger.info(f\"\\nResults saved: {output_csv}\")\n",
    "\n",
    "poem_summary = {\n",
    "    'n_verses': len(df),\n",
    "    'n_poems': len(poem_to_clusters),\n",
    "    'n_datasets': len(set(poem_to_dataset.values())),\n",
    "    'best_jaccard_threshold': best_jaccard,\n",
    "    'n_poem_clusters': len(set(cluster_assignments.values())),\n",
    "    'n_cross_dataset_clusters': len(cross_dataset_cluster_ids),\n",
    "    'n_poems_in_cross_dataset_clusters': sum(df['is_cross_dataset_poem_cluster'])\n",
    "}\n",
    "\n",
    "pd.DataFrame([poem_summary]).to_csv(RESULTS_DIR / 'poem_clustering_summary.csv', index=False)\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"Poem-level clustering complete\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Cross-dataset poem clusters: {poem_summary['n_cross_dataset_clusters']:,}\")\n",
    "logger.info(f\"Poems in cross-dataset clusters: {poem_summary['n_poems_in_cross_dataset_clusters']:,}\")\n",
    "logger.info(f\"All results saved to: {RESULTS_DIR}/\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "timing_logger.end_stage()\n",
    "\n",
    "resource_monitor.stop()\n",
    "total_time = time.time() - script_start_time\n",
    "\n",
    "system_info = get_system_info()\n",
    "resource_stats = resource_monitor.get_stats()\n",
    "timing_summary = timing_logger.get_summary()\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"Comprehensive clustering performance report\")\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"System information\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Hostname:            {system_info['hostname']}\")\n",
    "report_lines.append(f\"Platform:            {system_info['platform']}\")\n",
    "report_lines.append(f\"Python Version:      {system_info['python_version']}\")\n",
    "report_lines.append(f\"Processor:           {system_info['processor']}\")\n",
    "report_lines.append(f\"CPU Cores (Physical):{system_info['cpu_count_physical']}\")\n",
    "report_lines.append(f\"CPU Cores (Logical): {system_info['cpu_count_logical']}\")\n",
    "report_lines.append(f\"Total RAM:           {system_info['total_ram_gb']:.2f} GB\")\n",
    "report_lines.append(f\"GPU Available:       {system_info['gpu_available']}\")\n",
    "# if system_info['gpu_available']:\n",
    "#     report_lines.append(f\"GPU Name:            {system_info['gpu_name']}\")\n",
    "#     report_lines.append(f\"GPU Memory:          {system_info['gpu_total_memory_gb']:.2f} GB\")\n",
    "# report_lines.append(f\"Timestamp:           {system_info['timestamp']}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Peak resource usage\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Peak RAM Usage:      {resource_stats['peak_ram_gb']:.2f} GB\")\n",
    "report_lines.append(f\"Average RAM Usage:   {resource_stats['avg_ram_gb']:.2f} GB\")\n",
    "if system_info['gpu_available']:\n",
    "    report_lines.append(f\"Peak GPU Memory:     {resource_stats['peak_gpu_mem_gb']:.2f} GB\")\n",
    "    report_lines.append(f\"Average GPU Memory:  {resource_stats['avg_gpu_mem_gb']:.2f} GB\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Timing breakdown (by stage)\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "\n",
    "total_measured = sum(timing_summary.values())\n",
    "for stage_name, duration in timing_summary.items():\n",
    "    pct = (duration / total_measured * 100) if total_measured > 0 else 0\n",
    "    report_lines.append(f\"{stage_name:.<50} {duration:>8.1f}s ({pct:>5.1f}%)\")\n",
    "\n",
    "report_lines.append(f\"{'Total measured time':.<50} {total_measured:>8.1f}s\")\n",
    "report_lines.append(f\"{'Total wall clock time':.<50} {total_time:>8.1f}s ({total_time/60:>6.1f} min)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Detailed timing analysis\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "\n",
    "verse_stages = [k for k in timing_summary.keys() if k.startswith(('01_', '02_', '03_', '04_', '05_', '06_', '07_', '08_', '09_', '10_', '11_', '12_'))]\n",
    "verse_time = sum(timing_summary.get(k, 0) for k in verse_stages)\n",
    "\n",
    "poem_stages = [k for k in timing_summary.keys() if k.startswith(('13_', '14_', '15_', '16_', '17_'))]\n",
    "poem_time = sum(timing_summary.get(k, 0) for k in poem_stages)\n",
    "\n",
    "report_lines.append(f\"Verse-level clustering:  {verse_time:>8.1f}s ({verse_time/60:>6.1f} min)\")\n",
    "report_lines.append(f\"Poem-level clustering:   {poem_time:>8.1f}s ({poem_time/60:>6.1f} min)\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Clustering results summary\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(\"Verse-level:\")\n",
    "report_lines.append(f\"  Total verses:             {verse_summary['n_verses']:,}\")\n",
    "report_lines.append(f\"  Total clusters:           {verse_summary['n_total_clusters']:,}\")\n",
    "report_lines.append(f\"  Cross-dataset clusters:   {verse_summary['n_cross_clusters']:,} ({verse_summary['pct_cross_clusters']:.1f}%)\")\n",
    "report_lines.append(f\"  Cross-dataset verses:     {verse_summary['n_cross_verses']:,} ({verse_summary['pct_cross_verses']:.1f}%)\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"Poem-level:\")\n",
    "report_lines.append(f\"  Total poems:              {poem_summary['n_poems']:,}\")\n",
    "report_lines.append(f\"  Total clusters:           {poem_summary['n_poem_clusters']:,}\")\n",
    "report_lines.append(f\"  Cross-dataset clusters:   {poem_summary['n_cross_dataset_clusters']:,}\")\n",
    "report_lines.append(f\"  Poems in cross-clusters:  {poem_summary['n_poems_in_cross_dataset_clusters']:,}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"Performance metrics\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "if verse_time > 0:\n",
    "    report_lines.append(f\"Verse clustering throughput:  {verse_summary['n_verses'] / verse_time:.1f} verses/sec\")\n",
    "if poem_time > 0:\n",
    "    report_lines.append(f\"Poem clustering throughput:   {poem_summary['n_poems'] / poem_time:.1f} poems/sec\")\n",
    "report_lines.append(f\"Overall processing rate:      {verse_summary['n_verses'] / total_time:.1f} verses/sec\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"End of report\")\n",
    "report_lines.append(\"=\"*80)\n",
    "\n",
    "report_path = RESULTS_DIR / 'clustering_performance_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "for line in report_lines:\n",
    "    logger.info(line)\n",
    "\n",
    "logger.info(f\"\\nPerformance report saved to: {report_path}\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"All processing complete\")\n",
    "logger.info(\"=\"*80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af11592e-941a-452b-854e-18c0d7d44d3d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

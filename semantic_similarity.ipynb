{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7e40a5-11f1-48e5-9e4a-ca06fde042a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. DBBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db782c-0a4f-4731-9b46-ecc96d8a8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import faiss\n",
    "import os\n",
    "from typing import Dict\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "os.makedirs('dbbe_semantic_results', exist_ok=True)\n",
    "\n",
    "csv_path = 'dbbe_full.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.dropna()\n",
    "\n",
    "model_name = 'kevinkrahn/shlm-grc-en'\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "verses = df['verse'].tolist()\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(verses), batch_size), desc=\"Embeddings\"):\n",
    "    batch = verses[i:i+batch_size]\n",
    "    encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "nlist = 500\n",
    "k = 50\n",
    "nprobe = 10\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "index.train(embeddings.astype('float32'))\n",
    "index.add(embeddings.astype('float32'))\n",
    "index.nprobe = nprobe\n",
    "distances, indices = index.search(embeddings.astype('float32'), k)\n",
    "\n",
    "similarity_thresholds = [0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "threshold_results = []\n",
    "all_resolution_results = []\n",
    "\n",
    "for sim_threshold in similarity_thresholds:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Threshold: {sim_threshold:.2f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    for i in tqdm(range(len(embeddings)), desc=f\"Building graph (t={sim_threshold:.2f})\"):\n",
    "        for j_idx, distance in zip(indices[i], distances[i]):\n",
    "            if j_idx != i and j_idx != -1:\n",
    "                similarity = distance\n",
    "                if similarity >= sim_threshold:\n",
    "                    idx1, idx2 = min(i, j_idx), max(i, j_idx)\n",
    "                    candidate_pairs.add((idx1, idx2))\n",
    "    \n",
    "    n_pairs = len(candidate_pairs)\n",
    "    avg_degree = n_pairs * 2 / len(embeddings)\n",
    "    \n",
    "    print(f\"Candidate pairs: {n_pairs:,} (avg degree: {avg_degree:.1f})\")\n",
    "    \n",
    "    if n_pairs == 0:\n",
    "        print(\"No pairs found - skipping\")\n",
    "        threshold_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'n_pairs': 0,\n",
    "            'best_resolution': None,\n",
    "            'best_ari': 0,\n",
    "            'best_v_measure': 0,\n",
    "            'n_clusters': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for i, j in tqdm(candidate_pairs, desc=\"Edge weights\"):\n",
    "        sim = float(np.dot(embeddings[i], embeddings[j]))\n",
    "        edges.append((i, j))\n",
    "        weights.append(sim)\n",
    "    \n",
    "    g = ig.Graph(n=len(embeddings), edges=edges, directed=False)\n",
    "    g.es['weight'] = weights\n",
    "    \n",
    "    print(f\"Graph: {g.vcount()} nodes, {g.ecount()} edges\")\n",
    "    \n",
    "    w = np.array(weights)\n",
    "    w_scaled = ((w - w.min()) / (w.max() - w.min())) ** 3\n",
    "    g.es['weight'] = w_scaled.tolist()\n",
    "    \n",
    "    hub_thresh = 500\n",
    "    for v in range(g.vcount()):\n",
    "        if g.degree(v) > hub_thresh:\n",
    "            for e in g.incident(v):\n",
    "                g.es[e]['weight'] *= 0.5\n",
    "    \n",
    "    print(\"Leiden resolution sweep...\")\n",
    "    resolutions = np.logspace(-2, 1, 20)\n",
    "    \n",
    "    best_ari = -1\n",
    "    best_labels = None\n",
    "    best_res = None\n",
    "    best_v = None\n",
    "    \n",
    "    for res in tqdm(resolutions, desc=\"Resolutions\"):\n",
    "        partition = la.find_partition(\n",
    "            g,\n",
    "            la.CPMVertexPartition,\n",
    "            weights='weight',\n",
    "            resolution_parameter=res,\n",
    "            n_iterations=-1\n",
    "        )\n",
    "        labels = np.array(partition.membership)\n",
    "        ari = adjusted_rand_score(df['idgroup'], labels)\n",
    "        v_measure = v_measure_score(df['idgroup'], labels)\n",
    "        n_clusters = len(set(labels))\n",
    "        \n",
    "        col_name = f'cluster_t{int(sim_threshold*100)}_r{res:.6f}'\n",
    "        df[col_name] = labels\n",
    "        \n",
    "        all_resolution_results.append({\n",
    "            'threshold': sim_threshold,\n",
    "            'resolution': res,\n",
    "            'ari': ari,\n",
    "            'v_measure': v_measure,\n",
    "            'n_clusters': n_clusters,\n",
    "            'column_name': col_name\n",
    "        })\n",
    "        \n",
    "        if ari > best_ari:\n",
    "            best_ari = ari\n",
    "            best_labels = labels\n",
    "            best_res = res\n",
    "            best_v = v_measure\n",
    "    \n",
    "    n_clusters = len(set(best_labels))\n",
    "    \n",
    "    print(f\"Best resolution: {best_res:.4f}\")\n",
    "    print(f\"Best ARI: {best_ari:.4f}\")\n",
    "    print(f\"Best V-measure: {best_v:.4f}\")\n",
    "    print(f\"Clusters: {n_clusters}\")\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': sim_threshold,\n",
    "        'n_pairs': n_pairs,\n",
    "        'avg_degree': avg_degree,\n",
    "        'best_resolution': best_res,\n",
    "        'best_ari': best_ari,\n",
    "        'best_v_measure': best_v,\n",
    "        'n_clusters': n_clusters\n",
    "    })\n",
    "    \n",
    "    df[f'cluster_t{int(sim_threshold*100)}_best'] = best_labels\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH RESULTS (BEST PER THRESHOLD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame(threshold_results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_threshold_row = results_df.loc[results_df['best_ari'].idxmax()]\n",
    "best_threshold = best_threshold_row['threshold']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL BEST THRESHOLD: {best_threshold:.2f}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ARI: {best_threshold_row['best_ari']:.4f}\")\n",
    "print(f\"V-measure: {best_threshold_row['best_v_measure']:.4f}\")\n",
    "print(f\"Resolution: {best_threshold_row['best_resolution']:.4f}\")\n",
    "print(f\"Clusters: {int(best_threshold_row['n_clusters'])}\")\n",
    "\n",
    "df['cluster_best'] = df[f'cluster_t{int(best_threshold*100)}_best']\n",
    "\n",
    "df.to_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\", index=False)\n",
    "results_df.to_csv(\"dbbe_semantic_results/threshold_gridsearch_summary.csv\", index=False)\n",
    "\n",
    "all_resolution_df = pd.DataFrame(all_resolution_results)\n",
    "all_resolution_df.to_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "print(\"  - dbbe_semantic_results/threshold_gridsearch_summary.csv\")\n",
    "print(\"  - dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "print(f\"\\nTotal clustering columns created: {len(all_resolution_results)}\")\n",
    "print(f\"Thresholds tested: {len(similarity_thresholds)}\")\n",
    "print(f\"Resolutions per threshold: {len(resolutions)}\")\n",
    "\n",
    "def calculate_jaccard_similarity(clusters_a, clusters_b):\n",
    "    if not clusters_a or not clusters_b:\n",
    "        return 0.0\n",
    "    intersection = len(clusters_a & clusters_b)\n",
    "    union = len(clusters_a | clusters_b)\n",
    "    return intersection / union\n",
    "\n",
    "def reconstruct_poems(df):\n",
    "    poem_to_clusters = defaultdict(set)\n",
    "    poem_verse_counts = defaultdict(int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        poem_id = row['idoriginal_poem']\n",
    "        cluster_id = row['cluster_leiden_fixed']\n",
    "        poem_verse_counts[poem_id] += 1\n",
    "        if cluster_id != -1:\n",
    "            poem_to_clusters[poem_id].add(cluster_id)\n",
    "\n",
    "    print(f\"\\nReconstructed {len(poem_to_clusters)} poems\")\n",
    "    return poem_to_clusters, poem_verse_counts\n",
    "\n",
    "def evaluate_against_ground_truth(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for poem_id, predicted_cluster in poem_clusters.items():\n",
    "        if poem_id in poem_to_type:\n",
    "            y_true.append(poem_to_type[poem_id])\n",
    "            y_pred.append(predicted_cluster)\n",
    "\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    v_measure = v_measure_score(y_true, y_pred)\n",
    "\n",
    "    return ari, v_measure, y_true, y_pred\n",
    "\n",
    "def cluster_poems_jaccard(poem_to_clusters, similarity_threshold=0.66):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    n_poems = len(poem_ids)\n",
    "\n",
    "    edges = []\n",
    "    for i in range(n_poems):\n",
    "        for j in range(i + 1, n_poems):\n",
    "            pid_a, pid_b = poem_ids[i], poem_ids[j]\n",
    "            sim = calculate_jaccard_similarity(poem_to_clusters[pid_a], poem_to_clusters[pid_b])\n",
    "            if sim >= similarity_threshold:\n",
    "                edges.append((pid_a, pid_b))\n",
    "\n",
    "    class UF:\n",
    "        def __init__(self, elements):\n",
    "            self.parent = {e: e for e in elements}\n",
    "            self.rank = {e: 0 for e in elements}\n",
    "\n",
    "        def find(self, x):\n",
    "            if self.parent[x] != x:\n",
    "                self.parent[x] = self.find(self.parent[x])\n",
    "            return self.parent[x]\n",
    "\n",
    "        def union(self, x, y):\n",
    "            px, py = self.find(x), self.find(y)\n",
    "            if px == py: return\n",
    "            if self.rank[px] < self.rank[py]: px, py = py, px\n",
    "            self.parent[py] = px\n",
    "            if self.rank[px] == self.rank[py]: self.rank[px] += 1\n",
    "\n",
    "    uf = UF(poem_ids)\n",
    "    for a, b in edges:\n",
    "        uf.union(a, b)\n",
    "\n",
    "    poem_clusters = {pid: uf.find(pid) for pid in poem_ids}\n",
    "    return poem_clusters, edges\n",
    "\n",
    "def calculate_perfect_reconstruction_rate(df, poem_clusters):\n",
    "    poem_to_type = df.groupby('idoriginal_poem')['type_id'].first().to_dict()\n",
    "\n",
    "    gt_to_poems = defaultdict(set)\n",
    "    for poem_id, gt_type in poem_to_type.items():\n",
    "        gt_to_poems[gt_type].add(poem_id)\n",
    "\n",
    "    pred_to_poems = defaultdict(set)\n",
    "    for poem_id, pred_cluster in poem_clusters.items():\n",
    "        pred_to_poems[pred_cluster].add(poem_id)\n",
    "\n",
    "    perfectly_reconstructed = 0\n",
    "    total_gt_clusters = len(gt_to_poems)\n",
    "\n",
    "    for gt_type, gt_poems in gt_to_poems.items():\n",
    "        for pred_cluster, pred_poems in pred_to_poems.items():\n",
    "            if gt_poems == pred_poems:\n",
    "                perfectly_reconstructed += 1\n",
    "                break\n",
    "\n",
    "    reconstruction_rate = perfectly_reconstructed / total_gt_clusters if total_gt_clusters > 0 else 0\n",
    "    return reconstruction_rate, perfectly_reconstructed, total_gt_clusters\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/faiss_leiden_gridsearch_results.csv\")\n",
    "\n",
    "if 'cluster_best' in df.columns:\n",
    "    df['cluster_leiden_fixed'] = df['cluster_best']\n",
    "else:\n",
    "    raise ValueError(\"Column 'cluster_best' not found in CSV\")\n",
    "\n",
    "poem_to_clusters, _ = reconstruct_poems(df)\n",
    "\n",
    "thresholds = [0.50, 0.60, 0.70, 0.8]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nThreshold {thresh:.0%}...\")\n",
    "    poem_clusters, edges = cluster_poems_jaccard(poem_to_clusters, thresh)\n",
    "    df['poem_cluster_id'] = df['idoriginal_poem'].map(poem_clusters)\n",
    "\n",
    "    ari, v_measure, _, _ = evaluate_against_ground_truth(df, poem_clusters)\n",
    "    reconstruction_rate, n_perfect, n_total_gt = calculate_perfect_reconstruction_rate(df, poem_clusters)\n",
    "\n",
    "    results.append({\n",
    "        'threshold': thresh,\n",
    "        'n_poem_clusters': len(set(poem_clusters.values())),\n",
    "        'n_edges': len(edges),\n",
    "        'ari': ari,\n",
    "        'v_measure': v_measure,\n",
    "        'perfect_reconstruction_rate': reconstruction_rate,\n",
    "        'n_perfect_clusters': n_perfect,\n",
    "        'n_total_gt_clusters': n_total_gt\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "df.to_csv('dbbe_semantic_results/dbbe_poems_semantic.csv')\n",
    "results_df.to_csv('dbbe_semantic_results/dbbe_poems_semantic_stats.csv')\n",
    "\n",
    "results_df = results_df.sort_values('threshold')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(results_df['threshold'], results_df['ari'], marker='o', linestyle='-')\n",
    "plt.xticks(results_df['threshold'])\n",
    "plt.xlabel(\"Jaccard Similarity Threshold\")\n",
    "plt.ylabel(\"Adjusted Rand Index (ARI)\")\n",
    "plt.title(\"ARI of Poem Clustering vs Threshold\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dbbe_semantic_results/ari_poemlevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/all_threshold_resolution_results.csv\")\n",
    "\n",
    "df = df[~df['threshold'].isin([0.85, 0.75])]\n",
    "df = df[df['resolution'] <= 1.0]\n",
    "\n",
    "df['resolution'] = df['resolution'].round(4)\n",
    "df['threshold'] = df['threshold'].round(4)\n",
    "df['ari'] = df['ari'].round(4)\n",
    "\n",
    "heatmap_data = df.pivot(index='resolution', columns='threshold', values='ari')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    fmt=\".4f\", \n",
    "    cmap=\"viridis\", \n",
    "    cbar_kws={'label': 'ARI'},\n",
    "    annot_kws={\"fontsize\":14}\n",
    ")\n",
    "plt.ylabel(\"Resolution\")\n",
    "plt.xlabel(\"Similarity Threshold\")\n",
    "plt.title(\"ARI heatmap across Threshold and Resolution\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"dbbe_semantic_results/ari_verselevel_sem_dbbe.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll outputs saved to dbbe_semantic_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35f747-5b29-4d43-9afa-968c7c5db010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper printing verses that are orthographically dissimilar but share a cluster\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "poems = (\n",
    "    df.sort_values([\"idoriginal_poem\", \"order\"])\n",
    "      .groupby(\"idoriginal_poem\")[\"verse\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "poem_clusters = (\n",
    "    df.groupby(\"idoriginal_poem\")[\"poem_cluster_id\"]\n",
    "      .first()\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "cluster_to_poems = defaultdict(list)\n",
    "for pid, cid in poem_clusters.items():\n",
    "    cluster_to_poems[cid].append(pid)\n",
    "def versewise_min_distance(v1, v2):\n",
    "    v1_proc = [preprocess_text(v) for v in v1]\n",
    "    v2_proc = [preprocess_text(v) for v in v2]\n",
    "    \n",
    "    unmatched = set(range(len(v2_proc)))\n",
    "    total_dist = 0\n",
    "    \n",
    "    for va in v1_proc:\n",
    "        if not unmatched:\n",
    "            break\n",
    "        best_idx = min(unmatched, key=lambda i: jaccard_distance(va, v2_proc[i]))\n",
    "        total_dist += jaccard_distance(va, v2_proc[best_idx])\n",
    "        unmatched.remove(best_idx)\n",
    "        \n",
    "    return total_dist / max(len(v1_proc), len(v2_proc))\n",
    "\n",
    "def shingles(s, k=3):\n",
    "    s = preprocess_text(s)\n",
    "    return {s[i:i+k] for i in range(len(s)-k+1)} if len(s) >= k else {s}\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "\n",
    "    A, B = shingles(a), shingles(b)\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return 1 - (inter / union if union else 0)\n",
    "\n",
    "results = [] \n",
    "\n",
    "MAX_VERSES = 4\n",
    "MAX_CLUSTER_SIZE = 5  \n",
    "\n",
    "for cid, poem_ids in cluster_to_poems.items():\n",
    "    if len(poem_ids) > MAX_CLUSTER_SIZE:\n",
    "        continue\n",
    "\n",
    "    if len(poem_ids) < 2:\n",
    "        continue\n",
    "\n",
    "    for p1, p2 in combinations(poem_ids, 2):\n",
    "        v1, v2 = poems[p1], poems[p2]\n",
    "\n",
    "        verse_count = len(v1)\n",
    "\n",
    "        if verse_count > MAX_VERSES:\n",
    "            continue\n",
    "\n",
    "        dist = versewise_min_distance(v1, v2)\n",
    "\n",
    "        results.append((cid, p1, p2, dist, verse_count))\n",
    "\n",
    "results.sort(key=lambda x: x[3], reverse=True)  \n",
    "\n",
    "filtered_results = [r for r in results if r[4] > 1]\n",
    "\n",
    "TOP = 20\n",
    "for cid, p1, p2, dist, vcount in filtered_results[:TOP]:\n",
    "    print(f\"\\n=== Cluster {cid} | Poems in cluster <5 | Verses {vcount} | Distance {dist:.3f} ===\")\n",
    "\n",
    "    print(f\"\\nPoem {p1}:\")\n",
    "    for line in poems[p1]:\n",
    "        print(\"  \", line)\n",
    "\n",
    "    print(f\"\\nPoem {p2}:\")\n",
    "    for line in poems[p2]:\n",
    "        print(\"  \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6cd0e-cdf6-4b7b-88b3-2cf453928e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper printing all verses that were clustered together with a given target idgroup\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "\n",
    "target_idgroup = 831\n",
    "threshold_cols = ['cluster_t70', 'cluster_t75', 'cluster_t80', 'cluster_t85', 'cluster_t90', 'cluster_leiden_fixed']\n",
    "poem_to_cluster = df.groupby('idoriginal_poem')['poem_cluster_id'].first().to_dict()\n",
    "\n",
    "\n",
    "verse_row = df[df['idgroup'] == target_idgroup]\n",
    "if verse_row.empty:\n",
    "    print(\"idgroup not found\")\n",
    "else:\n",
    "    print(f\"Verse {target_idgroup}: {verse_row['verse'].iloc[0]}\\n\")dbbe_semantic_results\n",
    "\n",
    "    for col in threshold_cols:\n",
    "        cluster_id = verse_row[col].iloc[0]\n",
    "        same_cluster = df[df[col] == cluster_id]\n",
    "        same_cluster_sorted = same_cluster.sort_values(['idoriginal_poem', 'order'])\n",
    "\n",
    "        poem_ids_in_cluster = same_cluster_sorted['idoriginal_poem'].unique().tolist()\n",
    "\n",
    "        print(f\"=== {col} | Cluster {cluster_id} | {len(poem_ids_in_cluster)} poems ===\")\n",
    "\n",
    "        for _, row in same_cluster_sorted.iterrows():\n",
    "            poem_id = row['idoriginal_poem']\n",
    "            verse_text = row['verse']\n",
    "            type_id = row['type_id']\n",
    "            verse_group = row['idgroup']\n",
    "\n",
    "            poem_cluster = poem_to_cluster.get(poem_id, \"N/A\")\n",
    "            print(f\"  Poem ID: {poem_id} | Verse: {verse_text} | Poem Cluster: {poem_cluster} | Type ID: {type_id} | Verse Group {verse_group}\")\n",
    "\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4032ec-d2ec-419a-8094-306fba90fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper printing all poems for a poem_cluster_id\n",
    "df = pd.read_csv(\"dbbe_semantic_results/dbbe_poems_semantic.csv\")\n",
    "target_cluster_id = 36240\n",
    "\n",
    "cluster_df = df[df['poem_cluster_id'] == target_cluster_id]\n",
    "\n",
    "poems_in_cluster = (\n",
    "    cluster_df.sort_values(['idoriginal_poem', 'order'])\n",
    "              .groupby('idoriginal_poem')['verse']\n",
    "              .apply(list)\n",
    "              .to_dict()\n",
    ")\n",
    "\n",
    "for poem_id, verses in poems_in_cluster.items():\n",
    "    print(f\"\\nPoem {poem_id}:\")\n",
    "    for line in verses:\n",
    "        print(\"  \", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06a9c9-8e39-4efe-b9a9-933fd6aa4609",
   "metadata": {},
   "source": [
    "# 2. Full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bc56a-0b54-470e-b13a-7fe31f1e80e4",
   "metadata": {},
   "source": [
    "## 2.1 Verse level clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f25ab-375f-4cdd-afcc-c5a1b62886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:33:36 | INFO | ================================================================================\n",
      "09:33:36 | INFO | FAST JOINT THRESHOLD-RESOLUTION PARAMETER SWEEP\n",
      "09:33:36 | INFO | Sample: 15000, Bootstrap: 2, Workers: 16\n",
      "09:33:36 | INFO | ================================================================================\n",
      "09:33:36 | INFO | \n",
      "================================================================================\n",
      "09:33:36 | INFO | ðŸ“‚ LOADING CHECKPOINT\n",
      "09:33:36 | INFO | ================================================================================\n",
      "09:34:03 | INFO | âœ“ Loaded 1,536,616 embeddings in 26.8s\n",
      "09:34:03 | INFO | \n",
      "================================================================================\n",
      "09:34:03 | INFO | FAST SAMPLE PREPARATION\n",
      "09:34:03 | INFO | ================================================================================\n",
      "09:34:03 | INFO | Sample size: 14,998\n",
      "09:34:03 | INFO | Building FAISS index...\n",
      "09:34:04 | INFO | âœ“ Neighbor search complete in 0.9s\n",
      "09:34:04 | INFO | \n",
      "Coarse parameter grid:\n",
      "09:34:04 | INFO |   Thresholds: [96, 97, 98, 99]\n",
      "09:34:04 | INFO |   Resolutions: 8 values\n",
      "09:34:04 | INFO |   Total: 32 combinations\n",
      "09:34:04 | INFO | \n",
      "Precomputing cross-dataset similarities...\n",
      "09:34:05 | INFO | âœ“ Collected 1,196,822 pairs in 0.3s\n",
      "09:34:05 | INFO | Threshold range: 0.8871 (P50) to 0.9372 (P95)\n",
      "09:34:05 | INFO | \n",
      "Precomputing edge structures...\n",
      "09:34:06 | INFO | âœ“ Precomputed 627,935 edges in 1.2s\n",
      "09:34:06 | INFO | \n",
      "================================================================================\n",
      "09:34:06 | INFO | STAGE 1: COARSE PARALLEL SWEEP\n",
      "09:34:06 | INFO | ================================================================================\n",
      "09:34:06 | INFO | Testing 32 combinations with 16 workers...\n",
      "Coarse sweep: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.90it/s]\n",
      "09:34:18 | INFO | âœ“ Coarse sweep complete in 11.9s\n",
      "09:34:18 | INFO |   Valid results: 32 / 32\n",
      "09:34:18 | INFO | \n",
      "================================================================================\n",
      "09:34:18 | INFO | STAGE 2: FINE SWEEP AROUND BEST REGION\n",
      "09:34:18 | INFO | ================================================================================\n",
      "09:34:18 | INFO | Best coarse result (by stability): P96.0, res=5.18e-06\n",
      "09:34:18 | INFO |   Stability: 0.049\n",
      "09:34:18 | INFO |   Clusters: 10,610\n",
      "09:34:18 | INFO |   Cross-dataset: 2.1%\n",
      "09:34:18 | INFO | \n",
      "Fine grid:\n",
      "09:34:18 | INFO |   Thresholds: [86.0, 91.0, 95, 96.0]\n",
      "09:34:18 | INFO |   Resolutions: 7 values around 5.18e-06\n",
      "09:34:18 | INFO |   Total: 28 combinations\n",
      "09:34:18 | INFO | Testing 27 new combinations...\n",
      "Fine sweep: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:11<00:00,  2.45it/s]\n",
      "09:34:30 | INFO | âœ“ Fine sweep complete in 11.9s\n",
      "09:34:30 | INFO |   Valid results: 27 / 27\n",
      "09:34:30 | INFO | \n",
      "================================================================================\n",
      "09:34:30 | INFO | FINAL ANALYSIS - SELECTION BY STABILITY\n",
      "09:34:30 | INFO | ================================================================================\n",
      "09:34:30 | INFO | \n",
      "================================================================================\n",
      "09:34:30 | INFO | TOP 5 PARAMETER COMBINATIONS (BY STABILITY)\n",
      "09:34:30 | INFO | ================================================================================\n",
      "09:34:30 | INFO | \n",
      "#1. Threshold: P86.0 (0.9226), Resolution: 1.64e-06\n",
      "09:34:30 | INFO |      Stability: 0.263\n",
      "09:34:30 | INFO |      Clusters: 6,961, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 143 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.010, Quality: 156731.852\n",
      "09:34:30 | INFO | \n",
      "#2. Threshold: P86.0 (0.9226), Resolution: 2.40e-06\n",
      "09:34:30 | INFO |      Stability: 0.263\n",
      "09:34:30 | INFO |      Clusters: 6,961, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 143 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.010, Quality: 156686.557\n",
      "09:34:30 | INFO | \n",
      "#3. Threshold: P86.0 (0.9226), Resolution: 3.53e-06\n",
      "09:34:30 | INFO |      Stability: 0.260\n",
      "09:34:30 | INFO |      Clusters: 6,962, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 144 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.013, Quality: 156620.702\n",
      "09:34:30 | INFO | \n",
      "#4. Threshold: P86.0 (0.9226), Resolution: 5.18e-06\n",
      "09:34:30 | INFO |      Stability: 0.260\n",
      "09:34:30 | INFO |      Clusters: 6,962, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 144 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.013, Quality: 156524.277\n",
      "09:34:30 | INFO | \n",
      "#5. Threshold: P86.0 (0.9226), Resolution: 7.60e-06\n",
      "09:34:30 | INFO |      Stability: 0.254\n",
      "09:34:30 | INFO |      Clusters: 6,963, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 145 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.018, Quality: 156383.752\n",
      "09:34:30 | INFO | \n",
      "#6. Threshold: P86.0 (0.9226), Resolution: 1.12e-05\n",
      "09:34:30 | INFO |      Stability: 0.250\n",
      "09:34:30 | INFO |      Clusters: 6,966, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 148 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.020, Quality: 156181.957\n",
      "09:34:30 | INFO | \n",
      "#7. Threshold: P86.0 (0.9226), Resolution: 1.64e-05\n",
      "09:34:30 | INFO |      Stability: 0.221\n",
      "09:34:30 | INFO |      Clusters: 6,970, Singletons: 6,818\n",
      "09:34:30 | INFO |      Cross-dataset: 152 (2.2%)\n",
      "09:34:30 | INFO |      Modularity: 0.074, Quality: 155904.512\n",
      "09:34:30 | INFO | \n",
      "#8. Threshold: P91.0 (0.9295), Resolution: 1.64e-06\n",
      "09:34:30 | INFO |      Stability: 0.154\n",
      "09:34:30 | INFO |      Clusters: 8,453, Singletons: 8,275\n",
      "09:34:30 | INFO |      Cross-dataset: 178 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.030, Quality: 101438.548\n",
      "09:34:30 | INFO | \n",
      "#9. Threshold: P91.0 (0.9295), Resolution: 2.40e-06\n",
      "09:34:30 | INFO |      Stability: 0.154\n",
      "09:34:30 | INFO |      Clusters: 8,453, Singletons: 8,275\n",
      "09:34:30 | INFO |      Cross-dataset: 178 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.030, Quality: 101410.998\n",
      "09:34:30 | INFO | \n",
      "#10. Threshold: P91.0 (0.9295), Resolution: 3.53e-06\n",
      "09:34:30 | INFO |      Stability: 0.142\n",
      "09:34:30 | INFO |      Clusters: 8,454, Singletons: 8,275\n",
      "09:34:30 | INFO |      Cross-dataset: 179 (2.1%)\n",
      "09:34:30 | INFO |      Modularity: 0.079, Quality: 101372.220\n",
      "09:34:30 | INFO | \n",
      "================================================================================\n",
      "09:34:30 | INFO | ðŸŽ¯ SELECTED PARAMETERS (HIGHEST STABILITY)\n",
      "09:34:30 | INFO | ================================================================================\n",
      "09:34:30 | INFO | Threshold: P86.0 = 0.9226\n",
      "09:34:30 | INFO | Resolution: 1.637894e-06\n",
      "09:34:30 | INFO | Stability: 0.263\n",
      "09:34:30 | INFO | Clusters: 6,961\n",
      "09:34:30 | INFO | Cross-dataset: 2.1%\n",
      "09:34:30 | INFO | ================================================================================\n",
      "09:34:30 | INFO | \n",
      "Creating visualization...\n",
      "09:34:32 | INFO | âœ“ Plot saved: full_semantic_results_test/fast_parameter_sweep_summary.png\n",
      "09:34:32 | INFO | \n",
      "âœ“ Results saved to: full_semantic_results_test\n",
      "09:34:32 | INFO | ================================================================================\n",
      "09:34:32 | INFO | âœ… FAST JOINT PARAMETER SWEEP COMPLETE\n",
      "09:34:32 | INFO | ================================================================================\n",
      "09:34:32 | INFO | \n",
      "================================================================================\n",
      "09:34:32 | INFO | STAGE 3: FAST APPROXIMATE GRAPH CONSTRUCTION\n",
      "09:34:32 | INFO | ================================================================================\n",
      "09:34:32 | INFO | Normalizing embeddings...\n",
      "09:34:32 | INFO | Building FAISS IVF index...\n",
      "09:34:32 | INFO | FAISS params: nlist=4958, nprobe=16\n",
      "09:34:33 | INFO | âœ“ Using GPU acceleration for FAISS\n",
      "09:34:33 | INFO | Training on sample of 500,000 vectors...\n",
      "09:34:41 | INFO | âœ“ FAISS index built in 8.7s\n",
      "09:34:41 | INFO | Searching for 100 nearest neighbors...\n",
      "09:34:41 | INFO | Processing 31 search batches of size 50,000...\n",
      "Neighbor search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:51<00:00,  1.66s/it]\n",
      "09:35:33 | INFO | âœ“ Neighbor search complete in 52.2s\n",
      "09:35:33 | INFO | \n",
      "================================================================================\n",
      "09:35:33 | INFO | STAGE 4: MEMORY-EFFICIENT EDGE CONSTRUCTION\n",
      "09:35:33 | INFO | ================================================================================\n",
      "09:35:33 | INFO | Found edge checkpoint - loading...\n",
      "09:35:50 | INFO | âœ“ Loaded 14,968,272 edges from checkpoint\n",
      "09:35:50 | INFO | \n",
      "================================================================================\n",
      "09:35:50 | INFO | STAGE 5: HIERARCHICAL LEIDEN CLUSTERING\n",
      "09:35:50 | INFO | ================================================================================\n",
      "09:35:50 | INFO | Building graph...\n",
      "09:35:54 | INFO | Full graph: 1,536,616 nodes, 14,968,272 edges\n",
      "09:35:54 | INFO | \n",
      "Step 1: Coarse clustering (fast)...\n",
      "09:41:37 | INFO | âœ“ 546,985 coarse clusters in 343.0s\n",
      "09:41:44 | INFO | Cross-dataset coarse clusters: 23,449\n",
      "09:41:44 | INFO | \n",
      "Step 2: Refining with resolution 0.0000...\n",
      "Refining: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23449/23449 [03:14<00:00, 120.58it/s] \n",
      "09:44:58 | INFO | âœ“ Refined: 0, Skipped small: 9,468, Skipped large: 0\n",
      "09:44:58 | INFO | \n",
      "================================================================================\n",
      "09:44:58 | INFO | ANALYZING RESULTS\n",
      "09:44:58 | INFO | ================================================================================\n",
      "09:45:02 | INFO | \n",
      "FINAL RESULTS:\n",
      "09:45:02 | INFO |   Total clusters: 546,985\n",
      "09:45:02 | INFO |   Cross-dataset clusters: 23,449 (4.3%)\n",
      "09:45:02 | INFO |   Cross-dataset verses: 1,013,080 (65.9%)\n",
      "09:45:03 | INFO | \n",
      "Top 20 cross-dataset clusters:\n",
      "09:45:03 | INFO | \n",
      "================================================================================\n",
      "09:45:03 | INFO | SAVING RESULTS\n",
      "09:45:03 | INFO | ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cluster_id  size  n_datasets                 datasets                                        dataset_counts\n",
      "          0  1464           4 dbbe, papyri, phi, rhoby  {'rhoby': 5, 'dbbe': 157, 'phi': 842, 'papyri': 460}\n",
      "          1  1348           4 dbbe, papyri, phi, rhoby  {'rhoby': 14, 'dbbe': 96, 'phi': 641, 'papyri': 597}\n",
      "          2  1295           4 dbbe, papyri, phi, rhoby  {'rhoby': 14, 'dbbe': 97, 'phi': 778, 'papyri': 406}\n",
      "          3  1279           3        dbbe, papyri, phi               {'dbbe': 18, 'phi': 884, 'papyri': 377}\n",
      "          4  1228           4 dbbe, papyri, phi, rhoby   {'rhoby': 2, 'dbbe': 16, 'phi': 791, 'papyri': 419}\n",
      "          5  1196           2              papyri, phi                           {'phi': 821, 'papyri': 375}\n",
      "          6  1124           4 dbbe, papyri, phi, rhoby   {'rhoby': 1, 'dbbe': 14, 'phi': 692, 'papyri': 417}\n",
      "          7  1114           3        dbbe, papyri, phi                {'dbbe': 6, 'phi': 891, 'papyri': 217}\n",
      "          8  1101           4 dbbe, papyri, phi, rhoby {'rhoby': 29, 'dbbe': 338, 'phi': 623, 'papyri': 111}\n",
      "          9  1069           4 dbbe, papyri, phi, rhoby  {'rhoby': 6, 'dbbe': 220, 'phi': 439, 'papyri': 404}\n",
      "         10  1062           4 dbbe, papyri, phi, rhoby {'rhoby': 11, 'dbbe': 120, 'phi': 798, 'papyri': 133}\n",
      "         11  1058           4 dbbe, papyri, phi, rhoby    {'rhoby': 1, 'dbbe': 6, 'phi': 358, 'papyri': 693}\n",
      "         12  1045           4 dbbe, papyri, phi, rhoby   {'rhoby': 3, 'dbbe': 37, 'phi': 635, 'papyri': 370}\n",
      "         13  1044           4 dbbe, papyri, phi, rhoby   {'rhoby': 3, 'dbbe': 36, 'phi': 542, 'papyri': 463}\n",
      "         14  1039           4 dbbe, papyri, phi, rhoby    {'rhoby': 4, 'dbbe': 7, 'phi': 662, 'papyri': 366}\n",
      "         15  1009           3        dbbe, papyri, phi               {'dbbe': 13, 'phi': 736, 'papyri': 260}\n",
      "         16  1001           4 dbbe, papyri, phi, rhoby   {'rhoby': 6, 'dbbe': 48, 'phi': 788, 'papyri': 159}\n",
      "         17   991           2              papyri, phi                           {'phi': 697, 'papyri': 294}\n",
      "         18   985           4 dbbe, papyri, phi, rhoby   {'rhoby': 5, 'dbbe': 96, 'phi': 674, 'papyri': 210}\n",
      "         19   982           4 dbbe, papyri, phi, rhoby {'rhoby': 11, 'dbbe': 166, 'phi': 657, 'papyri': 148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:45:06 | INFO | âœ“ Full results: full_semantic_results_test/concatenated_cross_dataset_clusters.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import faiss\n",
    "from functools import partial\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/full_semantic_clustering_checkpoints\")\n",
    "RESULTS_DIR = Path(\"full_semantic_results_test\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Optimized parameters\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE = 10000\n",
    "N_THREADS = 16\n",
    "FAISS_NPROBE = 16\n",
    "\n",
    "# SPEED OPTIMIZATIONS\n",
    "SAMPLE_SIZE = 15000      # Sample for parameter selection\n",
    "N_BOOTSTRAP = 2          # Reduced from 3 (still captures stability)\n",
    "STABILITY_PAIRS = 2000   # Reduced from 3000 (still accurate)\n",
    "MAX_WORKERS = min(16, mp.cpu_count())  # Parallel workers\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"FAST JOINT THRESHOLD-RESOLUTION PARAMETER SWEEP\")\n",
    "logger.info(f\"Sample: {SAMPLE_SIZE}, Bootstrap: {N_BOOTSTRAP}, Workers: {MAX_WORKERS}\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING UTILITIES\n",
    "# ============================================================================\n",
    "CLEAN_PATTERN = re.compile(r'[^\\w\\s]')\n",
    "WHITESPACE_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "def preprocess_text(text: str, options: Dict[str, bool] = None) -> str:\n",
    "    if options is None:\n",
    "        options = {'lowercase': True, 'remove_diacritics': True, 'remove_punctuation': True}\n",
    "    text = str(text)\n",
    "    if options.get('lowercase', True):\n",
    "        text = text.lower()\n",
    "    if options.get('remove_diacritics', True):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    else:\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "    if options.get('remove_punctuation', True):\n",
    "        text = CLEAN_PATTERN.sub('', text)\n",
    "    text = WHITESPACE_PATTERN.sub(' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD OR CREATE CHECKPOINT\n",
    "# ============================================================================\n",
    "embeddings_file = CHECKPOINT_DIR / 'embeddings.npz'\n",
    "metadata_file = CHECKPOINT_DIR / 'metadata.pkl.gz'\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"ðŸ“‚ LOADING CHECKPOINT\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    embeddings_data = np.load(embeddings_file)\n",
    "    embeddings = embeddings_data['embeddings'].astype('float32')\n",
    "    \n",
    "    with gzip.open(metadata_file, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    df = pd.read_parquet(CHECKPOINT_DIR / 'df_minimal.parquet')\n",
    "    source_datasets = metadata['source_datasets']\n",
    "    dataset_to_indices = metadata['dataset_to_indices']\n",
    "    \n",
    "    logger.info(f\"âœ“ Loaded {len(embeddings):,} embeddings in {time.time()-start_time:.1f}s\")\n",
    "    \n",
    "    LOADED_FROM_CHECKPOINT = True\n",
    "\n",
    "else:\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"NO CHECKPOINT FOUND - CREATING EMBEDDINGS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    csv_path = 'concatenated.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['verse', 'source_dataset'])\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() >= 20]\n",
    "    df = df[df['verse'].fillna('').astype(str).str.len() < 256]\n",
    "    df['verse'] = df['verse'].apply(preprocess_text)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    logger.info(f\"Total verses: {len(df):,}\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    \n",
    "    model_name = 'kevinkrahn/shlm-grc-en'\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(\"Computing embeddings...\")\n",
    "    verses = df['verse'].tolist()\n",
    "    embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(verses), batch_size), desc=\"Embedding\"):\n",
    "        try:\n",
    "            batch = verses[i:i+batch_size]\n",
    "            encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "            \n",
    "            batch_embeddings = cls_pooling(model_output).cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error at batch {i}: {e}\")\n",
    "            embeddings.append(np.zeros((len(batch), model.config.hidden_size), dtype=np.float32))\n",
    "    \n",
    "    embeddings = np.vstack(embeddings).astype('float32')\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    source_datasets = df['source_dataset'].values\n",
    "    dataset_to_indices = defaultdict(list)\n",
    "    for idx, dataset in enumerate(source_datasets):\n",
    "        dataset_to_indices[dataset].append(idx)\n",
    "    \n",
    "    logger.info(\"\\nðŸ’¾ Saving checkpoint...\")\n",
    "    \n",
    "    np.savez_compressed(CHECKPOINT_DIR / 'embeddings.npz', embeddings=embeddings)\n",
    "    \n",
    "    essential_cols = ['verse', 'source_dataset']\n",
    "    for col in ['idoriginal_poem', 'idgroup']:\n",
    "        if col in df.columns:\n",
    "            essential_cols.append(col)\n",
    "    \n",
    "    df_minimal = df[essential_cols].copy()\n",
    "    for col in df_minimal.columns:\n",
    "        if df_minimal[col].dtype == 'object':\n",
    "            df_minimal[col] = df_minimal[col].astype(str)\n",
    "            df_minimal[col] = df_minimal[col].replace('nan', None).replace('None', None)\n",
    "    \n",
    "    df_minimal.to_parquet(CHECKPOINT_DIR / 'df_minimal.parquet', \n",
    "                          compression='gzip', index=True)\n",
    "    \n",
    "    metadata = {\n",
    "        'source_datasets': source_datasets,\n",
    "        'dataset_to_indices': dataset_to_indices\n",
    "    }\n",
    "    with gzip.open(CHECKPOINT_DIR / 'metadata.pkl.gz', 'wb') as f:\n",
    "        pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    logger.info(\"âœ“ Checkpoint saved\")\n",
    "    LOADED_FROM_CHECKPOINT = False\n",
    "\n",
    "# ============================================================================\n",
    "# FAST SAMPLING AND INDEXING\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"FAST SAMPLE PREPARATION\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "def stratified_sample(df, n_sample=15000):\n",
    "    \"\"\"Stratified sampling across datasets\"\"\"\n",
    "    datasets = df['source_dataset'].unique()\n",
    "    total_size = len(df)\n",
    "    sample_indices = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_indices = df[df['source_dataset'] == dataset].index.tolist()\n",
    "        dataset_size = len(dataset_indices)\n",
    "        proportion = dataset_size / total_size\n",
    "        n_from_dataset = int(n_sample * proportion)\n",
    "        n_from_dataset = min(n_from_dataset, dataset_size)\n",
    "        if n_from_dataset > 0:\n",
    "            sampled = np.random.choice(dataset_indices, size=n_from_dataset, replace=False)\n",
    "            sample_indices.extend(sampled)\n",
    "    \n",
    "    return np.array(sorted(sample_indices))\n",
    "\n",
    "# Sample\n",
    "sample_indices = stratified_sample(df, n_sample=SAMPLE_SIZE)\n",
    "sample_embeddings = embeddings[sample_indices].copy()\n",
    "sample_dataset_map = np.array([source_datasets[i] for i in sample_indices])\n",
    "\n",
    "logger.info(f\"Sample size: {len(sample_indices):,}\")\n",
    "\n",
    "# Build FAISS index\n",
    "logger.info(\"Building FAISS index...\")\n",
    "start_time = time.time()\n",
    "dimension = embeddings.shape[1]\n",
    "index_sample = faiss.IndexFlatIP(dimension)\n",
    "faiss.normalize_L2(sample_embeddings)\n",
    "index_sample.add(sample_embeddings)\n",
    "\n",
    "k = min(200, len(sample_embeddings) - 1)\n",
    "similarities, indices = index_sample.search(sample_embeddings, k)\n",
    "\n",
    "logger.info(f\"âœ“ Neighbor search complete in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED PARAMETER GRID (COARSE-TO-FINE)\n",
    "# ============================================================================\n",
    "\n",
    "# STAGE 1: Coarse grid for initial exploration\n",
    "threshold_percentiles_coarse = [96, 97, 98,99]   # 4 values\n",
    "resolutions_coarse = np.logspace(-6, -1, 8)      # 8 values spanning the range\n",
    "\n",
    "# STAGE 2: Fine grid around best coarse result (done after coarse sweep)\n",
    "\n",
    "logger.info(f\"\\nCoarse parameter grid:\")\n",
    "logger.info(f\"  Thresholds: {threshold_percentiles_coarse}\")\n",
    "logger.info(f\"  Resolutions: {len(resolutions_coarse)} values\")\n",
    "logger.info(f\"  Total: {len(threshold_percentiles_coarse) * len(resolutions_coarse)} combinations\")\n",
    "\n",
    "# ============================================================================\n",
    "# PRECOMPUTE CROSS-DATASET SIMILARITIES AND THRESHOLDS\n",
    "# ============================================================================\n",
    "logger.info(\"\\nPrecomputing cross-dataset similarities...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Vectorized approach - much faster\n",
    "cross_similarities = []\n",
    "for i in range(len(sample_embeddings)):\n",
    "    # Boolean mask for cross-dataset neighbors\n",
    "    neighbor_datasets = sample_dataset_map[indices[i, 1:]]  # Skip self\n",
    "    cross_mask = neighbor_datasets != sample_dataset_map[i]\n",
    "    cross_similarities.extend(similarities[i, 1:][cross_mask])\n",
    "\n",
    "cross_similarities = np.array(cross_similarities)\n",
    "logger.info(f\"âœ“ Collected {len(cross_similarities):,} pairs in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# Precompute all threshold values\n",
    "all_percentiles = list(range(50, 100, 5))  # For potential fine-tuning later\n",
    "threshold_lookup = {p: np.percentile(cross_similarities, p) for p in all_percentiles}\n",
    "\n",
    "logger.info(f\"Threshold range: {threshold_lookup[50]:.4f} (P50) to {threshold_lookup[95]:.4f} (P95)\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZED EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_stability_fast(partitions_list, n_nodes, sample_size=STABILITY_PAIRS):\n",
    "    \"\"\"Fast stability computation with smaller sample\"\"\"\n",
    "    if n_nodes < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    n_partitions = len(partitions_list)\n",
    "    \n",
    "    # Sample fewer pairs\n",
    "    n_sample_pairs = min(sample_size, n_nodes * (n_nodes - 1) // 2)\n",
    "    \n",
    "    # Vectorized pair sampling\n",
    "    pairs_i = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    pairs_j = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    valid = pairs_i != pairs_j\n",
    "    pairs_i = pairs_i[valid][:n_sample_pairs]\n",
    "    pairs_j = pairs_j[valid][:n_sample_pairs]\n",
    "    \n",
    "    # Vectorized co-clustering check\n",
    "    coclustering = 0\n",
    "    for membership in partitions_list:\n",
    "        membership_arr = np.array(membership)\n",
    "        matches = membership_arr[pairs_i] == membership_arr[pairs_j]\n",
    "        coclustering += np.sum(matches)\n",
    "    \n",
    "    stability = coclustering / (len(pairs_i) * n_partitions)\n",
    "    return stability\n",
    "\n",
    "def evaluate_single_combination(args):\n",
    "    \"\"\"\n",
    "    Evaluate a single (threshold, resolution) combination.\n",
    "    This function is designed to be called in parallel.\n",
    "    \"\"\"\n",
    "    threshold_pct, threshold, resolution, edges_data, weights_data, dataset_map, n_nodes = args\n",
    "    \n",
    "    try:\n",
    "        # Filter edges for this threshold\n",
    "        edge_mask = weights_data >= threshold\n",
    "        edges = edges_data[edge_mask]\n",
    "        weights = weights_data[edge_mask]\n",
    "        \n",
    "        if len(edges) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Build graph\n",
    "        g = ig.Graph(n=n_nodes, edges=edges.tolist(), directed=False)\n",
    "        g.es['weight'] = weights.tolist()\n",
    "        \n",
    "        # Bootstrap clustering (reduced iterations)\n",
    "        bootstrap_memberships = []\n",
    "        bootstrap_qualities = []\n",
    "        \n",
    "        for seed in range(N_BOOTSTRAP):\n",
    "            partition = la.find_partition(\n",
    "                g,\n",
    "                la.CPMVertexPartition,\n",
    "                weights='weight',\n",
    "                resolution_parameter=resolution,\n",
    "                n_iterations=5,  # Reduced from 10\n",
    "                seed=seed\n",
    "            )\n",
    "            bootstrap_memberships.append(partition.membership)\n",
    "            bootstrap_qualities.append(partition.quality())\n",
    "        \n",
    "        # Analyze\n",
    "        membership = bootstrap_memberships[0]\n",
    "        n_clusters = len(set(membership))\n",
    "        \n",
    "        if n_clusters == 0 or n_clusters == n_nodes:\n",
    "            return None\n",
    "        \n",
    "        # Compute metrics\n",
    "        stability = compute_stability_fast(bootstrap_memberships, n_nodes)\n",
    "        avg_size = n_nodes / n_clusters\n",
    "        \n",
    "        # Cross-dataset metrics\n",
    "        cluster_datasets = defaultdict(set)\n",
    "        cluster_sizes = defaultdict(int)\n",
    "        \n",
    "        for idx, cid in enumerate(membership):\n",
    "            cluster_datasets[cid].add(dataset_map[idx])\n",
    "            cluster_sizes[cid] += 1\n",
    "        \n",
    "        n_cross_clusters = sum(1 for datasets in cluster_datasets.values() \n",
    "                              if len(datasets) > 1)\n",
    "        pct_cross_clusters = (n_cross_clusters / n_clusters * 100) if n_clusters > 0 else 0\n",
    "        n_singleton = sum(1 for size in cluster_sizes.values() if size == 1)\n",
    "        \n",
    "        avg_quality = np.mean(bootstrap_qualities)\n",
    "        modularity = g.modularity(membership, weights='weight')\n",
    "        \n",
    "        return {\n",
    "            'threshold_percentile': threshold_pct,\n",
    "            'threshold_value': threshold,\n",
    "            'resolution': resolution,\n",
    "            'n_edges': g.ecount(),\n",
    "            'graph_density': 2*g.ecount()/(n_nodes*(n_nodes-1)) if n_nodes > 1 else 0,\n",
    "            'stability': stability,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singleton': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'n_cross_clusters': n_cross_clusters,\n",
    "            'pct_cross_clusters': pct_cross_clusters,\n",
    "            'avg_quality': avg_quality,\n",
    "            'modularity': modularity\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error at P{threshold_pct}, res={resolution:.2e}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# PRECOMPUTE EDGE STRUCTURES (MAJOR SPEEDUP)\n",
    "# ============================================================================\n",
    "logger.info(\"\\nPrecomputing edge structures...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build complete edge list with weights (only cross-dataset)\n",
    "all_edges_list = []\n",
    "all_weights_list = []\n",
    "\n",
    "for i in range(len(sample_embeddings)):\n",
    "    dataset_i = sample_dataset_map[i]\n",
    "    neighbors = indices[i, 1:]  # Skip self\n",
    "    sims = similarities[i, 1:]\n",
    "    \n",
    "    # Vectorized filtering\n",
    "    valid_mask = neighbors > i  # Avoid duplicates\n",
    "    valid_neighbors = neighbors[valid_mask]\n",
    "    valid_sims = sims[valid_mask]\n",
    "    \n",
    "    if len(valid_neighbors) > 0:\n",
    "        neighbor_datasets = sample_dataset_map[valid_neighbors]\n",
    "        cross_mask = neighbor_datasets != dataset_i\n",
    "        \n",
    "        final_neighbors = valid_neighbors[cross_mask]\n",
    "        final_sims = valid_sims[cross_mask]\n",
    "        \n",
    "        for j, sim in zip(final_neighbors, final_sims):\n",
    "            all_edges_list.append([i, j])\n",
    "            all_weights_list.append(sim)\n",
    "\n",
    "# Convert to numpy arrays (shared across all parameter combinations)\n",
    "all_edges = np.array(all_edges_list, dtype=np.int32)\n",
    "all_weights = np.array(all_weights_list, dtype=np.float32)\n",
    "\n",
    "logger.info(f\"âœ“ Precomputed {len(all_edges):,} edges in {time.time()-start_time:.1f}s\")\n",
    "# ============================================================================\n",
    "# STAGE 1: COARSE PARALLEL SWEEP\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"STAGE 1: COARSE PARALLEL SWEEP\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "coarse_args = []\n",
    "for threshold_pct in threshold_percentiles_coarse:\n",
    "    # Get threshold value, compute if not in lookup\n",
    "    if threshold_pct in threshold_lookup:\n",
    "        threshold = threshold_lookup[threshold_pct]\n",
    "    else:\n",
    "        threshold = np.percentile(cross_similarities, threshold_pct)\n",
    "        threshold_lookup[threshold_pct] = threshold\n",
    "    \n",
    "    for resolution in resolutions_coarse:\n",
    "        coarse_args.append((\n",
    "            threshold_pct,\n",
    "            threshold,\n",
    "            resolution,\n",
    "            all_edges,\n",
    "            all_weights,\n",
    "            sample_dataset_map,\n",
    "            len(sample_embeddings)\n",
    "        ))\n",
    "\n",
    "\n",
    "logger.info(f\"Testing {len(coarse_args)} combinations with {MAX_WORKERS} workers...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Parallel execution\n",
    "coarse_results = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(evaluate_single_combination, args): args \n",
    "               for args in coarse_args}\n",
    "    \n",
    "    with tqdm(total=len(futures), desc=\"Coarse sweep\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                coarse_results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "logger.info(f\"âœ“ Coarse sweep complete in {time.time()-start_time:.1f}s\")\n",
    "logger.info(f\"  Valid results: {len(coarse_results)} / {len(coarse_args)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 2: FINE SWEEP AROUND BEST COARSE RESULT\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"STAGE 2: FINE SWEEP AROUND BEST REGION\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "if len(coarse_results) == 0:\n",
    "    logger.error(\"No valid coarse results! Cannot proceed with fine sweep.\")\n",
    "    exit(1)\n",
    "\n",
    "# Convert to dataframe for analysis\n",
    "coarse_df = pd.DataFrame(coarse_results)\n",
    "\n",
    "# SELECTION CRITERION: STABILITY ONLY\n",
    "# Find best coarse result based on stability\n",
    "best_coarse = coarse_df.loc[coarse_df['stability'].idxmax()]\n",
    "best_thresh_pct = best_coarse['threshold_percentile']\n",
    "best_res_coarse = best_coarse['resolution']\n",
    "\n",
    "logger.info(f\"Best coarse result (by stability): P{best_thresh_pct}, res={best_res_coarse:.2e}\")\n",
    "logger.info(f\"  Stability: {best_coarse['stability']:.3f}\")\n",
    "logger.info(f\"  Clusters: {best_coarse['n_clusters']:,.0f}\")\n",
    "logger.info(f\"  Cross-dataset: {best_coarse['pct_cross_clusters']:.1f}%\")\n",
    "\n",
    "# Define fine grid around best region\n",
    "# Threshold: Â±10 percentile points\n",
    "thresh_fine = [max(50, best_thresh_pct - 10), \n",
    "               max(50, best_thresh_pct - 5),\n",
    "               best_thresh_pct,\n",
    "               min(95, best_thresh_pct + 5),\n",
    "               min(95, best_thresh_pct + 10)]\n",
    "thresh_fine = sorted(list(set(thresh_fine)))  # Remove duplicates\n",
    "\n",
    "# Resolution: Â±1 order of magnitude (log scale)\n",
    "log_res = np.log10(best_res_coarse)\n",
    "res_fine = np.logspace(log_res - 0.5, log_res + 0.5, 7)\n",
    "\n",
    "logger.info(f\"\\nFine grid:\")\n",
    "logger.info(f\"  Thresholds: {thresh_fine}\")\n",
    "logger.info(f\"  Resolutions: {len(res_fine)} values around {best_res_coarse:.2e}\")\n",
    "logger.info(f\"  Total: {len(thresh_fine) * len(res_fine)} combinations\")\n",
    "\n",
    "# Prepare fine sweep arguments\n",
    "fine_args = []\n",
    "for threshold_pct in thresh_fine:\n",
    "    if threshold_pct in threshold_lookup:\n",
    "        threshold = threshold_lookup[threshold_pct]\n",
    "    else:\n",
    "        threshold = np.percentile(cross_similarities, threshold_pct)\n",
    "        threshold_lookup[threshold_pct] = threshold\n",
    "    \n",
    "    for resolution in res_fine:\n",
    "        # Skip if already tested in coarse sweep\n",
    "        already_tested = any(\n",
    "            np.isclose(r['threshold_percentile'], threshold_pct) and \n",
    "            np.isclose(r['resolution'], resolution, rtol=0.1)\n",
    "            for r in coarse_results\n",
    "        )\n",
    "        if not already_tested:\n",
    "            fine_args.append((\n",
    "                threshold_pct,\n",
    "                threshold,\n",
    "                resolution,\n",
    "                all_edges,\n",
    "                all_weights,\n",
    "                sample_dataset_map,\n",
    "                len(sample_embeddings)\n",
    "            ))\n",
    "\n",
    "logger.info(f\"Testing {len(fine_args)} new combinations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Parallel execution\n",
    "fine_results = []\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(evaluate_single_combination, args): args \n",
    "               for args in fine_args}\n",
    "    \n",
    "    with tqdm(total=len(futures), desc=\"Fine sweep\") as pbar:\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                fine_results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "logger.info(f\"âœ“ Fine sweep complete in {time.time()-start_time:.1f}s\")\n",
    "logger.info(f\"  Valid results: {len(fine_results)} / {len(fine_args)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMBINE AND ANALYZE ALL RESULTS\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"FINAL ANALYSIS - SELECTION BY STABILITY\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Combine all results\n",
    "all_results = coarse_results + fine_results\n",
    "sweep_df = pd.DataFrame(all_results)\n",
    "\n",
    "# SELECTION CRITERION: STABILITY ONLY\n",
    "# Sort by stability (highest first)\n",
    "sweep_df = sweep_df.sort_values('stability', ascending=False)\n",
    "sweep_df.to_csv(RESULTS_DIR / 'joint_parameter_sweep_results.csv', index=False)\n",
    "\n",
    "# Find overall best (highest stability)\n",
    "best_params = sweep_df.iloc[0]\n",
    "best_threshold = best_params['threshold_value']\n",
    "best_resolution = best_params['resolution']\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"TOP 5 PARAMETER COMBINATIONS (BY STABILITY)\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "for idx, (i, row) in enumerate(sweep_df.head(10).iterrows(), 1):\n",
    "    logger.info(f\"\\n#{idx}. Threshold: P{row['threshold_percentile']} ({row['threshold_value']:.4f}), \"\n",
    "               f\"Resolution: {row['resolution']:.2e}\")\n",
    "    logger.info(f\"     Stability: {row['stability']:.3f}\")\n",
    "    logger.info(f\"     Clusters: {row['n_clusters']:,.0f}, Singletons: {row['n_singleton']:,.0f}\")\n",
    "    logger.info(f\"     Cross-dataset: {row['n_cross_clusters']:,.0f} ({row['pct_cross_clusters']:.1f}%)\")\n",
    "    logger.info(f\"     Modularity: {row['modularity']:.3f}, Quality: {row['avg_quality']:.3f}\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"ðŸŽ¯ SELECTED PARAMETERS (HIGHEST STABILITY)\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"Threshold: P{best_params['threshold_percentile']} = {best_threshold:.4f}\")\n",
    "logger.info(f\"Resolution: {best_resolution:.6e}\")\n",
    "logger.info(f\"Stability: {best_params['stability']:.3f}\")\n",
    "logger.info(f\"Clusters: {best_params['n_clusters']:,.0f}\")\n",
    "logger.info(f\"Cross-dataset: {best_params['pct_cross_clusters']:.1f}%\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK VISUALIZATION\n",
    "# ============================================================================\n",
    "logger.info(\"\\nCreating visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Heatmap of stability scores\n",
    "pivot_data = sweep_df.pivot_table(\n",
    "    values='stability',\n",
    "    index='resolution',\n",
    "    columns='threshold_percentile',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Round resolution values to 3 decimal places for display\n",
    "pivot_data.index = [f\"{res:.3e}\" for res in pivot_data.index]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, \n",
    "           cbar_kws={'label': 'Stability'})\n",
    "ax.set_ylabel('Resolution', fontweight='bold')\n",
    "ax.set_xlabel('Threshold Percentile', fontweight='bold')\n",
    "ax.set_title('Stability Heatmap', fontweight='bold', fontsize=14)\n",
    "\n",
    "# 2. Stability distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(sweep_df['stability'], bins=30, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(best_params['stability'], color='red', linestyle='--', linewidth=2, \n",
    "          label=f\"Best: {best_params['stability']:.3f}\")\n",
    "ax.set_xlabel('Stability', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Stability Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 3. Stability vs resolution for different thresholds\n",
    "ax = axes[1, 0]\n",
    "for thresh_pct in sorted(sweep_df['threshold_percentile'].unique()):\n",
    "    data = sweep_df[sweep_df['threshold_percentile'] == thresh_pct]\n",
    "    ax.plot(data['resolution'], data['stability'], 'o-', \n",
    "           label=f'P{thresh_pct}', alpha=0.7, markersize=4)\n",
    "ax.axhline(best_params['stability'], color='red', linestyle='--', \n",
    "          linewidth=1, alpha=0.5, label=f'Best: {best_params[\"stability\"]:.3f}')\n",
    "ax.set_xlabel('Resolution', fontweight='bold')\n",
    "ax.set_ylabel('Stability', fontweight='bold')\n",
    "ax.set_title('Stability vs Resolution', fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. Stability vs clusters (colored by threshold)\n",
    "ax = axes[1, 1]\n",
    "scatter = ax.scatter(sweep_df['n_clusters'], sweep_df['stability'],\n",
    "                    c=sweep_df['threshold_value'], cmap='viridis',\n",
    "                    s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax.scatter(best_params['n_clusters'], best_params['stability'],\n",
    "          color='red', s=200, marker='*', edgecolors='black', linewidth=2,\n",
    "          label='Best', zorder=10)\n",
    "ax.set_xlabel('Number of Clusters', fontweight='bold')\n",
    "ax.set_ylabel('Stability', fontweight='bold')\n",
    "ax.set_title('Stability vs Cluster Count (colored by threshold)', fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "plt.colorbar(scatter, ax=ax, label='Threshold Value')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "fig.suptitle(f'Fast Joint Parameter Sweep - Stability-Based Selection (n={len(sweep_df)} combinations)', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_DIR / 'fast_parameter_sweep_summary.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "logger.info(f\"âœ“ Plot saved: {plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "summary = {\n",
    "    'best_threshold_percentile': best_params['threshold_percentile'],\n",
    "    'best_threshold_value': best_threshold,\n",
    "    'best_resolution': best_resolution,\n",
    "    'stability': best_params['stability'],\n",
    "    'n_clusters': best_params['n_clusters'],\n",
    "    'n_singleton': best_params['n_singleton'],\n",
    "    'avg_cluster_size': best_params['avg_cluster_size'],\n",
    "    'n_cross_clusters': best_params['n_cross_clusters'],\n",
    "    'pct_cross_clusters': best_params['pct_cross_clusters'],\n",
    "    'modularity': best_params['modularity'],\n",
    "    'n_combinations_tested': len(sweep_df),\n",
    "    'sample_size': SAMPLE_SIZE,\n",
    "    'n_bootstrap': N_BOOTSTRAP,\n",
    "    'selection_criterion': 'stability'\n",
    "}\n",
    "\n",
    "pd.DataFrame([summary]).to_csv(RESULTS_DIR / 'best_parameters_summary.csv', index=False)\n",
    "\n",
    "params_for_clustering = {\n",
    "    'threshold': best_threshold,\n",
    "    'resolution': best_resolution\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'optimal_parameters.pkl', 'wb') as f:\n",
    "    pickle.dump(params_for_clustering, f)\n",
    "\n",
    "logger.info(f\"\\nâœ“ Results saved to: {RESULTS_DIR}\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"âœ… FAST JOINT PARAMETER SWEEP COMPLETE\")\n",
    "logger.info(\"=\"*80)\n",
    "# ============================================================================\n",
    "# STAGE 3: FAST GRAPH CONSTRUCTION WITH FAISS\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"STAGE 3: FAST APPROXIMATE GRAPH CONSTRUCTION\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Normalize embeddings\n",
    "logger.info(\"Normalizing embeddings...\")\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Build quantized FAISS index for speed\n",
    "logger.info(\"Building FAISS IVF index...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Optimize nlist based on data size (rule of thumb: 4*sqrt(n) to 16*sqrt(n))\n",
    "nlist = min(int(4 * np.sqrt(len(embeddings))), 8192)  # Cap at 8192 for memory\n",
    "nprobe = min(FAISS_NPROBE, nlist // 4)  # Don't probe more than 1/4 of clusters\n",
    "\n",
    "logger.info(f\"FAISS params: nlist={nlist}, nprobe={nprobe}\")\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index_full = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Try to use GPU if available (massive speedup)\n",
    "try:\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index_full = faiss.index_cpu_to_gpu(res, 0, index_full)\n",
    "    logger.info(\"âœ“ Using GPU acceleration for FAISS\")\n",
    "    on_gpu = True\n",
    "except Exception as e:\n",
    "    logger.info(f\"GPU not available ({e}), using CPU\")\n",
    "    on_gpu = False\n",
    "\n",
    "# Train on a sample if data is very large\n",
    "if len(embeddings) > 1000000:\n",
    "    train_sample_size = min(500000, len(embeddings))\n",
    "    train_indices = np.random.choice(len(embeddings), train_sample_size, replace=False)\n",
    "    train_data = embeddings[train_indices]\n",
    "    logger.info(f\"Training on sample of {train_sample_size:,} vectors...\")\n",
    "    index_full.train(train_data)\n",
    "else:\n",
    "    index_full.train(embeddings)\n",
    "\n",
    "index_full.add(embeddings)\n",
    "index_full.nprobe = nprobe\n",
    "\n",
    "logger.info(f\"âœ“ FAISS index built in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# Search for neighbors in LARGE batches (FAISS is optimized for batch queries)\n",
    "logger.info(f\"Searching for {N_NEIGHBORS} nearest neighbors...\")\n",
    "SEARCH_BATCH_SIZE = 50000  # Much larger batches for FAISS efficiency\n",
    "\n",
    "start_time = time.time()\n",
    "all_similarities = []\n",
    "all_indices = []\n",
    "\n",
    "n_search_batches = (len(embeddings) + SEARCH_BATCH_SIZE - 1) // SEARCH_BATCH_SIZE\n",
    "logger.info(f\"Processing {n_search_batches} search batches of size {SEARCH_BATCH_SIZE:,}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(embeddings), SEARCH_BATCH_SIZE), desc=\"Neighbor search\", total=n_search_batches):\n",
    "    batch_end = min(i + SEARCH_BATCH_SIZE, len(embeddings))\n",
    "    batch_emb = embeddings[i:batch_end]\n",
    "    \n",
    "    # FAISS search is MUCH faster with larger batches\n",
    "    D, I = index_full.search(batch_emb, N_NEIGHBORS)\n",
    "    all_similarities.append(D)\n",
    "    all_indices.append(I)\n",
    "\n",
    "all_similarities = np.vstack(all_similarities)\n",
    "all_indices = np.vstack(all_indices)\n",
    "\n",
    "logger.info(f\"âœ“ Neighbor search complete in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 4: MEMORY-EFFICIENT EDGE CONSTRUCTION\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"STAGE 4: MEMORY-EFFICIENT EDGE CONSTRUCTION\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Check if edge checkpoint exists\n",
    "edge_checkpoint = CHECKPOINT_DIR / 'edges_checkpoint.npz'\n",
    "\n",
    "if edge_checkpoint.exists():\n",
    "    logger.info(\"Found edge checkpoint - loading...\")\n",
    "    edge_data = np.load(edge_checkpoint)\n",
    "    all_edges = [(int(i), int(j)) for i, j in edge_data['edges']]\n",
    "    all_weights = edge_data['weights'].tolist()\n",
    "    logger.info(f\"âœ“ Loaded {len(all_edges):,} edges from checkpoint\")\n",
    "    \n",
    "else:\n",
    "    logger.info(\"Building edge list with cross-dataset filtering...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Strategy: Use numpy vectorization for speed while managing memory\n",
    "    # We'll process in chunks and use boolean masking\n",
    "    \n",
    "    all_edges = []\n",
    "    all_weights = []\n",
    "    \n",
    "    EDGE_BATCH_SIZE = 20000  # Process this many nodes at once\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(embeddings), EDGE_BATCH_SIZE), desc=\"Building edges\"):\n",
    "        batch_end = min(batch_start + EDGE_BATCH_SIZE, len(embeddings))\n",
    "        \n",
    "        # Get batch of node indices\n",
    "        batch_size = batch_end - batch_start\n",
    "        \n",
    "        # For each node in batch, get its neighbors\n",
    "        for local_idx in range(batch_size):\n",
    "            node_idx = batch_start + local_idx\n",
    "            dataset_i = source_datasets[node_idx]\n",
    "            \n",
    "            # Get neighbors (skip self at index 0)\n",
    "            neighbors = all_indices[node_idx, 1:]\n",
    "            sims = all_similarities[node_idx, 1:]\n",
    "            \n",
    "            # Vectorized filtering\n",
    "            # 1. Only neighbors with index > node_idx (avoid duplicates)\n",
    "            # 2. Similarity >= threshold\n",
    "            # 3. Cross-dataset\n",
    "            valid_mask = (neighbors > node_idx) & (sims >= threshold)\n",
    "            valid_neighbors = neighbors[valid_mask]\n",
    "            valid_sims = sims[valid_mask]\n",
    "            \n",
    "            # Check cross-dataset for valid neighbors\n",
    "            if len(valid_neighbors) > 0:\n",
    "                neighbor_datasets = np.array([source_datasets[n] for n in valid_neighbors])\n",
    "                cross_dataset_mask = neighbor_datasets != dataset_i\n",
    "                \n",
    "                final_neighbors = valid_neighbors[cross_dataset_mask]\n",
    "                final_sims = valid_sims[cross_dataset_mask]\n",
    "                \n",
    "                # Add edges\n",
    "                for neighbor, sim in zip(final_neighbors, final_sims):\n",
    "                    all_edges.append((node_idx, int(neighbor)))\n",
    "                    all_weights.append(float(sim))\n",
    "        \n",
    "        # Periodic memory cleanup\n",
    "        if (batch_start // EDGE_BATCH_SIZE) % 10 == 0 and batch_start > 0:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "    \n",
    "    logger.info(f\"âœ“ {len(all_edges):,} cross-dataset edges in {time.time()-start_time:.1f}s\")\n",
    "    \n",
    "    # Save edge checkpoint (in case clustering crashes)\n",
    "    logger.info(\"Saving edge checkpoint...\")\n",
    "    np.savez_compressed(\n",
    "        CHECKPOINT_DIR / 'edges_checkpoint.npz',\n",
    "        edges=np.array(all_edges, dtype=np.int32),\n",
    "        weights=np.array(all_weights, dtype=np.float32)\n",
    "    )\n",
    "    logger.info(\"âœ“ Edge checkpoint saved\")\n",
    "\n",
    "# ============================================================================\n",
    "# STAGE 5: HIERARCHICAL LEIDEN CLUSTERING\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"STAGE 5: HIERARCHICAL LEIDEN CLUSTERING\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "# Build graph\n",
    "logger.info(\"Building graph...\")\n",
    "g = ig.Graph(n=len(embeddings), edges=all_edges, directed=False)\n",
    "g.es['weight'] = all_weights\n",
    "\n",
    "logger.info(f\"Full graph: {g.vcount():,} nodes, {g.ecount():,} edges\")\n",
    "\n",
    "# Step 1: Coarse clustering\n",
    "logger.info(\"\\nStep 1: Coarse clustering (fast)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "coarse_partition = la.find_partition(\n",
    "    g,\n",
    "    la.CPMVertexPartition,\n",
    "    weights='weight',\n",
    "    resolution_parameter=0.01,  # Very low for speed\n",
    "    n_iterations=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "coarse_labels = np.array(coarse_partition.membership)\n",
    "n_coarse = len(set(coarse_labels))\n",
    "\n",
    "logger.info(f\"âœ“ {n_coarse:,} coarse clusters in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# Identify cross-dataset clusters\n",
    "coarse_cluster_info = defaultdict(lambda: {'datasets': set(), 'nodes': []})\n",
    "\n",
    "for idx, cid in enumerate(coarse_labels):\n",
    "    coarse_cluster_info[cid]['datasets'].add(source_datasets[idx])\n",
    "    coarse_cluster_info[cid]['nodes'].append(idx)\n",
    "\n",
    "cross_dataset_clusters = [cid for cid, info in coarse_cluster_info.items()\n",
    "                          if len(info['datasets']) > 1]\n",
    "\n",
    "logger.info(f\"Cross-dataset coarse clusters: {len(cross_dataset_clusters):,}\")\n",
    "\n",
    "# Step 2: Refine cross-dataset clusters\n",
    "logger.info(f\"\\nStep 2: Refining with resolution {resolution:.4f}...\")\n",
    "\n",
    "final_labels = coarse_labels.copy()\n",
    "next_cluster_id = n_coarse\n",
    "\n",
    "# Sort by size\n",
    "cluster_sizes = [(cid, len(coarse_cluster_info[cid]['nodes'])) \n",
    "                 for cid in cross_dataset_clusters]\n",
    "cluster_sizes.sort(key=lambda x: x[1])\n",
    "\n",
    "refined_count = 0\n",
    "skipped_small = 0\n",
    "skipped_large = 0\n",
    "\n",
    "for coarse_cid, size in tqdm(cluster_sizes, desc=\"Refining\"):\n",
    "    if size < 10:\n",
    "        skipped_small += 1\n",
    "        continue\n",
    "    \n",
    "    if size > 100000:\n",
    "        skipped_large += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        cluster_nodes = coarse_cluster_info[coarse_cid]['nodes']\n",
    "        subg = g.subgraph(cluster_nodes)\n",
    "        \n",
    "        if subg.ecount() > 0:\n",
    "            sub_partition = la.find_partition(\n",
    "                subg,\n",
    "                la.CPMVertexPartition,\n",
    "                weights='weight',\n",
    "                resolution_parameter=resolution,\n",
    "                n_iterations=10,\n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            sub_labels = np.array(sub_partition.membership)\n",
    "            \n",
    "            # Only refine if we split the cluster\n",
    "            if len(set(sub_labels)) > 1:\n",
    "                for sub_idx, sub_cid in enumerate(sub_labels):\n",
    "                    global_idx = cluster_nodes[sub_idx]\n",
    "                    final_labels[global_idx] = next_cluster_id + sub_cid\n",
    "                \n",
    "                next_cluster_id += len(set(sub_labels))\n",
    "                refined_count += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error refining cluster {coarse_cid}: {e}\")\n",
    "        continue\n",
    "\n",
    "logger.info(f\"âœ“ Refined: {refined_count:,}, Skipped small: {skipped_small:,}, \"\n",
    "            f\"Skipped large: {skipped_large:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYZE AND SAVE RESULTS\n",
    "# ============================================================================\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"ANALYZING RESULTS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "cluster_datasets = defaultdict(set)\n",
    "cluster_verses = defaultdict(list)\n",
    "\n",
    "for idx, cid in enumerate(final_labels):\n",
    "    cluster_datasets[cid].add(source_datasets[idx])\n",
    "    cluster_verses[cid].append(idx)\n",
    "\n",
    "n_total_clusters = len(set(final_labels))\n",
    "n_cross = sum(1 for ds in cluster_datasets.values() if len(ds) > 1)\n",
    "cross_verses = sum(len(cluster_verses[cid]) for cid in set(final_labels)\n",
    "                  if len(cluster_datasets[cid]) > 1)\n",
    "\n",
    "logger.info(f\"\\nFINAL RESULTS:\")\n",
    "logger.info(f\"  Total clusters: {n_total_clusters:,}\")\n",
    "logger.info(f\"  Cross-dataset clusters: {n_cross:,} ({n_cross/n_total_clusters*100:.1f}%)\")\n",
    "logger.info(f\"  Cross-dataset verses: {cross_verses:,} ({cross_verses/len(final_labels)*100:.1f}%)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['cluster_id'] = final_labels\n",
    "\n",
    "# Create cluster summary\n",
    "cluster_info = []\n",
    "for cid, datasets in cluster_datasets.items():\n",
    "    if len(datasets) > 1:\n",
    "        verses = cluster_verses[cid]\n",
    "        counts = Counter(source_datasets[i] for i in verses)\n",
    "        cluster_info.append({\n",
    "            'cluster_id': cid,\n",
    "            'size': len(verses),\n",
    "            'n_datasets': len(datasets),\n",
    "            'datasets': ', '.join(sorted(datasets)),\n",
    "            'dataset_counts': dict(counts)\n",
    "        })\n",
    "\n",
    "cluster_info_df = pd.DataFrame(cluster_info).sort_values('size', ascending=False)\n",
    "\n",
    "logger.info(f\"\\nTop 20 cross-dataset clusters:\")\n",
    "if len(cluster_info_df) > 0:\n",
    "    print(cluster_info_df.head(20).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"SAVING RESULTS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "df.to_csv(RESULTS_DIR / \"concatenated_cross_dataset_clusters.csv\", index=False)\n",
    "logger.info(f\"âœ“ Full results: {RESULTS_DIR / 'concatenated_cross_dataset_clusters.csv'}\")\n",
    "\n",
    "cross_mask = df['cluster_id'].map(lambda cid: len(cluster_datasets[cid]) > 1)\n",
    "df_cross = df[cross_mask].copy()\n",
    "\n",
    "df_cross = df_cross[['verse', 'source_dataset', 'cluster_id']]  # include other relevant columns if needed\n",
    "df_cross.to_csv(RESULTS_DIR / \"cross_dataset_verses_only.csv\", index=False)\n",
    "logger.info(f\"âœ“ Cross-dataset only: {RESULTS_DIR / 'cross_dataset_verses_only.csv'} ({len(df_cross):,} verses)\")\n",
    "\n",
    "\n",
    "cluster_info_df.to_csv(RESULTS_DIR / 'cross_dataset_clusters_summary.csv', index=False)\n",
    "logger.info(f\"âœ“ Cluster summary: {RESULTS_DIR / 'cross_dataset_clusters_summary.csv'}\")\n",
    "\n",
    "# Save final summary\n",
    "final_summary = {\n",
    "    'n_verses': len(df),\n",
    "    'n_datasets': len(set(source_datasets)),\n",
    "    'threshold': threshold,\n",
    "    'resolution': resolution,\n",
    "    'n_total_clusters': n_total_clusters,\n",
    "    'n_cross_clusters': n_cross,\n",
    "    'pct_cross_clusters': n_cross/n_total_clusters*100,\n",
    "    'n_cross_verses': cross_verses,\n",
    "    'pct_cross_verses': cross_verses/len(df)*100\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load sweep results\n",
    "sweep_df = pd.read_csv(\"full_semantic_results/joint_parameter_sweep_results.csv\")\n",
    "\n",
    "# Create a new column for x-axis labels: \"threshold (PXX)\"\n",
    "sweep_df['threshold_label'] = sweep_df.apply(\n",
    "    lambda row: f\"{row['threshold_value']:.4f} (P{int(row['threshold_percentile'])})\", axis=1\n",
    ")\n",
    "\n",
    "# Pivot for heatmap: use the new label column\n",
    "pivot_data = sweep_df.pivot_table(\n",
    "    values='stability',\n",
    "    index='resolution',\n",
    "    columns='threshold_label',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Round resolution values for display\n",
    "pivot_data.index = [f\"{res:.3e}\" for res in pivot_data.index]\n",
    "\n",
    "# Plot heatmap with viridis colormap\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivot_data, annot=True, fmt=\".3f\", cmap='viridis', cbar_kws={'label': 'Stability'})\n",
    "plt.xlabel(\"Cosine Similarity Threshold (Percentile)\", fontweight='bold')\n",
    "plt.ylabel(\"Resolution\", fontweight='bold')\n",
    "plt.title(\"Stability Heatmap with Threshold Values\", fontweight='bold', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_semantic_results/threshold_selection_sem_quant.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "pd.DataFrame([final_summary]).to_csv(RESULTS_DIR / 'clustering_summary.csv', index=False)\n",
    "logger.info(f\"âœ“ Summary: {RESULTS_DIR / 'clustering_summary.csv'}\")\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"âœ… CLUSTERING COMPLETE\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"All results saved to: {RESULTS_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb7344-1d8c-4bc0-8fcc-3b12085ee1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "RESULTS_DIR = \"full_semantic_results_test\"\n",
    "\n",
    "# Load datasets\n",
    "df_cross = pd.read_csv(f\"{RESULTS_DIR}/concatenated_cross_dataset_clusters.csv\")\n",
    "cluster_summary = pd.read_csv(f\"{RESULTS_DIR}/cross_dataset_clusters_summary.csv\")\n",
    "\n",
    "# Only keep clusters with >1 dataset\n",
    "diverse_clusters = cluster_summary[cluster_summary['n_datasets'] > 1] \\\n",
    "    .sort_values(by=['n_datasets', 'size'], ascending=[False, True]) \\\n",
    "    .head(50)\n",
    "\n",
    "# Ensure poem IDs are strings\n",
    "df_cross['idoriginal_poem'] = df_cross['idoriginal_poem'].astype(str)\n",
    "\n",
    "# Precompute poem_key\n",
    "df_cross['poem_key'] = list(zip(df_cross['source_dataset'], df_cross['idoriginal_poem']))\n",
    "\n",
    "# Precompute mapping from poem_key -> set of clusters\n",
    "poem_to_clusters = df_cross.groupby('poem_key')['cluster_id'].apply(set).to_dict()\n",
    "\n",
    "# Precompute cluster -> list of unique poems in that cluster\n",
    "cluster_to_poems = df_cross.groupby('cluster_id')['poem_key'].apply(set).to_dict()\n",
    "\n",
    "# Iterate over clusters\n",
    "for cid, cluster_row in diverse_clusters.iterrows():\n",
    "    if cid not in cluster_to_poems:\n",
    "        continue\n",
    "\n",
    "    poems_in_cluster = list(cluster_to_poems[cid])\n",
    "    \n",
    "    shared_cluster_results = []\n",
    "\n",
    "    # Check all cross-dataset poem pairs for shared clusters\n",
    "    for p1, p2 in combinations(poems_in_cluster, 2):\n",
    "        if p1[0] == p2[0]:  # skip same dataset\n",
    "            continue\n",
    "\n",
    "        shared = (poem_to_clusters[p1] & poem_to_clusters[p2]) - {cid}\n",
    "        if shared:\n",
    "            shared_cluster_results.append((p1, p2, shared))\n",
    "\n",
    "    # Skip cluster if no cross-dataset pairs share other clusters\n",
    "    if not shared_cluster_results:\n",
    "        continue\n",
    "\n",
    "    # Retrieve verses for this cluster\n",
    "    cluster_verses = df_cross[df_cross['cluster_id'] == cid]\n",
    "\n",
    "    print(f\"\\n=== Cluster {cid} ({len(cluster_verses)} verses, {cluster_row['n_datasets']} datasets) ===\")\n",
    "\n",
    "    for _, verse_row in cluster_verses.iterrows():\n",
    "        dataset = verse_row['source_dataset']\n",
    "        verse_text = verse_row['verse']\n",
    "        poem_id = verse_row['idoriginal_poem']\n",
    "        print(f\"[{dataset}] : {poem_id}: {verse_text}\")\n",
    "\n",
    "    print(\"\\n--- Other clusters shared by cross-dataset poem pairs in this cluster ---\")\n",
    "    for p1, p2, shared_clusters in shared_cluster_results:\n",
    "        print(f\"Poem pair {p1} & {p2} co-occur in clusters: {list(shared_clusters)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90a15c6-db59-46e5-b97d-fc1a67abc500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3020693/1581249325.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cross = pd.read_csv(f\"{RESULTS_DIR}/concatenated_cross_dataset_clusters.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cluster 14120 (9 verses, 4 datasets) ===\n",
      "[rhoby] : band3_griechenland_GR72: Ï„Î¿Î´ ÎµÏ„ÎµÎ¹ Ï„ÏÎ¹Ï„Ï‰ ÎºÎ±Ï„Ï‰ÏÏ…Î¾Î± ÏƒÏ‰Î¼Î±\n",
      "[dbbe] : 24938: Ï„Î¿Î½ Ï„Î¿Ï… Ï„ÎµÎ»Î¿Ï…Ï‚ Ï„ÏÎ¹Ï€Î·Ï‡Ï…Î½ Î±Î½Î¸ÎµÎ¹Î»Î¿Ï… Ï„Î±Ï†Î¿Î½\n",
      "[dbbe] : 25552: Î±Ï…Ï„Î±Ï Î¿Î³Îµ Ï„ÏÎ¹Ï„Î±Ï„Ï‰ ÎµÎ½Î¹ Î·Î¼Î±Ï„Î¹ Ï„Ï…Î¼Î²Î¿Î½ Î±Î½Î¿Î¹Î¾Î±Ï‚\n",
      "[phi] : 102903: Ï„ÏÎ¹ÏƒÏƒÎ¿Ï…Ï‚ ÎµÎ½Î¸Î±Î´Îµ Î¸Î±ÏˆÎµ Î»Î²Ï‰\n",
      "[phi] : 102903: Ï„ÏÎ¹ÏƒÏƒÎ¿Ï…Ï‚ ÎµÎ½Î¸Î±Î´Îµ Î¸Î±ÏˆÎµ Î»Î²Ï‰\n",
      "[phi] : 876907: Ï„Î¿Î½ Ï„ÏÎ¹ÎµÏ„Î· Ï„Ï…Î½Î²Ï‰ Ï„Ï‰Î´ Ï…Ï€ÎµÎ¸Î·ÎºÎµ Î½ÎµÎºÏ…Î½\n",
      "[phi] : 23049: ÏƒÎ± ÎµÏ„Î· Î·Ê¹ ÎºÎ±Î¹ Î¼Î·Î½Î±Ï‚ Ï„ÏÎ¹Ï‰Î½ Â²â¶Ï„ÏÎµÎ¹Ï‚Â²â¶ Ï„Î±Ï†ÎµÎ¹ÏƒÎ±\n",
      "[papyri] : 3581: Ï„Î¿ Ï„ÏÎ¹Ï„Î¿Î½ Î¹Î¼Î¿Ï…Î¸Î·Ï‚ Î¶Î¼Î¹Î½Î¿Ï‚ ÎµÏ‰Ï‚ Ï„Î·Ï‚ Ï„Î±Ï†Î·Ï‚\n",
      "[papyri] : 23049: ÏƒÎ± ÎµÏ„Î· Î· ÎºÎ±Î¹ Î¼Î·Î½Ï‰Î½ Ï„ÏÎ¹Ï‰Î½ Ï„Î±Ï†ÎµÎ¹ÏƒÎ±\n",
      "\n",
      "--- Other clusters shared by cross-dataset poem pairs in this cluster ---\n",
      "Poem pair ('phi', '23049') & ('papyri', '23049') co-occur in clusters: [4, 1052]\n",
      "\n",
      "=== Cluster 12289 (13 verses, 4 datasets) ===\n",
      "[rhoby] : band2_epigramme_auf_ikonen_Ik31: Î¿ÏÏ‰ ÏƒÎµ Ï„Î±Ï†Îµ Î´ÎµÎ¹Î»Î¹Ï‰ ÏƒÎ¿Ï… Ï„Î·Î½ Î¸ÎµÎ±Î½\n",
      "[dbbe] : 19610: Î¿ÏÏ‰ ÏƒÎµ Î¸Î±Î½Î±Ï„Îµ Î´ÎµÎ¹Î»Î·Ï‰ ÏƒÎ¿Ï… Ï„Î·Î½ Î¸ÎµÎ±Î½\n",
      "[dbbe] : 33805: Î¿ÏÏ‰ ÏƒÎµ Î¸Î±Î½Î±Ï„Îµ Î´ÎµÎ¹Î»Î¹Ï‰ ÏƒÎ¿Ï… Ï„Î·Î½ Î¸ÎµÎ±Î½\n",
      "[phi] : 898761: ÏƒÎµÎ¹Î¿Ï‚ Î±Î¼Ï†Î¹Î½Î¿Î¿Ï‚ Î¸Î¹Î¿Î¶Î¿Ï„ÎµÎ¹Î¿Ï‚ Î¸Î¹Î¿\n",
      "[phi] : 779322: Ï„Î¿Î½ Ï„Îµ Î¸ÎµÎ¿Ï‚ Ï€Î±Ï„Î±Î¾Î±Î¹ Î±Ï€Î¿ÏÎ¹Î± ÎºÎ±Î¹\n",
      "[phi] : 387536: Î¸ÎµÎ¿Ï… Î±Î½Ï„Î¹Î½Î¿Î¿Ï… Ï„Î¹Ï„Î¿Î½ Ï†Î»Î±\n",
      "[phi] : 931278: ÏƒÎ¿Ï… Î¹ÎµÏÎ±ÏƒÎ±Î¼ÎµÎ½Î·Î½ Î¸ÎµÎ±Ï‚ ÏƒÎµ\n",
      "[phi] : 793812: Ï…ÏƒÎ¹Î¿Ï‚ Î±Î¼ÎµÎ¹Î½Î¹Î¿Ï… Î±Ï‡Î±ÏÎ½ÎµÏ…Ï‚\n",
      "[papyri] : 27550: ÏƒÎµÎ¹Î±Ï‚ Ï„Î·Ï‚ ÎµÏ€Î·Î½Ï„Î»Î·Î¼ÎµÎ½Î·Ï‚ ÎºÎ±Ï„Î± Ï„Î¿ ÎµÎ¸Î¿Ï‚\n",
      "[papyri] : 1054: Ï‡Î±Î¹ÏÏ‰Î½ ÎµÏ‰Ï‚ Î±Î½ Î±Î¼Î¼Ï‰Î½Î¹Î¿Ï‚\n",
      "[papyri] : 3724: Î±Î¼Î¼Ï‰Î½Î¹Î·Î¿Ï… Î´Î¹Î± Î±Î¼ÎµÎ½Î½ÎµÏ‰Ï‚Î¼Î±ÏÏÎ·Î¿Ï…Ï‚\n",
      "[papyri] : 9130: Î±Ï€Î¿ Î±Î¼Ï†Î¿Î´Î¿Ï… Î±Î¼Ï‰Î½Î¹Î¿Ï…\n",
      "[papyri] : 38238: ÏƒÎµÎ¹Ï‚ Î±ÎºÎ¿Ï…ÏƒÎ±Ï‚ Ï€Î±ÏÎ± Ï„Î¿Ï… Î¸Îµ\n",
      "\n",
      "--- Other clusters shared by cross-dataset poem pairs in this cluster ---\n",
      "Poem pair ('rhoby', 'band2_epigramme_auf_ikonen_Ik31') & ('dbbe', '19610') co-occur in clusters: [2809, 17530]\n",
      "Poem pair ('rhoby', 'band2_epigramme_auf_ikonen_Ik31') & ('dbbe', '33805') co-occur in clusters: [2809, 17530]\n",
      "\n",
      "=== Cluster 12284 (13 verses, 4 datasets) ===\n",
      "[rhoby] : band3_griechenland_GR69: ÎµÏƒÏ„Î·ÏƒÎ± ÎºÎ±ÎºÏ‰Ï‚ Î»Î¹Î¸Î¿ÏƒÏ…Î½Î¸ÎµÏ„Î¿Î½ Ï„Î±Ï†Î¿Î½\n",
      "[rhoby] : band3_tÃ¼rkei_TR1: ÎºÎ±Ï„ÎµÏ„ÎµÎ¸Î· ÎµÎ½ Ï„Î¿Ï…Ï„Ï‰ Ï„Ï‰ Ï„Ï…Î¼Î²Ï‰ Î¼Î·Î½Î¹ Î¿ÎºÏ„Î¿Ï…Î¼Î²ÏÎ¹Ï‰ Î¹ ÎµÎ½ ÎµÏ„Ï‰ Ï‚Ï†Î¾Î± Î¹Î½Î´Î¹ÎºÏ„Î¹Ï‰Î½Î¿Ï‚ Ï‚\n",
      "[dbbe] : 21763: Ï„Î±Ï†Î¿Ï‚ Î¼Î±ÏÎ±Î¹Î½ÎµÎ¹ ÎºÎ±Î¹ ÎºÎ±Î»Ï…Ï€Ï„ÎµÎ¹ Î¼Îµ Î»Î¹Î¸Î¿Ï‚\n",
      "[phi] : 771344: ÎºÎ±Î¹ ÎµÏ€Î¹ Ï„Ï‰Î½ Ï„Î±Ï†Ï‰Î½ ÎµÎ¼ Î¼Î·Î½Î¹ Ï…Î±ÎºÎ¹Î½Î¸Î¹\n",
      "[phi] : 867006: Î¿Ï… Ï„Î±Ï†Î¿Ï² Î±Î»Î»Î± Î»Î¹Î¸Î¿Ï² Ï²Ï„Î·Î»Î· Î¼Î¿\n",
      "[phi] : 921290: Î¸Ï…Î¿Î½ Î»Î¿Î³Î¿Î½ ÎºÎ±Ï„ÎµÏ„ÎµÎ¸Î· ÎµÎ½ Ï„Î¿Ï…Ï„Ï‰ Ï„Ï‰ Ï„Ï…Î¼Î²Ï‰ Î¼Î·Î½Î¹ Ï‰ÎºÏ„Î¿Ï…\n",
      "[phi] : 841462: Î¼Î¿Ï… Î±ÏÏ„ÎµÎ¼ÎµÎ¹ Î¹Ï‚ Î¿ Î¿Ï…Î´Î¹Ï‚ Î±Î»Î¿Ï‚ Â²â¶Î±Î»Î»Î¿Ï‚Â²â¶ Ï„Î±Ï†Î·ÏƒÎµ\n",
      "[phi] : 867006: Î¿Ï… Ï„Î±Ï†Î¿Ï‚ Î±Î»Î»Î± Î»Î¹Î¸Î¿Ï‚ ÏƒÏ„Î·Î»Î· Î¼Î¿\n",
      "[phi] : 849041: Î¸Î·ÎºÎ· Î¼Î·Î½Î± ÎºÎ±Ï€Î¹Î»Î¿Ï…\n",
      "[phi] : 852227: Î¸Î·ÎºÎ· Ï„Î· Ï„Î¿Ï… ÎºÎµÎ»ÏƒÎ¿Ï… ÎµÎ¿ÏÏ„Î· Ï‰ÏƒÏ„Îµ Î¼Î·Î½Î¿Ï‚ Ï‰Î½Î¿Ï‚ ÎµÏ€Ï„Î±\n",
      "[papyri] : 78366: Î¼ÎµÏ„ÏÏ‰ Î±Î¸Î·Î½Î±Î¹Ï‰ ÎºÎ±Î¸Î¹ÏƒÏ„Î±Î½Î¿Î¼ÎµÎ½Î¿Î½ Î±Ï…Ï„Î¿Î½ Ï„Î¿Î½ Ï†Î¿ÏÎ¿Î½ Ï‚ Î±Î½Ï…Ï€ÎµÏÎ¸ÎµÏ„Ï‰Ï‚ ÎµÎ½ Î¼Î·Î½Î¹ ÎµÏ€ÎµÎ¹Ï† ÎºÎ±Ï„Ê¼ ÎµÏ„Î¿Ï‚\n",
      "[papyri] : 33203: Î¸Î·ÎºÎ± Ï„Ï‰ Ï€Î±ÏÏ‰Ï‡Î·Î¼ÎµÎ½Ï‰ Î¼Î·Î½Î¹\n",
      "[papyri] : 8862: Î¼Î·Î½Î¹ Ï‡Î¿Î¹Î±Îº Î±Î³Î½Î¹Î±Ï‚ ÏƒÎ±ÏÎ±Ï€Î¹Î´Î¿Ï‚ ÏƒÏ€Î¿ ca Î±Î³Î½Î¹Î±Ï‚\n",
      "\n",
      "--- Other clusters shared by cross-dataset poem pairs in this cluster ---\n",
      "Poem pair ('rhoby', 'band3_tÃ¼rkei_TR1') & ('phi', '921290') co-occur in clusters: [192]\n",
      "\n",
      "=== Cluster 11529 (15 verses, 4 datasets) ===\n",
      "[rhoby] : band2__epigramme_auf_edelmetallen_Me7: Î¹Ï‰Î±Î½Î½Î·Ï‚ Ï†ÎµÏÏ‰ ÏƒÎµ Î´Î¿Ï…ÎºÎ±Ï‚ Î½Î¿ÏƒÏ„Î¿Î³ÎºÏ‰Î½\n",
      "[dbbe] : 25785: Î±Î¼Î± Î¹Ï‰Î±Î½Î½Î· Î»ÎµÎ³Ï‰ Ï„Ï‰ Î¼ÎµÏ„Î± ÏƒÎ¿Ï…\n",
      "[dbbe] : 26334: Î±Î¼Î± Ï„Ï‰ Î¹Ï‰Î±Î½Î½Î· Î»ÎµÎ³Ï‰ Ï„Ï‰ Î¼ÎµÏ„Î± ÏƒÎ¿Ï…\n",
      "[dbbe] : 29468: Î¹Ï‰Î±Î½Î½Î·Ï‚ Ï„Î¿Î½ Î±Î³Î±Ï€Î·Ï„Î¿Î½ Î¼Îµ ÏƒÎºÎµÏ€Î¿Î¹Ï‚\n",
      "[phi] : 749213: Î½Î± Î¹Ï‰Î±Î½Î½Î¿Ï… Î¼Îµ\n",
      "[papyri] : 20773: Î¹Ï‰Î±Î½Î½Î·Ï‚ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¹ Î¼Î¿Î¹ Î¹Î¿Ï…ÏƒÏ„Î¿Ï‚\n",
      "[papyri] : 9184: ÎºÎ±Î¹ Î±Ï€ÎµÏ‡Ï‰ Ï€Î±ÏÎ± ÏƒÎ¿Ï… Ï„Î·Î½ ÏƒÏ…Î½Ï€ÎµÏ†Ï‰Î½Î·\n",
      "[papyri] : 37230: Ï€Î±ÏÎµÎ¹ÏƒÏ„Î¿Ï…Î½Ï„ÎµÏ‚ ÏƒÎµ ÎºÎ±Î¹ Î´ÎµÎ´Ï‰ÎºÎ± Ï„Î±Ï‚ ÎµÎ³Î³Ï…Î±Ï‚ ÎºÎ±Î¹ Î¿Ï…Îºapostrophe Î·Ï…ÏÎ¿Î½ ÎµÎ±Î½ Î±Î¼Î± Î¿Î»Î¿ÎºÎ¿Ï„Î¹Î½Î¿Î½ Î´Î¿Ï…Î½Î±Î¹ Î»Î¿Î³Ï‰\n",
      "[papyri] : 22156: Î¹Ï‰Î±Î½Î½Î· ÏƒÏ…Î½Î±Î»Î»Î±Î³Î¼Î±Ï„Î¿Î³ÏÎ±Ï†Ï‰\n",
      "[papyri] : 15454: Î± Î»Î¿Î³Ï‰Î½ ÎµÏ‡Ï‰ Î¼ÎµÏ„Ê¼ ÎµÏƒ\n",
      "[papyri] : 38415: Î¹Ï‰Î±Î½Î½Î· Î¶Ï…Î³Î¿ÏƒÏ„Î±Ï„Î· Î±Ï€Î¿ Î»Î¿Î³Î¿Ï… Î½Î±ÏÎ±Ï… ÎºÎ¿Ï…ÏÎ±Ï„Î¿ÏÎ¿Ï‚ ÏƒÎ¹Ï„Î¿Ï… ÎºÎ±Î³ÎºÎµÎ»Î»Ï‰\n",
      "[papyri] : 34512: Î¹Ï‰Î±Î½Î½Î·Ï‚ Î±Î¼Î¹\n",
      "[papyri] : 34369: ÏƒÎµÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Î¹ ÎµÎ³Ï‰ Î¹Ï‰Î±Î½Î½Î·Ï‚ ÎºÎ±Î¹\n",
      "[papyri] : 74138: ÏƒÎµÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Î¹ ÎµÎ³Ï‰ Î¹Ï‰Î±Î½Î½Î·Ï‚\n",
      "[papyri] : 34219: Î¹Ï‰Î±Î½Î½Î· Î¿Î¹Î½Î¿Ï… Ï‡ÏÎ·Ï„Î·Î½ Î¼Î¹\n",
      "\n",
      "--- Other clusters shared by cross-dataset poem pairs in this cluster ---\n",
      "Poem pair ('papyri', '34512') & ('dbbe', '26334') co-occur in clusters: [64]\n",
      "\n",
      "=== Cluster 10835 (16 verses, 4 datasets) ===\n",
      "[rhoby] : band2__epigramme_auf_edelmetallen_Me18: Î»Î·ÏƒÏ„Î·Ï‚ Î´Îµ Ï€Î±ÏÏ‰Î½ ÎµÏ…Î¼ÎµÎ½Î·Ï‚ Ï€ÏÎ¿Ï‚ Ï„Î±Î¹Ï‚ Ï€Ï…Î»Î±Î¹Ï‚\n",
      "[rhoby] : band1_epigrfreskenhab_122: Ï€Î¿ÏÏÏ‰ Ï€ÎµÏ†ÎµÏ…Î³Îµ Ï„Î·Î½ Ï€Ï…Î»Î·Î½ Ï„Î·Ï‚ ÎµÎ¹ÏƒÎ¿Î´Î¿Ï…\n",
      "[dbbe] : 33804: Ï€Î¿ÏÏÏ‰ Ï€Î¿ÏÏÏ‰ Ï€ÎµÏ†ÎµÏ…Î³Îµ Ï„Î·Î½ Ï€Ï…Î»Î·Î½ Ï„Î·Ï‚ ÎµÎ¹ÏƒÎ¿Î´Î¿Ï…\n",
      "[dbbe] : 27108: Ï€Ï‰ÏÎ¿ Ï€Ï‰ÏÎ¿ Ï€ÎµÏ†ÎµÏ…Î³Îµ Ï„Î·Î½ Ï€Ï…Î»Î·Î½ Ï„Î±Ï…Ï„Î·Î½\n",
      "[dbbe] : 35285: ÎµÎºÎ¿ÏˆÎµÎ½ Î±Ï€Î·Î³Î±Î³Îµ Ï€ÏÎ¿Ï‚ ÏƒÎºÎ¿Ï„Î¿Ï… Ï€Ï…Î»Î±Ï‚\n",
      "[dbbe] : 35290: ÎµÎºÎ¿ÏˆÎµÎ½ Î±Ï€Î·Î³Î±Î³Îµ Ï€ÏÎ¿Ï‚ ÏƒÎºÎ¿Ï„Î¿Ï… Ï€Ï…Î»Î±Ï‚\n",
      "[dbbe] : 35289: ÎµÎºÎ¿ÏˆÎµÎ½ Î±Ï€Î·Î³Î±Î³ÎµÎ½ Ï€ÏÎ¿Ï‚ ÏƒÎºÎ¿Ï„Î¿Ï…Ï‚ Ï€Ï…Î»Î±Ï‚\n",
      "[phi] : 893549: ÏÎ±Ï„ÎµÎ¹Î¿Î½ Ï„Î¿Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎµÏ†Î¿Î½Ï„Î¿Ï‚ ÎµÏ„ Ï„Î±Î½ Ï€Ï…Î»Î±Î½ Ï„Î±Î½ ÎµÏ€Î¹ ÏƒÎºÎ¿Ï„ÎµÏƒÏƒÎ±Î½\n",
      "[phi] : 786766: ÎµÎ½ Î±ÏÎ± Ï„Î¿Î¹ÏƒÎ¶ Î±Î´Î±Î¼Î±Î½Ï„Î¹ Ï€ÎµÏ†ÏÎ±Î³Î¼ÎµÎ½Î¿Î½ ÎµÏ„Î¿Ï â„ŽÎ¿Ï„ Î±Î¹Ï‡Î¼ÎµÎ½ ÏƒÏ„ÎµÏƒÎ±Î¼ Ï€ÏÎ¿ÏƒÎ¸Îµ Ï€Ï…Î»Î¿Î½ Î±Î½Ï„Î¹Î± Ï„Î¿Ï‡ÏƒÎ¿Ï†Î¿ÏÎ¿Î½\n",
      "[phi] : 787586: Ï„Î¿Î½Î´Îµ Ï€Î¿Ï„ÎµÎ¹Î´Î±Î¹Î±Ï‚ Î´ Î±Î¼Ï†Î¹ Ï€Ï…Î»Î±Ï‚ ÎµÎ»Ï…Î¸ÎµÎ½\n",
      "[phi] : 768926: ÏˆÎ·Ï†Î¹ÏƒÎ¶ÎµÎ½ Ï€Ï…Î»Î±Î´Î·Ï‚ Î±Î¹ÏƒÏ‡ÏÎ¹Ï‰Î½Î¿Ï‚ Ï€ÎµÏÎ¹Î¸Î¿Î¹\n",
      "[phi] : 787586: Ï„Î¿Î½Î´Îµ Ï€Î¿Ï„ÎµÎ¹Î´Î±Î¹Î±Ï‚ Î´ Î±Î¼Ï†Î¹ Ï€Ï…Î»Î±Ï‚ ÎµÎ»Ï…Î¸ÎµÎ½\n",
      "[phi] : 121548: ÎµÎ½ Î±ÏÎ± Ï„Î¿Î¹ÏƒÎ¶ Î±Î´Î±Î¼Î± â„ŽÎ¿Ï„ Î±Î¹Ï‡Î¼ÎµÎ½ ÏƒÏ„ÎµÏƒÎ±Î¼ Ï€ÏÎ¿ÏƒÎ¸Îµ Ï€Ï…Î»Î¿Î½ Î±Î½\n",
      "[papyri] : 19740: ÎºÎ±Î¹ Ï€Ï…Î»Î¿Î½Î¿Ï‚ ÎºÎ±Î¹ Ï€ÎµÏƒÏƒÎ¿Ï… ÎºÎ±Î¹ Î·Î¼Î¹ÎºÎ»Î¹Î²Î±Î½Î¿Ï… Ï€ÏÎ¿Ï‚ Ï„Î·Î½ Î¼ÎµÏ‡ÏÎ¹ Î½Ï…Î½\n",
      "[papyri] : 19748: Ï„Î¿Ï… Ï„Îµ Ï€ÏÎ¿Î¸Ï…ÏÎ¿Ï… ÎºÎ±Î¹ Ï€Ï…Î»Î¿Î½Î¿Ï‚ ÎºÎ±Î¹ Ï€ÎµÏƒÏƒÎ¿Ï… Ï€Î±ÏÎ±Î´ÏÎ¿Î¼Î¹Î´Î¿Ï‚ ÎºÎ±Î¹\n",
      "[papyri] : 19126: Ï€Ï…Î»Î·Ï‚ ÏƒÎ¿Î·Î½Î·Ï‚ Î´Î¹Î± Ï†Î±Î½Ï‰Ï†ÎµÏ‰Ï‚ Î²Î¿Î·Î¸Î¿Ï… Î´Î¹ÎµÎ³ÏÎ±ÏˆÎµÎ½ Î±ÏÏ€Î±Î·ÏƒÎ¹Ï‚\n",
      "\n",
      "--- Other clusters shared by cross-dataset poem pairs in this cluster ---\n",
      "Poem pair ('rhoby', 'band1_epigrfreskenhab_122') & ('dbbe', '33804') co-occur in clusters: [472, 112]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### This prints verses that were clustered together, prioritizing cross-dataset clusters, aiming for verses belonging to small poems, \n",
    "### Only showing verses of which the corresponding poem also shares other verses \n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "RESULTS_DIR = \"full_semantic_results_test\"\n",
    "\n",
    "# Load datasets\n",
    "df_cross = pd.read_csv(f\"{RESULTS_DIR}/concatenated_cross_dataset_clusters.csv\")\n",
    "cluster_summary = pd.read_csv(f\"{RESULTS_DIR}/cross_dataset_clusters_summary.csv\")\n",
    "\n",
    "# Only keep clusters with >1 dataset\n",
    "diverse_clusters = cluster_summary[cluster_summary['n_datasets'] > 1] \\\n",
    "    .sort_values(by=['n_datasets', 'size'], ascending=[False, True]) \\\n",
    "    .head(50)\n",
    "\n",
    "# Ensure poem IDs are strings\n",
    "df_cross['idoriginal_poem'] = df_cross['idoriginal_poem'].astype(str)\n",
    "\n",
    "# Precompute poem_key\n",
    "df_cross['poem_key'] = list(zip(df_cross['source_dataset'], df_cross['idoriginal_poem']))\n",
    "\n",
    "# Compute poem lengths\n",
    "poem_lengths = df_cross.groupby('poem_key').size().to_dict()\n",
    "\n",
    "# Precompute mapping from poem_key -> set of clusters (only poems with <10 verses)\n",
    "poem_to_clusters = {\n",
    "    k: set(v['cluster_id'])\n",
    "    for k, v in df_cross.groupby('poem_key')\n",
    "    if poem_lengths[k] < 10\n",
    "}\n",
    "\n",
    "# Precompute cluster -> list of unique poems in that cluster (only <10 verses)\n",
    "cluster_to_poems = {}\n",
    "for cid, group in df_cross.groupby('cluster_id'):\n",
    "    poems = set([k for k in group['poem_key'] if poem_lengths[k] < 10])\n",
    "    if poems:\n",
    "        cluster_to_poems[cid] = poems\n",
    "\n",
    "# Iterate over clusters\n",
    "for idx, cluster_row in diverse_clusters.iterrows():\n",
    "    cid = cluster_row['cluster_id']\n",
    "    if cid not in cluster_to_poems:\n",
    "        continue\n",
    "\n",
    "    poems_in_cluster = list(cluster_to_poems[cid])\n",
    "    \n",
    "    shared_cluster_results = []\n",
    "\n",
    "    # Check all cross-dataset poem pairs for shared clusters\n",
    "    for p1, p2 in combinations(poems_in_cluster, 2):\n",
    "        if p1[0] == p2[0]:  # skip same dataset\n",
    "            continue\n",
    "\n",
    "        shared = (poem_to_clusters[p1] & poem_to_clusters[p2]) - {cid}\n",
    "        if shared:\n",
    "            shared_cluster_results.append((p1, p2, shared))\n",
    "\n",
    "    # Skip cluster if no cross-dataset pairs share other clusters\n",
    "    if not shared_cluster_results:\n",
    "        continue\n",
    "\n",
    "    # Retrieve verses for this cluster\n",
    "    cluster_verses = df_cross[df_cross['cluster_id'] == cid]\n",
    "\n",
    "    print(f\"\\n=== Cluster {cid} ({len(cluster_verses)} verses, {cluster_row['n_datasets']} datasets) ===\")\n",
    "\n",
    "    for _, verse_row in cluster_verses.iterrows():\n",
    "        dataset = verse_row['source_dataset']\n",
    "        verse_text = verse_row['verse']\n",
    "        poem_id = verse_row['idoriginal_poem']\n",
    "        print(f\"[{dataset}] : {poem_id}: {verse_text}\")\n",
    "\n",
    "    print(\"\\n--- Other clusters shared by cross-dataset poem pairs in this cluster ---\")\n",
    "    for p1, p2, shared_clusters in shared_cluster_results:\n",
    "        print(f\"Poem pair {p1} & {p2} co-occur in clusters: {list(shared_clusters)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a83d6be-138e-40ef-9d15-4f0838fbd4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3020693/1656130796.py:10: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cross = pd.read_csv(f\"{RESULTS_DIR}/concatenated_cross_dataset_clusters.csv\")\n",
      "/tmp/ipykernel_3020693/1656130796.py:11: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  original_df = pd.read_csv(\"concatenated.csv\")  # contains full poems\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '32985') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 32985 (1 verses)\n",
      "Îµá¼°Ï‚ Ï„á½¸Î½ Î»Î¿Ï…Îº(á¾¶Î½) á¼„Î½Ï‰Î¸(ÎµÎ½) Ï„ÏÎµá¿–Ï‚ ÏƒÏ„Î¯Ï‡(Î¿Î¹) Ï€Î¬Î»Î¹Î½Î‡\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '17824') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 17824 (1 verses)\n",
      "+ Ï„Î­Î»Î¿Ï‚ Î´ÎµÏ…Ï„Î­ÏÎ±Ï‚ Ï„á¿¶Î½ Ï„ÏÎ¹á¿¶Î½ Ï„á¿†Ï‚ Ï„ÏÎ¹Î²Î¯Î²Î»Î¿Ï…\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23115') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23115 (1 verses)\n",
      "+ Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚ Î´Î¯Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ Ï„Îµá½´Î½ Ï€Î¿Î»Ï…Î¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '42748') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 42748 (5 verses)\n",
      "Î£ÏÏÏ‰Î½\n",
      "Î Ï„Î¿Î»ÎµÎ¼Î±á¿–Î¿Ï‚ á¼¸Î¿Ï…-\n",
      "Î»Î¹Î±Î½Î¿á¿¦ ÏƒÎ¬ÎºÎºÎ¿Î¹\n",
      "Ï„ÏÎµá¿–Ï‚, Î³Î¯(Î½Î¿Î½Ï„Î±Î¹) Î³Â´. Î´Â´ (á¼”Ï„Î¿Ï…Ï‚)\n",
      "á¼‰Î¸á½ºÏ ÎºÏ›Â´.\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '69698') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 69698 (4 verses)\n",
      "... á¼ˆÎ¸á½ºÏ Î¹Â´\n",
      "Ï„.ÏƒÏƒ...Ï„Î¹ÏÎ±Ïƒ\n",
      "Î£Î¿Î¼Î¼.Î½Ï„ (Ï„ÏÎ¹ÏŽÎ²Î¿Î»Î¿Î½)Â´\n",
      "...\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23178') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23178 (1 verses)\n",
      "+Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚. Î´á¿–Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ | Ï„Îµá½´Î½ Ï€Î¿Î»ÏÎ¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½.\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23595') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23595 (1 verses)\n",
      "Îµá¼°Ï‚ Ï„(á½¸Î½) Î»Î¿Ï…Îºá¾¶Î½ á¼„Î½Ï‰Î¸(ÎµÎ½) Ï„ÏÎµá¿–Ï‚ ÏƒÏ„Î¯Ï‡Î¿Î¹ Ï€Î¬Î»(Î¹Î½):\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23180') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23180 (1 verses)\n",
      "+ Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚. Î´Î¯Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ Ï„Îµ|á½´Î½ Ï€Î¿Î»ÏÎ¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½.\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '31882') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 31882 (1 verses)\n",
      "â€§:â€§ ÎºÎ±á½¶ Î´á½¶Ï‚ Î¼ÎµÏ„ÎµÎ»Î¸Îµá¿–Î½ ÎºÎ±á½¶ Ï„Ïá½¶Ï‚ á¼¢ ÎºÎ±á½¶ Ï€Î¿Î»Î»á½±ÎºÎ¹Ï‚.\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '30735') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 30735 (5 verses)\n",
      "ÎœÎ±ÏÏ‰Î½á½¶Ï‚ Î Î±-\n",
      "Î»Î®Î¼Î¿Î½Î¿Ï‚, á½„Î½Î¿Î¹\n",
      "Ï„ÏÎµá¿–Ï‚, Î³(Î¯Î½Î¿Î½Ï„Î±Î¹) Î³Â´,\n",
      "Î´Î¹(á½°) ÎšÎ¿Î»Î»Î¿ÏÎ¸Î¿Ï…\n",
      "á½€Î½Î·Î»(Î¬Ï„Î¿Ï…), á¼€Ï‡ÏÏÎ¿Ï….\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '32103') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 32103 (5 verses)\n",
      "Î»Î¹(Ï„Ïá¿¶Î½)(*) Î´Â´ ð…µ\n",
      "á½€ÎºÏ„Î±ÏƒÎ¿á¿¦Ï†Î±\n",
      "ÎºÎ±á½¶ Ï„ÏÎ¹ÏƒÎ¿á¿¦Ï†Î± á¼„Î½Ï‰ ÎºÎ±á½¶\n",
      "ÎºÎ¬Ï„Ï‰ Î´Î¹ÎºÏŒÎ½Ï„Ï…Î»Î±(*)\n",
      "Î´Ï‰Î´ÎµÎºÎ¬ÎºÏ…ÎºÎ»Î±.\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '19321') & ('phi', '775777') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 19321 (2 verses)\n",
      "(...)\n",
      "Ï„Ï…Ï€á¿¶Î½ Î´Î¹á¾½ Î±á½Ï„(á¿¶Î½) Ï„á½´Î½ Ï„ÏÎ¹Î¬Î´Î± Îº(Î±á½¶) Î¼ÏŒÎ½(...):\n",
      "\n",
      "[phi] Poem ID: 775777 (2 verses)\n",
      ".â€” â€” á¼Îº Î¤ÏÎ¹ÎºÎ¿ÏÏ[Î¸Î¿Ï…]\n",
      ".[á¼ˆÏƒÎºÎ»]Î·Ï€Î¹Î¬Î´Î¿Ï….\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '32985') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 32985 (1 verses)\n",
      "Îµá¼°Ï‚ Ï„á½¸Î½ Î»Î¿Ï…Îº(á¾¶Î½) á¼„Î½Ï‰Î¸(ÎµÎ½) Ï„ÏÎµá¿–Ï‚ ÏƒÏ„Î¯Ï‡(Î¿Î¹) Ï€Î¬Î»Î¹Î½Î‡\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '17824') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 17824 (1 verses)\n",
      "+ Ï„Î­Î»Î¿Ï‚ Î´ÎµÏ…Ï„Î­ÏÎ±Ï‚ Ï„á¿¶Î½ Ï„ÏÎ¹á¿¶Î½ Ï„á¿†Ï‚ Ï„ÏÎ¹Î²Î¯Î²Î»Î¿Ï…\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23115') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23115 (1 verses)\n",
      "+ Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚ Î´Î¯Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ Ï„Îµá½´Î½ Ï€Î¿Î»Ï…Î¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '42748') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 42748 (5 verses)\n",
      "Î£ÏÏÏ‰Î½\n",
      "Î Ï„Î¿Î»ÎµÎ¼Î±á¿–Î¿Ï‚ á¼¸Î¿Ï…-\n",
      "Î»Î¹Î±Î½Î¿á¿¦ ÏƒÎ¬ÎºÎºÎ¿Î¹\n",
      "Ï„ÏÎµá¿–Ï‚, Î³Î¯(Î½Î¿Î½Ï„Î±Î¹) Î³Â´. Î´Â´ (á¼”Ï„Î¿Ï…Ï‚)\n",
      "á¼‰Î¸á½ºÏ ÎºÏ›Â´.\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('papyri', '69698') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[papyri] Poem ID: 69698 (4 verses)\n",
      "... á¼ˆÎ¸á½ºÏ Î¹Â´\n",
      "Ï„.ÏƒÏƒ...Ï„Î¹ÏÎ±Ïƒ\n",
      "Î£Î¿Î¼Î¼.Î½Ï„ (Ï„ÏÎ¹ÏŽÎ²Î¿Î»Î¿Î½)Â´\n",
      "...\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23178') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23178 (1 verses)\n",
      "+Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚. Î´á¿–Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ | Ï„Îµá½´Î½ Ï€Î¿Î»ÏÎ¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½.\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23595') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23595 (1 verses)\n",
      "Îµá¼°Ï‚ Ï„(á½¸Î½) Î»Î¿Ï…Îºá¾¶Î½ á¼„Î½Ï‰Î¸(ÎµÎ½) Ï„ÏÎµá¿–Ï‚ ÏƒÏ„Î¯Ï‡Î¿Î¹ Ï€Î¬Î»(Î¹Î½):\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n",
      "\n",
      "================================================================================\n",
      "Poem pair ('dbbe', '23180') & ('phi', '842630') share 1 clusters (score=1.000, combined length=2)\n",
      "================================================================================\n",
      "\n",
      "[dbbe] Poem ID: 23180 (1 verses)\n",
      "+ Ï„ÏÎ¹á½°Ï‚ Î¼Î¿Î½á½°Ï‚. Î´Î¯Î´Î¿Ï… Ï€Î¿Î½Î­Î¿Î½Ï„Î¹ Ï„Îµ|á½´Î½ Ï€Î¿Î»ÏÎ¿Î»Î²Î¿Î½ á¼€ÏÏ‰Î³Î®Î½.\n",
      "\n",
      "[phi] Poem ID: 842630 (1 verses)\n",
      ".Ï„(ÏÎ¹Ï„Î·Î¼ÏŒÏÎ¹Î¿Î½) {Â²â·Ï„(ÏÎ¯Ï„Î¿Î½)?}Â²â·.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "RESULTS_DIR = \"full_semantic_results_test\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load data\n",
    "# ----------------------------\n",
    "df_cross = pd.read_csv(f\"{RESULTS_DIR}/concatenated_cross_dataset_clusters.csv\")\n",
    "original_df = pd.read_csv(\"concatenated.csv\")  # contains full poems\n",
    "\n",
    "# Ensure IDs are strings\n",
    "df_cross['idoriginal_poem'] = df_cross['idoriginal_poem'].astype(str)\n",
    "original_df['idoriginal_poem'] = original_df['idoriginal_poem'].astype(str)\n",
    "\n",
    "# Build unique poem key\n",
    "df_cross['poem_key'] = list(zip(df_cross['source_dataset'], df_cross['idoriginal_poem']))\n",
    "\n",
    "# ----------------------------\n",
    "# Compute poem lengths (number of verses)\n",
    "# ----------------------------\n",
    "poem_lengths = df_cross.groupby('poem_key').size().to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# Precompute poem -> clusters\n",
    "# ----------------------------\n",
    "poem_to_clusters = df_cross.groupby('poem_key')['cluster_id'].apply(set).to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# Precompute cluster -> poems\n",
    "# ----------------------------\n",
    "cluster_to_poems = df_cross.groupby('cluster_id')['poem_key'].apply(set).to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# Identify candidate poem pairs that share clusters\n",
    "# ----------------------------\n",
    "shared_cluster_counts = defaultdict(int)  # (p1, p2) -> count of shared clusters\n",
    "\n",
    "# Only consider cross-dataset poem pairs\n",
    "for cid, poems in cluster_to_poems.items():\n",
    "    poems = list(poems)\n",
    "    for p1, p2 in combinations(poems, 2):\n",
    "        if p1[0] == p2[0]:  # skip same dataset\n",
    "            continue\n",
    "        key = tuple(sorted([p1, p2]))\n",
    "        shared_cluster_counts[key] += 1\n",
    "\n",
    "# ----------------------------\n",
    "# Rank poem pairs by shared clusters / total length\n",
    "# ----------------------------\n",
    "# Using a fast scoring function: shared clusters divided by total verses\n",
    "top_pairs = sorted(\n",
    "    shared_cluster_counts.items(),\n",
    "    key=lambda x: x[1] / (poem_lengths[x[0][0]] + poem_lengths[x[0][1]]),\n",
    "    reverse=True\n",
    ")[:20]  # top 20 pairs\n",
    "\n",
    "# ----------------------------\n",
    "# Print full poems for these pairs\n",
    "# ----------------------------\n",
    "for (p1, p2), score in top_pairs:\n",
    "    shared_count = shared_cluster_counts[(p1, p2)]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Poem pair {p1} & {p2} share {shared_count} clusters \"\n",
    "          f\"(score={score:.3f}, combined length={poem_lengths[p1]+poem_lengths[p2]})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for poem_key in [p1, p2]:\n",
    "        dataset, poem_id = poem_key\n",
    "        poem_verses = original_df[(original_df['source_dataset'] == dataset) & \n",
    "                                  (original_df['idoriginal_poem'] == poem_id)].sort_values('order')\n",
    "        print(f\"\\n[{dataset}] Poem ID: {poem_id} ({len(poem_verses)} verses)\")\n",
    "        for _, row in poem_verses.iterrows():\n",
    "            verse_text = row.get('original_verse', row['verse'])\n",
    "            print(verse_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2581db-2ee2-4f56-8278-fe1a1756b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RESULTS_DIR = \"full_semantic_results_test\"\n",
    "df_cross = pd.read_csv(f\"{RESULTS_DIR}/cross_dataset_verses_only.csv\")\n",
    "\n",
    "# Cluster IDs you want\n",
    "cluster_ids = [2452, 2602, 2311]\n",
    "\n",
    "# Filter rows for these clusters\n",
    "df_filtered = df_cross[df_cross['cluster_id'].isin(cluster_ids)]\n",
    "\n",
    "# Group by cluster and print\n",
    "for cid, cluster_data in df_filtered.groupby('cluster_id'):\n",
    "    print(f\"\\n=== Cluster {cid} ({len(cluster_data)} verses) ===\\n\")\n",
    "    \n",
    "    for _, row in cluster_data.iterrows():\n",
    "        print(f\"[{row['source_dataset']}] : {row['verse']}\")\n",
    "    \n",
    "    print(\"\\n\" + \".\"*50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce302ee-e71a-424d-99e0-0ebde1e2bb83",
   "metadata": {},
   "source": [
    "## 2.2 Poem level clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ba281-70cc-443d-9ad7-06523e8ab724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34538c48-fabc-4b4f-85a9-aeea3cbe8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "POEM-LEVEL CLUSTERING BASED ON VERSE CLUSTER JACCARD SIMILARITY\n",
      "================================================================================\n",
      "Input: full_semantic_results_test/concatenated_cross_dataset_clusters.csv\n",
      "Sample size: 15,000\n",
      "Jaccard threshold range: (0.01, 0.1, 13)\n",
      "Workers: 8\n",
      "Scratch dir: /scratch/gent/vo/000/gvo00042/vsc48660/poem_clustering_temp\n",
      "================================================================================\n",
      "\n",
      "Loading data from: full_semantic_results_test/concatenated_cross_dataset_clusters.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761598/1156764358.py:556: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Loaded 1,536,616 verses\n",
      "\n",
      "Reconstructing poems from verses...\n",
      "  âœ“ Total verses: 1,536,616\n",
      "  âœ“ Valid verses (cluster_id != -1): 1,536,616\n",
      "  âœ“ Reconstructed 174,646 poems\n",
      "  âœ“ Average unique clusters per poem: 8.1\n",
      "  âœ“ Poems by dataset:\n",
      "      dbbe: 12,615\n",
      "      papyri: 51,083\n",
      "      phi: 110,156\n",
      "      rhoby: 792\n",
      "\n",
      "================================================================================\n",
      "GRID SEARCH: JACCARD THRESHOLD\n",
      "================================================================================\n",
      "\n",
      "Stratified sampling of 15,000 poems...\n",
      "  âœ“ Sampled 15,000 poems\n",
      "\n",
      "Building cluster-to-poems inverted index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:00<00:00, 265990.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding cross-dataset candidate pairs (batched)...\n",
      "  âœ“ Loading cached candidate pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter grid:\n",
      "  Jaccard thresholds: 13 values from 0.01 to 0.10\n",
      "\n",
      "Running grid search with 8 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid search: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [05:51<00:00, 27.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Grid search complete in 379.9s\n",
      "âœ“ Results saved: full_semantic_results_cross_dataset_poems/poem_parameter_grid_search.csv\n",
      "\n",
      "Creating visualizations...\n",
      "âœ“ Visualization saved: full_semantic_results_cross_dataset_poems/poem_grid_search_comprehensive.png\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ SELECTED CONFIGURATION (HIGHEST QUALITY)\n",
      "================================================================================\n",
      "Jaccard threshold: 0.018\n",
      "Quality score: 0.516\n",
      "Cohesion: 0.320\n",
      "Cross-dataset clusters: 10.0\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING ALL POEMS (BATCHED, CROSS-DATASET ONLY)\n",
      "================================================================================\n",
      "Jaccard threshold: 0.018\n",
      "\n",
      "Building cluster-to-poems inverted index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174646/174646 [00:01<00:00, 134668.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding cross-dataset candidate pairs (batched)...\n",
      "  âœ“ Loading cached candidate pairs...\n",
      "\n",
      "Clustering 174,646 poems...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:07<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Performed 12,447 merges\n",
      "\n",
      "  âœ“ Total poem clusters: 162,199\n",
      "  âœ“ Cross-dataset clusters: 18\n",
      "  âœ“ Singleton poems: 162,181\n",
      "  âœ“ Avg cluster size: 1.08\n",
      "  âœ“ Max cluster size: 12428\n",
      "\n",
      "âœ“ Results saved: full_semantic_results_cross_dataset_poems/poems_clustered_by_verse_jaccard.csv\n",
      "\n",
      "================================================================================\n",
      "âœ… POEM-LEVEL CLUSTERING COMPLETE\n",
      "================================================================================\n",
      "Time: 7.2 minutes\n",
      "Cross-dataset poem clusters: 18\n",
      "Poems in cross-dataset clusters: 128,112\n",
      "All results saved to: full_semantic_results_cross_dataset_poems/\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "RESULTS_DIR = Path(\"full_semantic_results_test\")\n",
    "POEM_RESULTS_DIR = Path(\"full_semantic_results_cross_dataset_poems\")\n",
    "SCRATCH_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/poem_clustering_temp\")\n",
    "POEM_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "SCRATCH_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INPUT_CSV = RESULTS_DIR / \"concatenated_cross_dataset_clusters.csv\"\n",
    "\n",
    "SAMPLE_SIZE = 15000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "JACCARD_THRESHOLD_RANGE = (0.01, 0.1, 13)\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POEM-LEVEL CLUSTERING BASED ON VERSE CLUSTER JACCARD SIMILARITY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Input: {INPUT_CSV}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE:,}\")\n",
    "print(f\"Jaccard threshold range: {JACCARD_THRESHOLD_RANGE}\")\n",
    "print(f\"Workers: {MAX_WORKERS}\")\n",
    "print(f\"Scratch dir: {SCRATCH_DIR}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "@njit\n",
    "def compute_jaccard_similarity(a_clusters, b_clusters):\n",
    "    a_set = set(a_clusters)\n",
    "    b_set = set(b_clusters)\n",
    "    \n",
    "    intersection = len(a_set & b_set)\n",
    "    union = len(a_set | b_set)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "class FastUnionFind:\n",
    "    __slots__ = ['parent', 'rank']\n",
    "    \n",
    "    def __init__(self, elements):\n",
    "        self.parent = {e: e for e in elements}\n",
    "        self.rank = {e: 0 for e in elements}\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        px, py = self.find(x), self.find(y)\n",
    "        if px == py:\n",
    "            return False\n",
    "        if self.rank[px] < self.rank[py]:\n",
    "            px, py = py, px\n",
    "        self.parent[py] = px\n",
    "        if self.rank[px] == self.rank[py]:\n",
    "            self.rank[px] += 1\n",
    "        return True\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        clusters = defaultdict(set)\n",
    "        for elem in self.parent.keys():\n",
    "            clusters[self.find(elem)].add(elem)\n",
    "        return dict(clusters)\n",
    "\n",
    "def reconstruct_poems_from_verses(df):\n",
    "    print(\"\\nReconstructing poems from verses...\")\n",
    "    \n",
    "    required_cols = ['idoriginal_poem', 'cluster_id', 'source_dataset']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"ERROR: '{col}' column not found in input CSV.\")\n",
    "    \n",
    "    valid_mask = df['cluster_id'] != -1\n",
    "    df_valid = df[valid_mask].copy()\n",
    "    \n",
    "    print(f\"  âœ“ Total verses: {len(df):,}\")\n",
    "    print(f\"  âœ“ Valid verses (cluster_id != -1): {len(df_valid):,}\")\n",
    "    \n",
    "    df_valid['poem_composite_id'] = df_valid['idoriginal_poem'].astype(str) + '___' + df_valid['source_dataset'].astype(str)\n",
    "    \n",
    "    if 'order' in df_valid.columns:\n",
    "        df_valid = df_valid.sort_values(['poem_composite_id', 'order'])\n",
    "    else:\n",
    "        df_valid = df_valid.sort_values('poem_composite_id')\n",
    "\n",
    "    poem_to_clusters = {}\n",
    "    poem_to_dataset = {}\n",
    "    poem_to_size = {}\n",
    "    \n",
    "    for composite_id, group in df_valid.groupby('poem_composite_id'):\n",
    "        cluster_sequence = group['cluster_id'].values\n",
    "        poem_to_clusters[composite_id] = np.unique(cluster_sequence).astype(np.int32)\n",
    "        poem_to_dataset[composite_id] = group['source_dataset'].iloc[0]\n",
    "        poem_to_size[composite_id] = len(poem_to_clusters[composite_id])\n",
    "    \n",
    "    print(f\"  âœ“ Reconstructed {len(poem_to_clusters):,} poems\")\n",
    "    print(f\"  âœ“ Average unique clusters per poem: {np.mean(list(poem_to_size.values())):.1f}\")\n",
    "    \n",
    "    dataset_counts = defaultdict(int)\n",
    "    for dataset in poem_to_dataset.values():\n",
    "        dataset_counts[dataset] += 1\n",
    "    \n",
    "    print(f\"  âœ“ Poems by dataset:\")\n",
    "    for dataset, count in sorted(dataset_counts.items()):\n",
    "        print(f\"      {dataset}: {count:,}\")\n",
    "    \n",
    "    poems_by_dataset = defaultdict(list)\n",
    "    for poem_id, dataset in poem_to_dataset.items():\n",
    "        poems_by_dataset[dataset].append(poem_id)\n",
    "    \n",
    "    poem_metadata = {\n",
    "        'poem_to_size': poem_to_size,\n",
    "        'poems_by_dataset': dict(poems_by_dataset)\n",
    "    }\n",
    "    \n",
    "    return poem_to_clusters, poem_to_dataset, poem_metadata\n",
    "\n",
    "def build_cluster_to_poems_index(poem_to_clusters):\n",
    "    print(\"\\nBuilding cluster-to-poems inverted index...\")\n",
    "    cluster_to_poems = defaultdict(list)\n",
    "    for poem_id, cluster_array in tqdm(poem_to_clusters.items(), desc=\"Indexing\"):\n",
    "        for cluster_id in cluster_array:\n",
    "            cluster_to_poems[int(cluster_id)].append(poem_id)\n",
    "    \n",
    "    for cluster_id in cluster_to_poems:\n",
    "        cluster_to_poems[cluster_id] = np.array(cluster_to_poems[cluster_id])\n",
    "    \n",
    "    return dict(cluster_to_poems)\n",
    "\n",
    "def stratified_sample_poems(poem_to_clusters, poem_to_dataset, poem_to_size, n_sample=10000):\n",
    "    print(f\"\\nStratified sampling of {n_sample:,} poems...\")\n",
    "    \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    poem_metadata = []\n",
    "    for poem_id in poem_to_clusters.keys():\n",
    "        metadata = {\n",
    "            'poem_id': poem_id,\n",
    "            'n_verses': poem_to_size[poem_id],\n",
    "            'source': poem_to_dataset[poem_id]\n",
    "        }\n",
    "        poem_metadata.append(metadata)\n",
    "    \n",
    "    poem_df = pd.DataFrame(poem_metadata)\n",
    "    \n",
    "    poem_df['size_bin'] = pd.cut(poem_df['n_verses'], \n",
    "                                  bins=[0, 5, 10, 20, 50, np.inf],\n",
    "                                  labels=['tiny', 'small', 'medium', 'large', 'huge'])\n",
    "    \n",
    "    sample_indices = []\n",
    "    \n",
    "    for (source, size_bin), group in poem_df.groupby(['source', 'size_bin']):\n",
    "        n_in_group = len(group)\n",
    "        proportion = n_in_group / len(poem_df)\n",
    "        n_sample_group = max(1, int(n_sample * proportion))\n",
    "        n_sample_group = min(n_sample_group, n_in_group)\n",
    "        \n",
    "        sampled = group.sample(n=n_sample_group, random_state=RANDOM_SEED)\n",
    "        sample_indices.extend(sampled['poem_id'].tolist())\n",
    "    \n",
    "    if len(sample_indices) < n_sample:\n",
    "        remaining = n_sample - len(sample_indices)\n",
    "        available = set(poem_to_clusters.keys()) - set(sample_indices)\n",
    "        if available:\n",
    "            additional = np.random.choice(list(available), \n",
    "                                         size=min(remaining, len(available)), \n",
    "                                         replace=False)\n",
    "            sample_indices.extend(additional)\n",
    "    \n",
    "    sample_indices = sample_indices[:n_sample]\n",
    "    print(f\"  âœ“ Sampled {len(sample_indices):,} poems\")\n",
    "    \n",
    "    return sample_indices\n",
    "\n",
    "def find_cross_dataset_candidate_pairs_batched(poem_to_clusters, poem_to_dataset, cluster_to_poems, poems_by_dataset):\n",
    "    print(\"\\nFinding cross-dataset candidate pairs (batched)...\")\n",
    "    \n",
    "    datasets = list(poems_by_dataset.keys())\n",
    "    pair_file = SCRATCH_DIR / \"candidate_pairs.npz\"\n",
    "    \n",
    "    if pair_file.exists():\n",
    "        print(\"  âœ“ Loading cached candidate pairs...\")\n",
    "        data = np.load(pair_file)\n",
    "        return set(zip(data['p1'], data['p2']))\n",
    "    \n",
    "    all_pairs = set()\n",
    "    \n",
    "    for i, dataset1 in enumerate(datasets):\n",
    "        for dataset2 in datasets[i+1:]:\n",
    "            print(f\"  Processing {dataset1} Ã— {dataset2}...\")\n",
    "            poems1 = poems_by_dataset[dataset1]\n",
    "            poems2 = poems_by_dataset[dataset2]\n",
    "            poems2_set = set(poems2)\n",
    "            \n",
    "            batch_pairs = set()\n",
    "            \n",
    "            for poem_id in tqdm(poems1, desc=f\"  {dataset1}\"):\n",
    "                clusters = poem_to_clusters[poem_id]\n",
    "                \n",
    "                candidates = set()\n",
    "                for cluster_id in clusters:\n",
    "                    if int(cluster_id) in cluster_to_poems:\n",
    "                        candidates.update(cluster_to_poems[int(cluster_id)])\n",
    "                \n",
    "                candidates = candidates & poems2_set\n",
    "                \n",
    "                for other_poem in candidates:\n",
    "                    pair = tuple(sorted([poem_id, other_poem]))\n",
    "                    batch_pairs.add(pair)\n",
    "            \n",
    "            all_pairs.update(batch_pairs)\n",
    "            print(f\"    Found {len(batch_pairs):,} pairs\")\n",
    "    \n",
    "    print(f\"  âœ“ Total candidate pairs: {len(all_pairs):,}\")\n",
    "    \n",
    "    if all_pairs:\n",
    "        p1_list, p2_list = zip(*all_pairs)\n",
    "        np.savez_compressed(pair_file, p1=p1_list, p2=p2_list)\n",
    "        print(f\"  âœ“ Cached pairs to {pair_file}\")\n",
    "    \n",
    "    return all_pairs\n",
    "\n",
    "def compute_cluster_cohesion(poem_to_clusters, cluster_assignments, max_sample=500):\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    cohesions = []\n",
    "    \n",
    "    clusters = defaultdict(list)\n",
    "    for poem_id in poem_ids:\n",
    "        cluster_id = cluster_assignments.get(poem_id)\n",
    "        if cluster_id is not None:\n",
    "            clusters[cluster_id].append(poem_id)\n",
    "    \n",
    "    for cluster_id, cluster_poems in clusters.items():\n",
    "        if len(cluster_poems) < 2:\n",
    "            continue\n",
    "        \n",
    "        if len(cluster_poems) > 30:\n",
    "            sampled = np.random.choice(cluster_poems, 30, replace=False)\n",
    "        else:\n",
    "            sampled = cluster_poems\n",
    "        \n",
    "        overlaps = []\n",
    "        for i in range(len(sampled)):\n",
    "            for j in range(i+1, min(i+10, len(sampled))):\n",
    "                jaccard = compute_jaccard_similarity(\n",
    "                    poem_to_clusters[sampled[i]], \n",
    "                    poem_to_clusters[sampled[j]]\n",
    "                )\n",
    "                overlaps.append(jaccard)\n",
    "        \n",
    "        if overlaps:\n",
    "            cohesions.append(np.mean(overlaps))\n",
    "        \n",
    "        if len(cohesions) >= max_sample:\n",
    "            break\n",
    "    \n",
    "    return np.mean(cohesions) if cohesions else 0.0\n",
    "\n",
    "def evaluate_single_config(args):\n",
    "    jaccard_thresh, sample_poems, poem_to_clusters, candidate_pairs, poem_to_dataset = args\n",
    "    \n",
    "    try:\n",
    "        sample_set = set(sample_poems)\n",
    "        cross_dataset_pairs = set()\n",
    "        \n",
    "        for p1, p2 in candidate_pairs:\n",
    "            if p1 in sample_set and p2 in sample_set:\n",
    "                cross_dataset_pairs.add((p1, p2))\n",
    "        \n",
    "        uf = FastUnionFind(sample_poems)\n",
    "        merges = 0\n",
    "        \n",
    "        for p1, p2 in cross_dataset_pairs:\n",
    "            jaccard = compute_jaccard_similarity(\n",
    "                poem_to_clusters[p1], \n",
    "                poem_to_clusters[p2]\n",
    "            )\n",
    "            \n",
    "            if jaccard >= jaccard_thresh:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "        \n",
    "        poem_clusters = uf.get_clusters()\n",
    "        cluster_assignments = {}\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            for poem in poems:\n",
    "                cluster_assignments[poem] = cluster_id\n",
    "        \n",
    "        n_clusters = len(poem_clusters)\n",
    "        cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "        n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "        avg_size = np.mean(cluster_sizes)\n",
    "        max_size = max(cluster_sizes) if cluster_sizes else 0\n",
    "        \n",
    "        n_cross_dataset_clusters = 0\n",
    "        for cluster_id, poems in poem_clusters.items():\n",
    "            datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "            if len(datasets) > 1:\n",
    "                n_cross_dataset_clusters += 1\n",
    "        \n",
    "        cohesion = compute_cluster_cohesion(poem_to_clusters, cluster_assignments)\n",
    "        \n",
    "        return {\n",
    "            'jaccard_threshold': jaccard_thresh,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singletons': n_singletons,\n",
    "            'n_cross_dataset_clusters': n_cross_dataset_clusters,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'max_cluster_size': max_size,\n",
    "            'cohesion': cohesion,\n",
    "            'merges': merges,\n",
    "            'n_cross_dataset_pairs': len(cross_dataset_pairs)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error at jaccard={jaccard_thresh:.2f}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_parameters(poem_to_clusters, poem_to_dataset, poem_to_size, poems_by_dataset):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GRID SEARCH: JACCARD THRESHOLD\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    sample_poems = stratified_sample_poems(poem_to_clusters, poem_to_dataset, \n",
    "                                          poem_to_size, SAMPLE_SIZE)\n",
    "    \n",
    "    sample_poems_set = set(sample_poems)\n",
    "    sample_poems_by_dataset = defaultdict(list)\n",
    "    for poem_id in sample_poems:\n",
    "        sample_poems_by_dataset[poem_to_dataset[poem_id]].append(poem_id)\n",
    "    \n",
    "    cluster_to_poems = build_cluster_to_poems_index({p: poem_to_clusters[p] for p in sample_poems})\n",
    "    \n",
    "    candidate_pairs = find_cross_dataset_candidate_pairs_batched(\n",
    "        {p: poem_to_clusters[p] for p in sample_poems},\n",
    "        poem_to_dataset,\n",
    "        cluster_to_poems,\n",
    "        sample_poems_by_dataset\n",
    "    )\n",
    "\n",
    "    jaccard_thresholds = np.linspace(JACCARD_THRESHOLD_RANGE[0], \n",
    "                                     JACCARD_THRESHOLD_RANGE[1], \n",
    "                                     int(JACCARD_THRESHOLD_RANGE[2]))\n",
    "    \n",
    "    print(f\"\\nParameter grid:\")\n",
    "    print(f\"  Jaccard thresholds: {len(jaccard_thresholds)} values from {jaccard_thresholds[0]:.2f} to {jaccard_thresholds[-1]:.2f}\")\n",
    "\n",
    "    args_list = []\n",
    "    for jaccard_thresh in jaccard_thresholds:\n",
    "        args_list.append((\n",
    "            jaccard_thresh, sample_poems, poem_to_clusters,\n",
    "            candidate_pairs, poem_to_dataset\n",
    "        ))\n",
    "\n",
    "    print(f\"\\nRunning grid search with {MAX_WORKERS} workers...\")\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(evaluate_single_config, args): args for args in args_list}\n",
    "        with tqdm(total=len(futures), desc=\"Grid search\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"âœ“ Grid search complete in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    def normalize(series):\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        if max_val - min_val < 1e-10:\n",
    "            return pd.Series(0.5, index=series.index)\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "    cohesion_score = normalize(results_df['cohesion'])\n",
    "    \n",
    "    cross_dataset_ratio = results_df['n_cross_dataset_clusters'] / (results_df['n_clusters'] + 1e-10)\n",
    "    cross_dataset_score = normalize(cross_dataset_ratio)\n",
    "    \n",
    "    singleton_ratio = results_df['n_singletons'] / len(sample_poems)\n",
    "    balance_score = np.clip(1 - singleton_ratio, 0, 1)\n",
    "\n",
    "    results_df['quality_score'] = (\n",
    "        cohesion_score * 0.50 +\n",
    "        cross_dataset_score * 0.30 +\n",
    "        balance_score * 0.20\n",
    "    )\n",
    "\n",
    "    results_df = results_df.sort_values('quality_score', ascending=False)\n",
    "    results_csv = POEM_RESULTS_DIR / 'poem_parameter_grid_search.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"âœ“ Results saved: {results_csv}\")\n",
    "\n",
    "    create_grid_visualizations(results_df)\n",
    "\n",
    "    best = results_df.iloc[0]\n",
    "    best_jaccard = float(best['jaccard_threshold'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¯ SELECTED CONFIGURATION (HIGHEST QUALITY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Jaccard threshold: {best_jaccard:.3f}\")\n",
    "    print(f\"Quality score: {best['quality_score']:.3f}\")\n",
    "    print(f\"Cohesion: {best['cohesion']:.3f}\")\n",
    "    print(f\"Cross-dataset clusters: {best['n_cross_dataset_clusters']}\")\n",
    "\n",
    "    return best_jaccard, results_df\n",
    "\n",
    "def create_grid_visualizations(results_df):\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    sns.set_palette(\"colorblind\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Poem-Level Cross-Dataset Clustering: Grid Search', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    best = results_df.iloc[0]\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['quality_score'], 'o-', linewidth=2, markersize=6)\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2, label='Best')\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Quality Score', fontweight='bold')\n",
    "    ax.set_title('Quality vs Jaccard', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['cohesion'], 'o-', linewidth=2, markersize=6, color='orange')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Cohesion', fontweight='bold')\n",
    "    ax.set_title('Cohesion vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['n_cross_dataset_clusters'], 'o-', linewidth=2, markersize=6, color='green')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_title('Cross-Dataset Clusters vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(results_df['jaccard_threshold'], results_df['n_clusters'], 'o-', linewidth=2, markersize=6, color='purple')\n",
    "    ax.axvline(best['jaccard_threshold'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Jaccard Threshold', fontweight='bold')\n",
    "    ax.set_ylabel('Total Clusters', fontweight='bold')\n",
    "    ax.set_title('Total Clusters vs Jaccard', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(results_df['cohesion'], results_df['n_cross_dataset_clusters'],\n",
    "              c=results_df['quality_score'], cmap='RdYlGn', s=100, edgecolors='black')\n",
    "    ax.scatter(best['cohesion'], best['n_cross_dataset_clusters'],\n",
    "              color='red', s=300, marker='*', edgecolors='black', linewidth=2, zorder=10)\n",
    "    ax.set_xlabel('Cohesion', fontweight='bold')\n",
    "    ax.set_ylabel('Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_title('Cohesion vs Cross-Dataset', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 2]\n",
    "    ax.hist(results_df['quality_score'], bins=15, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(best['quality_score'], color='red', linestyle='--', linewidth=2, label=f\"Best: {best['quality_score']:.3f}\")\n",
    "    ax.set_xlabel('Quality Score', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title('Score Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = POEM_RESULTS_DIR / 'poem_grid_search_comprehensive.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Visualization saved: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def cluster_all_poems_batched(poem_to_clusters, poem_to_dataset, poems_by_dataset, jaccard_threshold):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLUSTERING ALL POEMS (BATCHED, CROSS-DATASET ONLY)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Jaccard threshold: {jaccard_threshold:.3f}\")\n",
    "    \n",
    "    cluster_to_poems = build_cluster_to_poems_index(poem_to_clusters)\n",
    "    \n",
    "    candidate_pairs = find_cross_dataset_candidate_pairs_batched(\n",
    "        poem_to_clusters, poem_to_dataset, cluster_to_poems, poems_by_dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nClustering {len(poem_to_clusters):,} poems...\")\n",
    "    poem_ids = list(poem_to_clusters.keys())\n",
    "    uf = FastUnionFind(poem_ids)\n",
    "    \n",
    "    merges = 0\n",
    "    batches = [list(candidate_pairs)[i:i+BATCH_SIZE] for i in range(0, len(candidate_pairs), BATCH_SIZE)]\n",
    "    \n",
    "    for batch in tqdm(batches, desc=\"Processing batches\"):\n",
    "        for p1, p2 in batch:\n",
    "            jaccard = compute_jaccard_similarity(\n",
    "                poem_to_clusters[p1], \n",
    "                poem_to_clusters[p2]\n",
    "            )\n",
    "            \n",
    "            if jaccard >= jaccard_threshold:\n",
    "                if uf.union(p1, p2):\n",
    "                    merges += 1\n",
    "    \n",
    "    print(f\"  âœ“ Performed {merges:,} merges\")\n",
    "    \n",
    "    poem_clusters = uf.get_clusters()\n",
    "    cluster_assignments = {}\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        for poem in poems:\n",
    "            cluster_assignments[poem] = cluster_id\n",
    "    \n",
    "    n_clusters = len(poem_clusters)\n",
    "    cluster_sizes = [len(poems) for poems in poem_clusters.values()]\n",
    "    n_singletons = sum(1 for size in cluster_sizes if size == 1)\n",
    "    \n",
    "    n_cross_dataset_clusters = 0\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "        if len(datasets) > 1:\n",
    "            n_cross_dataset_clusters += 1\n",
    "    \n",
    "    print(f\"\\n  âœ“ Total poem clusters: {n_clusters:,}\")\n",
    "    print(f\"  âœ“ Cross-dataset clusters: {n_cross_dataset_clusters:,}\")\n",
    "    print(f\"  âœ“ Singleton poems: {n_singletons:,}\")\n",
    "    print(f\"  âœ“ Avg cluster size: {np.mean(cluster_sizes):.2f}\")\n",
    "    print(f\"  âœ“ Max cluster size: {max(cluster_sizes)}\")\n",
    "    \n",
    "    return cluster_assignments, poem_clusters\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nLoading data from: {INPUT_CSV}\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    \n",
    "    print(f\"  âœ“ Loaded {len(df):,} verses\")\n",
    "    \n",
    "    poem_to_clusters, poem_to_dataset, poem_metadata = reconstruct_poems_from_verses(df)\n",
    "    poem_to_size = poem_metadata['poem_to_size']\n",
    "    poems_by_dataset = poem_metadata['poems_by_dataset']\n",
    "    \n",
    "    best_jaccard, grid_results = grid_search_parameters(\n",
    "        poem_to_clusters, poem_to_dataset, poem_to_size, poems_by_dataset\n",
    "    )\n",
    "    \n",
    "    del grid_results\n",
    "    gc.collect()\n",
    "    \n",
    "    cluster_assignments, poem_clusters = cluster_all_poems_batched(\n",
    "        poem_to_clusters, poem_to_dataset, poems_by_dataset, best_jaccard\n",
    "    )\n",
    "    \n",
    "    df['poem_composite_id'] = df['idoriginal_poem'].astype(str) + '___' + df['source_dataset'].astype(str)\n",
    "    df['poem_cluster_id'] = df['poem_composite_id'].map(cluster_assignments)\n",
    "    \n",
    "    cross_dataset_cluster_ids = set()\n",
    "    for cluster_id, poems in poem_clusters.items():\n",
    "        datasets = set(poem_to_dataset.get(p) for p in poems)\n",
    "        if len(datasets) > 1:\n",
    "            cross_dataset_cluster_ids.add(cluster_id)\n",
    "    \n",
    "    df['is_cross_dataset_poem_cluster'] = df['poem_cluster_id'].isin(cross_dataset_cluster_ids)\n",
    "    \n",
    "    output_csv = POEM_RESULTS_DIR / 'poems_clustered_by_verse_jaccard.csv'\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nâœ“ Results saved: {output_csv}\")\n",
    "    \n",
    "    summary = {\n",
    "        'n_verses': len(df),\n",
    "        'n_poems': len(poem_to_clusters),\n",
    "        'n_datasets': len(set(poem_to_dataset.values())),\n",
    "        'best_jaccard_threshold': best_jaccard,\n",
    "        'n_poem_clusters': len(set(cluster_assignments.values())),\n",
    "        'n_cross_dataset_clusters': len(cross_dataset_cluster_ids),\n",
    "        'n_poems_in_cross_dataset_clusters': sum(df['is_cross_dataset_poem_cluster']),\n",
    "        'processing_time_minutes': (time.time() - start_time) / 60\n",
    "    }\n",
    "    \n",
    "    pd.DataFrame([summary]).to_csv(POEM_RESULTS_DIR / 'poem_clustering_summary.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… POEM-LEVEL CLUSTERING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Time: {summary['processing_time_minutes']:.1f} minutes\")\n",
    "    print(f\"Cross-dataset poem clusters: {summary['n_cross_dataset_clusters']:,}\")\n",
    "    print(f\"Poems in cross-dataset clusters: {summary['n_poems_in_cross_dataset_clusters']:,}\")\n",
    "    print(f\"All results saved to: {POEM_RESULTS_DIR}/\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aacf471-8edf-40d6-a996-f603c4d71980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv('full_semantic_results/poem_parameter_grid_search.csv')\n",
    "\n",
    "# If there's only one unique min_shared value, we can plot quality vs threshold\n",
    "unique_min_shared = df['min_shared'].unique()\n",
    "if len(unique_min_shared) > 1:\n",
    "    print(\"Multiple min_shared values detected, consider using a heatmap instead.\")\n",
    "else:\n",
    "    # Sort by threshold for plotting\n",
    "    df = df.sort_values('threshold')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='threshold',\n",
    "        y='quality_score',\n",
    "        marker='o',\n",
    "        linewidth=2,\n",
    "        markersize=8\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Quality Score vs Threshold (min_shared={unique_min_shared[0]})',\n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Cosine Similarity Threshold', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Quality Score', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Optionally save the figure\n",
    "    plt.savefig('full_semantic_results/poem_quality_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e75755-ae60-46c8-9bf4-9607a6b16d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3761598/2819930227.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('full_semantic_results_cross_dataset_poems/poems_clustered_by_verse_jaccard.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 113143___phi | Poems: 2\n",
      "  Poem 113143 (1 verses):\n",
      "    ÎºÎ±ÏÎ¿Î¼ÎµÎ¼Ï†Î¹Ï„Î·Ï‚ Â²â·ÎµÎ»Î»Î·Î½Î¿Î¼ÎµÎ¼Ï†Î¹Ï„Î·Ï‚Â²â·  [source: phi]\n",
      "  Poem 116832 (1 verses):\n",
      "    Î¿Î½Î¹ÎºÎ·Ï†Î¿ÏÎ±Î± Î¼Î·Î½Î¹Î¼ÎµÏƒÎ¿ÏÎ·ÎºÎ²Î¹Î½Î´Î¹ÎºÏ„Î¹Î¿Î½Î¿Ï‚Î·  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 116771___papyri | Poems: 2\n",
      "  Poem 900065 (1 verses):\n",
      "    Î¸ÎµÏ…Î´Ï‰ÏÎ· ÎµÏ…Ï†Î¹Î»Î·Ï„Î¿Ï…  [source: phi]\n",
      "  Poem 116771 (1 verses):\n",
      "    Î»ÎµÎ¼Î±Î¹Ï‰Ï…Ï€ÎµÏÏ„ÏÏ…Î³  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 193635___phi | Poems: 2\n",
      "  Poem 193635 (2 verses):\n",
      "    tranquilla per annos  [source: phi]\n",
      "    aemulaque in cunctis  [source: phi]\n",
      "  Poem 73252 (3 verses):\n",
      "    olius ca  [source: papyri]\n",
      "    abban agens midpunctus d ca  [source: papyri]\n",
      "    donatus cu ca  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 23372___dbbe | Poems: 2\n",
      "  Poem 23372 (1 verses):\n",
      "    Î·Ï„Î± Î´ Î±Î¹Î±Ï‚ Ï€Î¿Î»ÎµÎ¼Î¹Î¶Îµ Î¼Î¿Î½Ï‰Î¹ Î¼Î¿Î½Î¿Ï‚ ÎµÎºÏ„Î¿ÏÎ¹ Î´Î¹Ï‰Î¹  [source: dbbe]\n",
      "  Poem 36235 (7 verses):\n",
      "    Ï‚ ÎµÎ½Î¿Î¼Î¹Î¶Î¿Î½ Î³Î±Ï Î¿Ï„Î¹ Î¿Ï…Îº Î±Î½Î·Î»Î¸ÎµÎ½ ÎµÎ¹Ï‚ Ï„Î·Î½ Î»Ï…ÎºÏ‰ Î±Î»Î»Î± ÎµÎ¼ÎµÎ¹Î½ÎµÎ½  [source: papyri]\n",
      "    ÎºÎ±Î¹ ÎµÎ¹Î´Ï‰Ï‚ Ï„Î·Î½ Ï€ÏÎ¿Î±Î¹ÏÎµÏƒÎ¹Î½ Ï„Ï‰Î½ Ï‡Ï‰ÏÎ¹ÎºÏ‰Î½ Ï‰Ï‚ ÎµÎ¸ÎµÎ»Î¿Ï…ÏƒÎ¹Î½ Ï€ÏÎ±Î¹Î´ÎµÏ…ÏƒÎ±Î¹ ÎµÎ¼ÎµÎ¹Î½Î± ÎµÎºÎµÎ¹  [source: papyri]\n",
      "    Î¸Î»Î¹Î²Î¿Î¼ÎµÎ½Î¿Ï‚ ÎµÏ‰Ï‚ Î¿Ï„Îµ ÎµÎ´ÎµÎ¾Î±Î¼Î·Î½ Ï€Î±ÏÊ¼ Î±Ï…Ï„Î·Ï‚ Î³ÏÎ±Î¼Î¼Î±Ï„Î± Ï‰Ï‚ ÎµÎ½ Ï„Î· Î»Ï…ÎºÏ‰ Î´Î¹Î±Î³ÎµÎ¹ Î¼Î· Î´Î¿Î¾Î· ÏƒÎ¿Î¹ Î¿Ï…Î½  [source: papyri]\n",
      "    Î±Ï€Î¿ÏƒÏ„Î·Î½Î±Î¹ Ï„Ï‰ ÎµÎ½Î´Î¿Î¾Î¿Ï„Î±Ï„Ï‰ Î±ÏÏ‡Î¿Î½Ï„Î¹ Î±Î»Î»Î± ÎµÎ±Î½ Î´Î¿Î¾Î· Î±Ï…Ï„Ï‰ Î±Î½ÎµÎ»Î¸ÎµÎ¹Î½ Î±Î½ÎµÎ»Î¸Îµ Î¼ÎµÏ„Ê¼ Î±Ï…Ï„Î¿Ï… Î¿Ï€ÎµÏ Î½Î¿Î¼Î¹Î¶Ï‰  [source: papyri]\n",
      "    Î¿Ï…Îº Î±Î½ÎµÏ‡ÎµÏ„Î±Î¹ Î±ÏÏ„Î¹ Î±Î½ÎµÎ»Î¸ÎµÎ¹Î½ ÎµÏ‰Ï‚ Î¿Ï„Îµ Î³ÎµÎ½Î·Ï„Î±Î¹ ÎºÎ±Ï„Î±ÏƒÏ„Î±ÏƒÎ¹Ï‚ Ï„ÎµÎ»ÎµÎ¹Î± Ï€ÎµÏÎ¹ Ï„Î¿Ï… Î³ÏÎ±Ï†ÎµÎ¹Î½ Ï…Î¼Î±Ï‚  [source: papyri]\n",
      "    Ï‰Ï‚ Î¿ Ï…Î¹Î¿Ï‚ Ï„Î¿Ï… ÎºÎ±Ï„Î± ÎºÎ¿Î»Î¿Ï„ÏƒÎµ Î±Î½Î·Î»Î¸ÎµÎ½ ÎµÏ‰Ï‚ Î¼Î¿Ï…Î½Î±ÎµÎ¹ ÎºÎ±Î¹ ÎµÏ€Î¿Î»ÎµÎ¼Î·Î¸Î· ÎµÎºÎµÎ¹ ÎºÎ±Î¹ Ï€Î¿Î»Î»Î¿Ï…Ï‚ ÎµÎº Ï„Ï‰Î½  [source: papyri]\n",
      "    Î±ÎºÎ¿Î»Î¿Ï…Î¸Î¿Ï…Î½Ï„Ï‰Î½ Î±Ï…Ï„Ï‰ ÎµÏƒÏ†Î±Î¾Î±Î½ ÎºÎ±Î¹ Ï…Ï€ÎµÏƒÏ„ÏÎµÏˆÎµÎ½ ÎµÎ¹Ï‚ Ï„Î·Î½ Î±Î½Ï„Î¹Î½Î¿Î¿Ï… ÎºÎ±Î¹ Î¿Ï…Îº ÎµÏ„Î¿Î»Î¼Î·ÏƒÎµÎ½ ÎµÏ„Î¹ Î±Î½ÎµÎ»Î¸ÎµÎ¹Î½  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 26685___dbbe | Poems: 2\n",
      "  Poem 26685 (1 verses):\n",
      "    Ï„Î·Î½ Ï‡ÎµÎ¹ÏÎ± Ï„ÎµÎ¹Î½ÎµÎ¹ Ï„Î·Î½ Ï€ÏÎ¹Î½ ÎµÎ¾Î·ÏÎ±Î¼Î¼ÎµÎ½Î·Î½  [source: dbbe]\n",
      "  Poem 764622 (3 verses):\n",
      "    Î²ÎµÎ½Î´Î¹Ï‚ Î±Ï€Î¿Î»Î»Î¿Î´Ï‰ÏÎ¿Ï…  [source: phi]\n",
      "    Ï„Ï‰ Ï„ÎµÎºÎ½Ï‰ Î±Ï€Î¿Î»Î»Î¿Î´Ï‰ÏÏ‰  [source: phi]\n",
      "    Î±Î½ÎµÎ¸Î·ÎºÎµÎ½ Ï‡ÎµÏÎµ Ï€Î±ÏÎ¿Î´ÎµÎ¹Ï„Î±  [source: phi]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 28147___papyri | Poems: 2\n",
      "  Poem 792314 (4 verses):\n",
      "    Î±Î¸Î·Î½Î±Î¹Î±Î¹ ÎµÏÎ³Î±Î½Î·Î¹ Ï€Î¿Î»Î¹Î±Î´Î¹ Î±Î½ÎµÎ¸Î·ÎºÎµ  [source: phi]\n",
      "    Î´ÎµÎ¹Î½Î¿Î¼ÎµÎ½Î·Ï‚ Î»Ï…ÎºÎ¹Î½Î¿ ÎºÎ±Î¹ Î· Î³Ï…Î½Î· ÎºÎ±Î¹ Î¿Î¹ Ï€Î±Î¹Î´ÎµÏ‚  [source: phi]\n",
      "    ÏƒÏ‰Î½ Î´ÎµÎºÎ±Ï„Î·Î½ Î´Ï‰ÏÏ‰Î½ Î¸ÎµÎ± ÎµÏÎ³Î±Î½Î· ÎµÏ…Î¾Î±Î¼ÎµÎ½Î¿Ï‚ ÏƒÎ¿Î¹  [source: phi]\n",
      "    ÎºÎ±Î»Î¿Î½ Î´ÎµÎ¹Î½Î¿Î¼ÎµÎ½Î·Ï‚ ÏƒÏ„Î·ÏƒÎµÎ½ Î±Î³Î±Î»Î¼Î± Ï„Î¿Î´Îµ  [source: phi]\n",
      "  Poem 28147 (3 verses):\n",
      "    Î²Î¹Î²Î»Î¹Î¿Ï†Ï…Î»Î±Î¾Î¹ Î´Î·Î¼Î¿ÏƒÎ¹Ï‰Î½ Î»Î¿Î³Ï‰Î½  [source: papyri]\n",
      "    Ï€Î±ÏÎ± Î¼Ï…ÏƒÎ¸Î·Ï‚ Ï„Î¿Ï… Î¼Ï…ÏƒÎ¸Î¿Ï…  [source: papyri]\n",
      "    ÎºÎ±Î¹ Ï„Ï‰Î½ Î»Î¿Î¹Ï€Ï‰Î½ ÎµÏ€Î¹Ï„Î·ÏÎ·  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 29140___papyri | Poems: 2\n",
      "  Poem 894650 (1 verses):\n",
      "    Â²great altar masons marksÂ² Ï€Î² Î²  [source: phi]\n",
      "  Poem 29140 (15 verses):\n",
      "    vac check  [source: papyri]\n",
      "    ca Ï… Î±Î½Î¿Ï…Ï€Î¿Ï…ÎµÎ¹ÏƒÎ¹Î¿Ï…  [source: papyri]\n",
      "    ca Î¿Ï…Î²Î¹Î´ Ï„Î¿Ï€Î¿Ï… Î¿Ï… Î±ÏÎ¿Ï…ÏÏ‰Î½ Î² Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Ï‰Î½ Î²  [source: papyri]\n",
      "    hand 1 ca Î½ÎµÎ¹Î»Î¿Ï… Ï„Î¿Ï… ÎºÎ±Î¹ ÎºÎµÏ†Î±Î» ÏƒÏ‰Ï„Î¿Ï…  [source: papyri]\n",
      "    ca Î±Î´ Ï„Î¿Ï€Î¿Ï… Î²Ï‰Î¼Î¿Ï… Î±ÏÎ¿Ï…ÏÏ‰Î½ Îµ  [source: papyri]\n",
      "    hand 2 ca ÎµÏ€ÎµÏƒÎºÎµÏ†Î¸Î·ÏƒÎ±Î½ ÎµÎ½ Î±Î²ÏÎ¿Ï‡Ï‰ hand 1 Ï†Î¿ÏÎ¿Ï… Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Ï‰Î½ Îµ  [source: papyri]\n",
      "    hand 1 ca Î±Î½Ï„ÎµÎ¹Ï†Ï‰Î½Î¿Ï‚ vac  [source: papyri]\n",
      "    ca Î¿ Î¾Ï…Î»Î¹Ï„Î¹Î´Î¿Ï‚ Î±ÏÎ¿Ï…ÏÎ±Ï‚ Î± Î´ Î· Î»Î²  [source: papyri]\n",
      "    hand 2 ca ÎµÏ€ÎµÏƒÎºÎµÏ†Î¸Î· ÎµÎ½ Î±Î²ÏÎ¿Ï‡Ï‰ hand 1 Ï†Î¿ÏÎ¿Ï… Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Î·Ï‚ Î± Î³ Î¹Î²  [source: papyri]\n",
      "    hand 1 ÏƒÎ±Ï„Î±Î²Î¿Ï…Ï‚ ÏƒÏ„Î¿Ï„Î¿Î·Ï„ÎµÏ‰Ï‚ vac  [source: papyri]\n",
      "    ca Î·Î¾Î¿ Ï„Î¿Ï€Î¿Ï… ÏƒÏ‰Î»Î½Î·Ï‚ Î±ÏÎ¿Ï…ÏÎ±Ï‚ Î± ð…µ Î· Î»Î²  [source: papyri]\n",
      "    hand 2 ÎµÏ€ÎµÏƒÎºÎµÏ†Î¸Î· ÎµÎ½ Î±Î²ÏÎ¿Ï‡Ï‰ hand 1 Ï†Î¿ÏÎ¿Ï… Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Î·Ï‚ Î± ð…·ÎºÎ´  [source: papyri]\n",
      "    hand 1 Î¿Ï…Î±Î»ÎµÎ½Ï„Î¿Ï‚ Ï„Î¿Ï… Î½ÎµÎ¼ÎµÏƒÎ¹Ï‰Î½Î¿Ï‚  [source: papyri]\n",
      "    ca Î´Ï Ï„Î¿Ï€Î¿Ï… ÎºÎ±Î¹Î½Î·Ï‚ Î´Î¹Ï‰ÏÏ…Î³Î¿Ï‚ Î±ÏÎ¿Ï…ÏÎ±Ï‚ Î± ð…µ Î»Î²  [source: papyri]\n",
      "    ca hand 1 Î±ÏÎ¿Ï…ÏÎ±Ï‚ Î± Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Ï‰Î½ Î·  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 31601___papyri | Poems: 2\n",
      "  Poem 905935 (2 verses):\n",
      "    nan  [source: phi]\n",
      "    Î¹ÏƒÏ€Î±Î½  [source: phi]\n",
      "  Poem 31601 (1 verses):\n",
      "    Ï„Î¿Ï€Ï…ÏÎ¿Ï… Î±ÏÏ„Î±Î²Î±Ï‚  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 32234___papyri | Poems: 2\n",
      "  Poem 923373 (1 verses):\n",
      "    ÏƒÎºÎµ Â²â·ÎºÎ±Ï„ÎµÏƒÎºÎµÏ…Î±ÏƒÂ²â·  [source: phi]\n",
      "  Poem 32234 (6 verses):\n",
      "    ca Î·Ï„Î·Ï‚ ÏƒÎ±ÏÎ± ca  [source: papyri]\n",
      "    ca Ï‡Ï…Î±Ï‡ Ï„Î·  [source: papyri]\n",
      "    ca ÎºÎ±Î¸Ï‰Ï‚ ÎµÎ½ÎµÏƒÏ„Î¹  [source: papyri]\n",
      "    ca ÎºÎ¿Ï…Ï†Ï‰Î½  [source: papyri]\n",
      "    ca Î¹ Ï†ÏÎ¿Î½Ï„Î¹ÏƒÎ±Î¹  [source: papyri]\n",
      "    ca Î·Ï‚ ÎµÎ±Î½ ÎµÏ…  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 33214___papyri | Poems: 2\n",
      "  Poem 912516 (1 verses):\n",
      "    Î¹Ï‚ Â²â¶ÎµÎ¹Ï‚Â²â¶ Î¸ÎµÎ¿Ï‚ Î¿ Î²Î¿Î·Î¸Ï‰Î½ c  [source: phi]\n",
      "  Poem 33214 (4 verses):\n",
      "    ÎµÎ¾ Ï‰Î½ Î´ÎµÏ…ÏÎµÎ¹ ca  [source: papyri]\n",
      "    Ï‰Î½ Î¿Î¼Î¿Î»Î¿Î³Î·ÏƒÎ± ca  [source: papyri]\n",
      "    Î¸ÎµÎ¿Î´Î¿Ï„Î¿Ï‚ ÎµÎ¾ ca  [source: papyri]\n",
      "    Î·ÏÎ±ÎºÎ»ÎµÏ‰Î½Î¿Ï‚ ca  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 33829___papyri | Poems: 2\n",
      "  Poem 705326 (4 verses):\n",
      "    Â²altar with tanit symbolÂ²  [source: phi]\n",
      "    Â²altar with flame flanked by two treesÂ²  [source: phi]\n",
      "    Â²vessel withÂ² Î» Â²in the centerÂ²  [source: phi]\n",
      "    Â²fivepointed starÂ²  [source: phi]\n",
      "  Poem 33829 (11 verses):\n",
      "    ca Î¶ Î±Î¼Î²Î»Î±Î³Î±ÏÏ ca  [source: papyri]\n",
      "    ca ÎºÎµÏÎ±Ï„Î¹Î± Î»Î± Î³Î¹Î½Î¿Î½Ï„Î±Î¹ ca  [source: papyri]\n",
      "    ca Î³Î¿Î¸apostropheÎ¸Î¿Î¹Ï‚ Ï… ca  [source: papyri]\n",
      "    ca Î±Ï€Î¿ÏƒÏ„Î¿Î»Î¹Ï‰ ÎºÎµÏÎ¼Î±Ï„Î¿Ï‚ Îº  [source: papyri]\n",
      "    ca Î´ ÎºÎµÏÎ¼Î±Ï„Î¿Ï‚ ÎºÎµÏÎ±Ï„Î¹Î± Ï›  [source: papyri]\n",
      "    ca Ï…Î¹Î¿Ï‚ Ï…Ï€ÎµÏ Î³Î¿Î¸Î¸Ï‰Î½ Î¸ v  [source: papyri]\n",
      "    ca Ï‰Î½ ÎµÎ¹Ï‚ Ï„Î·Î¼ Ï€Î¿Î»Îµ ca  [source: papyri]\n",
      "    ca Î±Î½Î½Î¿Ï… Ï„Î·Ï‚ Î±Ï…Ï„Î·Ï‚ Îµ ca  [source: papyri]\n",
      "    ca Ï‰Î½Î¿Ï‚ Ï…Ï€Î¿Î´ÎµÎºÏ„ ca  [source: papyri]\n",
      "    ca Ï„Î·Ï‚ Î±Ï…Ï„Î·Ï‚ Îµ Î¹Î½Î´Î¹ÎºÏ„Î¹Î¿Î½Î¿Ï‚ Î½Î¿Î¼Î¹ÏƒÎ¼Î±Ï„Î¹Î¿Î½ Î± Ï›  [source: papyri]\n",
      "    ca Ï‚ Î¼ÎµÏƒÎ¿ÏÎ· Î» Îµ Î¹Î½Î´Î¹ÎºÏ„Î¹Î¿Î½Î¿Ï‚  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 37485___papyri | Poems: 2\n",
      "  Poem 775069 (1 verses):\n",
      "    Ï†Î¹Î»Î¿ÎºÏÎ±Ï„Î¿Ï…Ï‚ Â²vacÂ²  [source: phi]\n",
      "  Poem 37485 (8 verses):\n",
      "    ca Ï„Î¿ Î³ÏÎ±Î¼Î¼Î±Ï„Î¹Î¿Î½ ÎºÏ…ÏÎ¹Î¿Î½  [source: papyri]\n",
      "    ÎºÎ±Î¹ Î²ÎµÎ²Î±Î¹Î¿Î½ ÎºÎ±Î¹ ÎµÏ€ÎµÏÏ‰Ï„Î·Î¸ÎµÎ¹Ï‚ Ï‰Î¼Î¿Î»Î¿Î³Î·ÏƒÎ± hand 2 Î±Ï…ÏÎ·Î»Î¹Î¿Ï‚ Î¹Ï‰Î±Î½Î½Î·Ï‚ ÎµÏ…  [source: papyri]\n",
      "    Î»Î¿Î³Î¹Î¿Ï… Î¿ Ï€ÏÎ¿ÎºÎµÎ¹Î¼ÎµÎ½Î¿Ï‚ ÎµÏƒÏ‡Î¿Î½ Ï„Î±Ï‚ Ï„Î¿Ï… Î±Ï…Ï„Î¿Ï…  [source: papyri]\n",
      "    ca Î¿ÎºÏ„Ï‰ ÎºÎ±Î¹ Î±Ï€Î¿Î´Ï‰ÏƒÏ‰  [source: papyri]\n",
      "    ca Ï‰Ï‚ Ï€ÏÎ¿ÎºÎ¹Ï„Î±Î¹ Î±Ï…ÏÎ·Î»Î¹Î¿Ï‚ Ï†Î¹Î»Î¿  [source: papyri]\n",
      "    ca Î±Ï€Î¿ ca Î±Î¾Î¹Ï‰Î¸ÎµÎ¹Ï‚ ÎµÎ³ÏÎ±ÏˆÎ± Ï…Ï€ÎµÏ Î±Ï…Ï„Î¿Ï…  [source: papyri]\n",
      "    Ï€Î±ÏÎ¿Î½Ï„Î¿Ï‚ Î±Î³ÏÎ±Î¼Î¼Î±Ï„Î¿Ï…  [source: papyri]\n",
      "    hand 3 stauros Î´Î¹Ê¼ ÎµÎ¼Î¿Ï… ca ÏƒÏ…Î¼Î²Î¿Î»Î±Î¹Î¿Î³ÏÎ±Ï†Î¿Ï… ÎµÎ³ÏÎ±Ï†Î· stauros monogram  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 75175___papyri | Poems: 2\n",
      "  Poem 947942 (4 verses):\n",
      "    ÏƒÎµ c  [source: phi]\n",
      "    ÎµÏ…ÎµÏÎ³ÎµÏƒÎ¹Î±Î¹Ï‚ Ï‡ÏÎ·ÏƒÎ¹  [source: phi]\n",
      "    Î¼Î¿Î½ Ï„Î· Ï€Î¿Î»ÎµÎ¹ Î³ÎµÎ³Î¿Î½Î¿Ï„Î± ÎºÎ±Ï„Î±  [source: phi]\n",
      "    Ï„Î± Î³ÏÎ±Ï†ÎµÎ½Ï„Î± Î±Ï…Ï„Ï‰ ÏˆÎ·Ï†Î¹ÏƒÎ¼Î±Ï„Î±  [source: phi]\n",
      "  Poem 75175 (4 verses):\n",
      "    ÎµÏ€ÎµÎ¹Ï† Î¹Ï› Ï„Î¿Ï… Î² ÎµÏ„Î¿Ï…Ï‚ Î¿Î½Î¿Î¼Î±Ï„Î¿Ï‚ ca  [source: papyri]\n",
      "    Ï€ ÎºÎ±Î¹ ÏƒÎµ ca  [source: papyri]\n",
      "    Ï…Ï€ÎµÏ Î¼ÎµÏÎ¹ÏƒÎ¼Î¿Ï… Î»Î·Î¼Î¼Î±Ï„Ï‰Î½ ca  [source: papyri]\n",
      "    Î±Î»Î»Î±Ï‚ Î¹Î· Î¿Î¼Î¿Î¹Ï‰Ï‚ ca  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 75852___papyri | Poems: 2\n",
      "  Poem 792965 (1 verses):\n",
      "    Î½ÎµÎ¿Ï„Î·Ï„Î¿Ï‚ ÎµÎ½ Î±ÎºÎ¼Î·Î¹  [source: phi]\n",
      "  Poem 75852 (2 verses):\n",
      "    Î¿Î½Î½Ï‰Ï†ÏÎ¹Ï‚ Î½ÎµÏ‰Ï„ÎµÏÎ¿Ï‚ ÏˆÎµÎ½Î± ÏƒÏ…Î½Î± Ï„Ï†Î¿Ï…Ï„Î¹ Ï€ÎµÏ„ÎµÏ‡Ï‰  [source: papyri]\n",
      "    ÎºÎ±Î¹ Î¼ÎµÏ„Î¿Ï‡Î¿Î¹Ï‚ ÎµÏƒÏ‡Î¿Î½ Ï…Ï€ÎµÏ ÎµÎ»Î±Î¹ÎºÏ‰Î½ Î½Î¿Ï„Î¿Ï… ÎºÎ±Î¹ Î»Î¹Î²Î¿Ï‚ Î´ÏÎ±Ï‡Î¼Î±Ï‚ Î´ hand 2 Î¿Î½Î½Ï‰Ï†ÏÎ¹Ï‚ ÏƒÎµÏƒÎ·Î¼ÎµÎ¹Ï‰Î¼Î±Î¹  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 76928___papyri | Poems: 2\n",
      "  Poem 927411 (1 verses):\n",
      "    ÎºÏ…Î¼Î·ÏƒÎ¹Ï‚ Â²â¶ÎºÎ¿Î¹Î¼Î·ÏƒÎ¹Ï‚Â²â¶ Î±Î½Î±ÏƒÏ„Î±ÏƒÎ¹Î¿Ï…  [source: phi]\n",
      "  Poem 76928 (3 verses):\n",
      "    ÏˆÎµÎ½Î±Î¼Î¿Ï…Î½Î¹Ï‚ Ï€ÎµÎºÏ…ÏƒÎ¹Î¿Ï‚ Ï†ÎµÎ½Î½Î·ÏƒÎ¹ Ï€Î¹Î²Î¿Ï…Ï‡Î¹  [source: papyri]\n",
      "    Ï€ÎµÏ„ÎµÎ·ÏƒÎ¹Î¿Ï‚ Ï‡Î±Î¹ÏÎµÎ¹Î½ Î±Ï€ÎµÏ‡Ï‰ Ï€Î±ÏÎ± ÏƒÎ¿Ï… Î´ÏÎ±Ï‡Î¼Î±Ï‚Î´Î´Î¹Ï‰Î²Î¿Î»Î¿Î½Ï…Ï€ÎµÏ  [source: papyri]\n",
      "    Ï„Ï‰Î½ Î´Î·Î¼Ï‰ÏƒÎ¹Ï‰Î½Î¹Î³ÎµÏ„Î¿Ï…Ï‚ Î½ÎµÏÏ‰Î½Î¿Ï‚ Ï„Î¿Ï… ÎºÏ…  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 78520___papyri | Poems: 2\n",
      "  Poem 932115 (10 verses):\n",
      "    lucio aelio caesari  [source: phi]\n",
      "    imperatoris traiani hadri  [source: phi]\n",
      "    ani augusti pontifi  [source: phi]\n",
      "    cis maximi tribunicia  [source: phi]\n",
      "    potestate xxi imperatoris ii consulis  [source: phi]\n",
      "    iii patris patriae filio divi traia  [source: phi]\n",
      "    ti divi nervae pro  [source: phi]\n",
      "    nepoti tribunicia potestate  [source: phi]\n",
      "    consuli ii colonia aelia ha  [source: phi]\n",
      "    driana augusta  [source: phi]\n",
      "  Poem 78520 (8 verses):\n",
      "    ca exemplum tabulae togipurae ca  [source: papyri]\n",
      "    servio middot scipione salvidieno middot orfito lucio middot peducaeo priscino  [source: papyri]\n",
      "    consulibus anno xiii imperatoris middot caesaris  [source: papyri]\n",
      "    nervae traiani augusti middot germanici dacici mense  [source: papyri]\n",
      "    alexandreae middot ad aegupt middot descriptum  [source: papyri]\n",
      "    et recognitum ex exemplo in foro augusti in quo scri  [source: papyri]\n",
      "    ptum fuit marci middot filius middot pollia iulianus ca  [source: papyri]\n",
      "    togam puram sumpsisse  [source: papyri]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('full_semantic_results_cross_dataset_poems/poems_clustered_by_verse_jaccard.csv')\n",
    "import pandas as pd\n",
    "\n",
    "def get_filtered_clusters_cross_source_large(df, max_poem_verses=100, min_poems=2, max_poems=200):\n",
    "    \"\"\"\n",
    "    Efficient filtering for large-scale data:\n",
    "      - poems with <= max_poem_verses\n",
    "      - clusters with size between min_poems and max_poems\n",
    "      - only clusters where each poem comes from a different source_dataset\n",
    "    Returns filtered dataframe and dict: cluster_id -> list of poem_ids\n",
    "    \"\"\"\n",
    "    # Step 1: Count verses per poem\n",
    "    poem_verse_counts = df.groupby('idoriginal_poem')['verse'].size()\n",
    "    valid_poems = poem_verse_counts[poem_verse_counts <= max_poem_verses].index\n",
    "\n",
    "    # Step 2: Filter dataframe to valid poems\n",
    "    df_valid = df[df['idoriginal_poem'].isin(valid_poems)]\n",
    "\n",
    "    # Step 3: Compute cluster-level info efficiently\n",
    "    cluster_poem_df = df_valid.groupby('poem_cluster_id').agg(\n",
    "        poems=('idoriginal_poem', 'nunique'),\n",
    "        unique_sources=('source_dataset', 'nunique')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Step 4: Filter clusters by size and unique sources\n",
    "    valid_clusters = cluster_poem_df[\n",
    "        (cluster_poem_df['poems'] >= min_poems) &\n",
    "        (cluster_poem_df['poems'] <= max_poems) &\n",
    "        (cluster_poem_df['poems'] == cluster_poem_df['unique_sources'])\n",
    "    ]['poem_cluster_id'].values\n",
    "\n",
    "    # Step 5: Filter dataframe for these clusters\n",
    "    df_final = df_valid[df_valid['poem_cluster_id'].isin(valid_clusters)]\n",
    "\n",
    "    # Step 6: Build cluster -> poems mapping\n",
    "    clusters_dict = df_final.groupby('poem_cluster_id')['idoriginal_poem'].unique().to_dict()\n",
    "\n",
    "    return df_final, clusters_dict\n",
    "\n",
    "\n",
    "# ===================== Usage =====================\n",
    "\n",
    "filtered_df, clusters_dict = get_filtered_clusters_cross_source_large(df)\n",
    "\n",
    "# Print first 5 clusters with their source_dataset\n",
    "for i, (cluster_id, poem_ids) in enumerate(clusters_dict.items()):\n",
    "    if i >= 50:\n",
    "        break\n",
    "    print(f\"\\nCluster {cluster_id} | Poems: {len(poem_ids)}\")\n",
    "    poem_subset = filtered_df[filtered_df['poem_cluster_id'] == cluster_id][['idoriginal_poem','verse','source_dataset']]\n",
    "    for pid in poem_ids:\n",
    "        verses = poem_subset[poem_subset['idoriginal_poem'] == pid]\n",
    "        print(f\"  Poem {pid} ({len(verses)} verses):\")\n",
    "        for _, row in verses.iterrows():\n",
    "            print(f\"    {row['verse']}  [source: {row['source_dataset']}]\")\n",
    "    print(\"-\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1524822-de95-4960-8b30-edde96d5109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 116206___papyri | Poems: 2\n",
      "  Poem 127710 (2 verses):\n",
      "    Î±Î½Î´ÏÎ¹ Î¼Î¿Ï…ÏƒÎ¹ÎºÏ‰ Ï‡ÏÎ·ÏƒÏ„Ï‰  [source: phi]\n",
      "    Ï†Î¹Î»Î¹Ï€Ï€Ï‰ ÎµÏÎ¼Î¹Î¿Î½Î· Î¼Î½  [source: phi]\n",
      "  Poem 116886 (2 verses):\n",
      "    Î±Ï€Î¿ Î¼Î·Î½Î± Ï€ÏÎµÏƒÎ²Ï…Ï„ÎµÏÎ¿Ï…  [source: papyri]\n",
      "    Î¿Î½Î¹ÎºÎ·Ï†Î¿ÏÎ±Î±Î¼Î·Î½Î¹ Î¸Ï‰Î¸  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 117092___papyri | Poems: 2\n",
      "  Poem 763310 (4 verses):\n",
      "    Î¼Î·Î½Î¹Ï‚ Ï…Ï€ÎµÏ Ï„Î¿Ï… Ï…Î¹Î¿Ï…  [source: phi]\n",
      "    Î±ÏÎ¹ÏƒÏ„Î¿Î²Î¿Ï…Î»Î¿Ï… âµâ¶ ÎµÏ…Ï‡Î·Î½  [source: phi]\n",
      "    Î¼Î·Î½Î¹Ï‚ Ï…Ï€ÎµÏ Ï„Î¿Ï… Ï…Î¹Î¿Ï…  [source: phi]\n",
      "    Î±ÏÎ¹ÏƒÏ„Î¿Î²Î¿Ï…Î»Î¿Ï… âµâ¶ ÎµÏ…Ï‡Î·Î½  [source: phi]\n",
      "  Poem 117092 (2 verses):\n",
      "    Î¿Î½Î¹ÎºÎ±Î¹Ï†Î¿ÏÎµÎ±Î·Î¼Î¹ÏƒÏ…  [source: papyri]\n",
      "    Î¼Î·Î½Î¹Î¼ÎµÏƒÎ¿ÏÎ· Î¹Î½Î´Î¹ÎºÏ„Î¹Î¿Î½Î¿Ï‚  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 128136___phi | Poems: 2\n",
      "  Poem band3_tÃ¼rkei_TR6 (2 verses):\n",
      "    Ï„ÏÎ¿Î¼Ï‰ Ï€ÏÎ¿Î²Î»ÎµÏ€Îµ Ï„Î·Î½ Î¸ÎµÎ¹Î±Î½ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¹Î±Î½  [source: rhoby]\n",
      "    Î´Ï…ÏƒÎ¹Î½ Ï„Î±Î¿Î¹ÏƒÎ¹Î½ Ï‰ÏÎ±Î¹ÏƒÎ¼ÎµÎ½Î¿Î¹Ï‚  [source: rhoby]\n",
      "  Poem 128136 (2 verses):\n",
      "    Î´Î¿Î³Î¼Î±Ï„Î¹ Î²Î¿Ï…Î»Î·Ï‚ Î´Î·Î¼Î¿Ï…  [source: phi]\n",
      "    Ï‰Î½ Î´Ï…Î¿ Î³Î±Î¹Î¿Î½ Î¹Î¿Ï…Î»Î¹Î¿Î½  [source: phi]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 14065___papyri | Poems: 2\n",
      "  Poem 875470 (4 verses):\n",
      "    ÎµÏ„Î¿Ï…Ï‚ Î¶Ï€Ï…Ê¹ Ï€Î±Î½Î·Î¼Î¿Ï… Î¸Ê¹  [source: phi]\n",
      "    Î´Î¹Î´Ï…Î¼Î¿Î¹ Ï†Î±Î¹Î½Ï‰Î½  [source: phi]\n",
      "    ÎºÎ±ÏÎºÎ¹Î½Î¿Ï‚ Ï†Î±ÎµÎ¸Ï‰Î½  [source: phi]\n",
      "    Î»ÎµÏ‰Î½ Ï†Ï‰ÏƒÏ†Î¿ÏÎ¿Ï‚ Ï€Ï…ÏÎ¿ÎµÎ¹Ï‚ ÏƒÏ„Î¹Î»Î²Ï‰Î½  [source: phi]\n",
      "  Poem 14065 (3 verses):\n",
      "    Î±Ï‡Ï…ÏÎ¿Ï… ÏƒÎ±ÏÎ³Î±Î½Î· Î¼Î¹Î±  [source: papyri]\n",
      "    Î³Î¹Î½ÎµÏ„Î±Î¹ ÏƒÎ±ÏÎ³Î±Î½Î· Î±  [source: papyri]\n",
      "    Î¹ ÎµÏ„Î¿Ï…Ï‚ Î¸ ÎµÏ„Î¿Ï…Ï‚ Î² ÎµÏ„Î¿Ï…Ï‚ Ï‡Î¿Î¹Î±Îº Î´  [source: papyri]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Cluster 17733___dbbe | Poems: 2\n",
      "  Poem 17733 (2 verses):\n",
      "    Î¿ Ï„Î¿Ï…Î´Îµ Î¼Î¿Î¹ Î´Î¿Ï…Ï‚ Ï€ÏÎ¿Ï‚ Ï„ÎµÎ»Î¿Ï‚ Ï„Î¿Ï…ÏÎ³Î¿Î½ Ï†Î¸Î±ÏƒÎ±Î¹  [source: dbbe]\n",
      "    Î´Î¿Î¹Î·Ï‚ Î¹Î´ÎµÎ¹Î½ Î¼Îµ Ï€Ï„Î±Î¹Î¼Î±Ï„Ï‰Î½ Ï„ÎµÏÎ¼Î± Î»Î¿Î³Îµ  [source: dbbe]\n",
      "  Poem 948722 (2 verses):\n",
      "    Î¼Ï…ÏƒÏ„Î±Î¹ Î±ÎºÎ¿Î»â¸â°Ï‰  [source: phi]\n",
      "    Î±ÏÏ‡Î¹Î±Ï„Ï‰Ï ÏƒÏ…ÏÎ¿Ï‚  [source: phi]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_filtered_clusters_cross_source_large(df, min_poem_verses=2, max_poem_verses=100, min_poems=2, max_poems=200):\n",
    "    \"\"\"\n",
    "    Efficient filtering for large-scale data:\n",
    "      - poems with >= min_poem_verses and <= max_poem_verses\n",
    "      - clusters with size between min_poems and max_poems\n",
    "      - only clusters where each poem comes from a different source_dataset\n",
    "    Returns filtered dataframe and dict: cluster_id -> list of poem_ids\n",
    "    \"\"\"\n",
    "    # Step 1: Count verses per poem\n",
    "    poem_verse_counts = df.groupby('idoriginal_poem')['verse'].size()\n",
    "    valid_poems = poem_verse_counts[\n",
    "        (poem_verse_counts >= min_poem_verses) & \n",
    "        (poem_verse_counts <= max_poem_verses)\n",
    "    ].index\n",
    "\n",
    "    # Step 2: Filter dataframe to valid poems\n",
    "    df_valid = df[df['idoriginal_poem'].isin(valid_poems)]\n",
    "\n",
    "    # Step 3: Compute cluster-level info efficiently\n",
    "    cluster_poem_df = df_valid.groupby('poem_cluster_id').agg(\n",
    "        poems=('idoriginal_poem', 'nunique'),\n",
    "        unique_sources=('source_dataset', 'nunique')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Step 4: Filter clusters by size and unique sources\n",
    "    valid_clusters = cluster_poem_df[\n",
    "        (cluster_poem_df['poems'] >= min_poems) &\n",
    "        (cluster_poem_df['poems'] <= max_poems) &\n",
    "        (cluster_poem_df['poems'] == cluster_poem_df['unique_sources'])\n",
    "    ]['poem_cluster_id'].values\n",
    "\n",
    "    # Step 5: Filter dataframe for these clusters\n",
    "    df_final = df_valid[df_valid['poem_cluster_id'].isin(valid_clusters)]\n",
    "\n",
    "    # Step 6: Build cluster -> poems mapping\n",
    "    clusters_dict = df_final.groupby('poem_cluster_id')['idoriginal_poem'].unique().to_dict()\n",
    "\n",
    "    return df_final, clusters_dict\n",
    "\n",
    "# ===================== Usage =====================\n",
    "\n",
    "filtered_df, clusters_dict = get_filtered_clusters_cross_source_large(df, min_poem_verses=2)\n",
    "\n",
    "# Print first 5 clusters with their source_dataset\n",
    "for i, (cluster_id, poem_ids) in enumerate(clusters_dict.items()):\n",
    "    if i >= 5:  # limit to first 5 clusters for demo\n",
    "        break\n",
    "    print(f\"\\nCluster {cluster_id} | Poems: {len(poem_ids)}\")\n",
    "    poem_subset = filtered_df[filtered_df['poem_cluster_id'] == cluster_id][['idoriginal_poem','verse','source_dataset']]\n",
    "    for pid in poem_ids:\n",
    "        verses = poem_subset[poem_subset['idoriginal_poem'] == pid]\n",
    "        print(f\"  Poem {pid} ({len(verses)} verses):\")\n",
    "        for _, row in verses.iterrows():\n",
    "            print(f\"    {row['verse']}  [source: {row['source_dataset']}]\")\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43229069-c5f6-4ba0-a6bc-33be79b54f8c",
   "metadata": {},
   "source": [
    "## 2.3 poem level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c259d88e-3b88-40b4-8d1d-c4b3ff23a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:51:09 | INFO | ================================================================================\n",
      "10:51:09 | INFO | POEM-LEVEL CLUSTERING VIA AGGREGATED EMBEDDINGS\n",
      "10:51:09 | INFO | ================================================================================\n",
      "10:51:09 | INFO | Loading verse data and embeddings...\n",
      "/tmp/ipykernel_3761598/354242294.py:651: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('concatenated.csv')\n",
      "10:51:38 | INFO | âœ“ Loaded 1,536,616 verse embeddings\n",
      "10:51:38 | INFO | \n",
      "================================================================================\n",
      "10:51:38 | INFO | AGGREGATING VERSE EMBEDDINGS TO POEMS\n",
      "10:51:38 | INFO | ================================================================================\n",
      "10:51:40 | INFO | Found 261,216 unique poems\n",
      "10:51:40 | INFO | Computing poem embeddings via mean pooling...\n",
      "Aggregating:   0%|          | 0/261216 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2330186 is out of bounds for axis 0 with size 1536616",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 728\u001b[0m\n\u001b[1;32m    725\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 728\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 662\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    659\u001b[0m verse_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(embeddings_file)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    660\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ“ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(verse_embeddings)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m verse embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 662\u001b[0m poem_embeddings, poem_ids, poem_datasets, poem_sizes \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_verse_embeddings_to_poems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverse_embeddings\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m verse_embeddings\n\u001b[1;32m    667\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[15], line 83\u001b[0m, in \u001b[0;36maggregate_verse_embeddings_to_poems\u001b[0;34m(df, verse_embeddings)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m poem_id \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28msorted\u001b[39m(poem_to_verse_indices\u001b[38;5;241m.\u001b[39mkeys()), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     82\u001b[0m     verse_indices \u001b[38;5;241m=\u001b[39m poem_to_verse_indices[poem_id]\n\u001b[0;32m---> 83\u001b[0m     verse_embs \u001b[38;5;241m=\u001b[39m \u001b[43mverse_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mverse_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     85\u001b[0m     poem_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(verse_embs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m     poem_emb \u001b[38;5;241m=\u001b[39m poem_emb \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(poem_emb) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2330186 is out of bounds for axis 0 with size 1536616"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import faiss\n",
    "import gc\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"/scratch/gent/vo/000/gvo00042/vsc48660/poem_embeddings_checkpoints\")\n",
    "RESULTS_DIR = Path(\"full_semantic_results_poem_level\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "N_BOOTSTRAP = 2\n",
    "STABILITY_PAIRS = 2000\n",
    "MAX_WORKERS = 16\n",
    "BATCH_SIZE = 64\n",
    "N_NEIGHBORS = 100\n",
    "FAISS_NPROBE = 16\n",
    "\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(\"POEM-LEVEL CLUSTERING VIA AGGREGATED EMBEDDINGS\")\n",
    "logger.info(\"=\"*80)\n",
    "\n",
    "def aggregate_verse_embeddings_to_poems(df, verse_embeddings):\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"AGGREGATING VERSE EMBEDDINGS TO POEMS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    df['poem_composite_id'] = df['idoriginal_poem'].astype(str) + '___' + df['source_dataset'].astype(str)\n",
    "    \n",
    "    poem_to_verse_indices = defaultdict(list)\n",
    "    for idx, poem_id in enumerate(df['poem_composite_id']):\n",
    "        poem_to_verse_indices[poem_id].append(idx)\n",
    "    \n",
    "    logger.info(f\"Found {len(poem_to_verse_indices):,} unique poems\")\n",
    "    \n",
    "    poem_embeddings = []\n",
    "    poem_ids = []\n",
    "    poem_datasets = []\n",
    "    poem_sizes = []\n",
    "    \n",
    "    checkpoint_file = CHECKPOINT_DIR / 'poem_embeddings.npz'\n",
    "    metadata_file = CHECKPOINT_DIR / 'poem_metadata.pkl.gz'\n",
    "    \n",
    "    if checkpoint_file.exists() and metadata_file.exists():\n",
    "        logger.info(\"Loading poem embeddings from checkpoint...\")\n",
    "        data = np.load(checkpoint_file)\n",
    "        poem_embeddings = data['embeddings']\n",
    "        \n",
    "        with gzip.open(metadata_file, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        poem_ids = metadata['poem_ids']\n",
    "        poem_datasets = metadata['poem_datasets']\n",
    "        poem_sizes = metadata['poem_sizes']\n",
    "        \n",
    "        logger.info(f\"âœ“ Loaded {len(poem_embeddings):,} poem embeddings\")\n",
    "        \n",
    "    else:\n",
    "        logger.info(\"Computing poem embeddings via mean pooling...\")\n",
    "        \n",
    "        for poem_id in tqdm(sorted(poem_to_verse_indices.keys()), desc=\"Aggregating\"):\n",
    "            verse_indices = poem_to_verse_indices[poem_id]\n",
    "            verse_embs = verse_embeddings[verse_indices]\n",
    "            \n",
    "            poem_emb = np.mean(verse_embs, axis=0)\n",
    "            poem_emb = poem_emb / (np.linalg.norm(poem_emb) + 1e-8)\n",
    "            \n",
    "            poem_embeddings.append(poem_emb)\n",
    "            poem_ids.append(poem_id)\n",
    "            \n",
    "            dataset = df.iloc[verse_indices[0]]['source_dataset']\n",
    "            poem_datasets.append(dataset)\n",
    "            poem_sizes.append(len(verse_indices))\n",
    "        \n",
    "        poem_embeddings = np.array(poem_embeddings, dtype=np.float32)\n",
    "        \n",
    "        logger.info(f\"âœ“ Created {len(poem_embeddings):,} poem embeddings\")\n",
    "        logger.info(f\"âœ“ Average verses per poem: {np.mean(poem_sizes):.1f}\")\n",
    "        \n",
    "        logger.info(\"Saving checkpoint...\")\n",
    "        np.savez_compressed(checkpoint_file, embeddings=poem_embeddings)\n",
    "        \n",
    "        metadata = {\n",
    "            'poem_ids': poem_ids,\n",
    "            'poem_datasets': poem_datasets,\n",
    "            'poem_sizes': poem_sizes\n",
    "        }\n",
    "        with gzip.open(metadata_file, 'wb') as f:\n",
    "            pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        logger.info(\"âœ“ Checkpoint saved\")\n",
    "    \n",
    "    dataset_counts = Counter(poem_datasets)\n",
    "    logger.info(f\"\\nPoems by dataset:\")\n",
    "    for dataset, count in sorted(dataset_counts.items()):\n",
    "        logger.info(f\"  {dataset}: {count:,}\")\n",
    "    \n",
    "    return poem_embeddings, poem_ids, poem_datasets, poem_sizes\n",
    "\n",
    "def stratified_sample_poems(poem_datasets, poem_sizes, n_sample=10000):\n",
    "    logger.info(f\"\\nStratified sampling of {n_sample:,} poems...\")\n",
    "    \n",
    "    df_meta = pd.DataFrame({\n",
    "        'index': range(len(poem_datasets)),\n",
    "        'dataset': poem_datasets,\n",
    "        'size': poem_sizes\n",
    "    })\n",
    "    \n",
    "    df_meta['size_bin'] = pd.cut(df_meta['size'], \n",
    "                                  bins=[0, 5, 10, 20, 50, np.inf],\n",
    "                                  labels=['tiny', 'small', 'medium', 'large', 'huge'])\n",
    "    \n",
    "    sample_indices = []\n",
    "    \n",
    "    for (dataset, size_bin), group in df_meta.groupby(['dataset', 'size_bin']):\n",
    "        n_in_group = len(group)\n",
    "        proportion = n_in_group / len(df_meta)\n",
    "        n_sample_group = max(1, int(n_sample * proportion))\n",
    "        n_sample_group = min(n_sample_group, n_in_group)\n",
    "        \n",
    "        sampled = group.sample(n=n_sample_group, random_state=42)\n",
    "        sample_indices.extend(sampled['index'].tolist())\n",
    "    \n",
    "    if len(sample_indices) < n_sample:\n",
    "        remaining = n_sample - len(sample_indices)\n",
    "        available = set(range(len(poem_datasets))) - set(sample_indices)\n",
    "        if available:\n",
    "            additional = np.random.choice(list(available), \n",
    "                                         size=min(remaining, len(available)), \n",
    "                                         replace=False)\n",
    "            sample_indices.extend(additional)\n",
    "    \n",
    "    sample_indices = sorted(sample_indices[:n_sample])\n",
    "    logger.info(f\"âœ“ Sampled {len(sample_indices):,} poems\")\n",
    "    \n",
    "    return np.array(sample_indices)\n",
    "\n",
    "def compute_stability_fast(partitions_list, n_nodes, sample_size=STABILITY_PAIRS):\n",
    "    if n_nodes < 100:\n",
    "        return 0.0\n",
    "    \n",
    "    n_partitions = len(partitions_list)\n",
    "    n_sample_pairs = min(sample_size, n_nodes * (n_nodes - 1) // 2)\n",
    "    \n",
    "    pairs_i = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    pairs_j = np.random.randint(0, n_nodes, n_sample_pairs)\n",
    "    valid = pairs_i != pairs_j\n",
    "    pairs_i = pairs_i[valid][:n_sample_pairs]\n",
    "    pairs_j = pairs_j[valid][:n_sample_pairs]\n",
    "    \n",
    "    coclustering = 0\n",
    "    for membership in partitions_list:\n",
    "        membership_arr = np.array(membership)\n",
    "        matches = membership_arr[pairs_i] == membership_arr[pairs_j]\n",
    "        coclustering += np.sum(matches)\n",
    "    \n",
    "    stability = coclustering / (len(pairs_i) * n_partitions)\n",
    "    return stability\n",
    "\n",
    "def evaluate_single_combination(args):\n",
    "    threshold_pct, threshold, resolution, edges_data, weights_data, dataset_map, n_nodes = args\n",
    "    \n",
    "    try:\n",
    "        edge_mask = weights_data >= threshold\n",
    "        edges = edges_data[edge_mask]\n",
    "        weights = weights_data[edge_mask]\n",
    "        \n",
    "        if len(edges) == 0:\n",
    "            return None\n",
    "        \n",
    "        g = ig.Graph(n=n_nodes, edges=edges.tolist(), directed=False)\n",
    "        g.es['weight'] = weights.tolist()\n",
    "        \n",
    "        bootstrap_memberships = []\n",
    "        bootstrap_qualities = []\n",
    "        \n",
    "        for seed in range(N_BOOTSTRAP):\n",
    "            partition = la.find_partition(\n",
    "                g,\n",
    "                la.CPMVertexPartition,\n",
    "                weights='weight',\n",
    "                resolution_parameter=resolution,\n",
    "                n_iterations=5,\n",
    "                seed=seed\n",
    "            )\n",
    "            bootstrap_memberships.append(partition.membership)\n",
    "            bootstrap_qualities.append(partition.quality())\n",
    "        \n",
    "        membership = bootstrap_memberships[0]\n",
    "        n_clusters = len(set(membership))\n",
    "        \n",
    "        if n_clusters == 0 or n_clusters == n_nodes:\n",
    "            return None\n",
    "        \n",
    "        stability = compute_stability_fast(bootstrap_memberships, n_nodes)\n",
    "        avg_size = n_nodes / n_clusters\n",
    "        \n",
    "        cluster_datasets = defaultdict(set)\n",
    "        cluster_sizes = defaultdict(int)\n",
    "        \n",
    "        for idx, cid in enumerate(membership):\n",
    "            cluster_datasets[cid].add(dataset_map[idx])\n",
    "            cluster_sizes[cid] += 1\n",
    "        \n",
    "        n_cross_clusters = sum(1 for datasets in cluster_datasets.values() \n",
    "                              if len(datasets) > 1)\n",
    "        pct_cross_clusters = (n_cross_clusters / n_clusters * 100) if n_clusters > 0 else 0\n",
    "        n_singleton = sum(1 for size in cluster_sizes.values() if size == 1)\n",
    "        \n",
    "        n_multi_dataset = sum(1 for datasets in cluster_datasets.values() if len(datasets) >= 3)\n",
    "        \n",
    "        avg_quality = np.mean(bootstrap_qualities)\n",
    "        modularity = g.modularity(membership, weights='weight')\n",
    "        \n",
    "        return {\n",
    "            'threshold_percentile': threshold_pct,\n",
    "            'threshold_value': threshold,\n",
    "            'resolution': resolution,\n",
    "            'n_edges': g.ecount(),\n",
    "            'graph_density': 2*g.ecount()/(n_nodes*(n_nodes-1)) if n_nodes > 1 else 0,\n",
    "            'stability': stability,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_singleton': n_singleton,\n",
    "            'avg_cluster_size': avg_size,\n",
    "            'n_cross_clusters': n_cross_clusters,\n",
    "            'pct_cross_clusters': pct_cross_clusters,\n",
    "            'n_multi_dataset_clusters': n_multi_dataset,\n",
    "            'avg_quality': avg_quality,\n",
    "            'modularity': modularity\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error at P{threshold_pct}, res={resolution:.2e}: {e}\")\n",
    "        return None\n",
    "\n",
    "def grid_search_parameters(sample_embeddings, sample_datasets):\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"PARAMETER GRID SEARCH\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    logger.info(\"Building FAISS index for sample...\")\n",
    "    dimension = sample_embeddings.shape[1]\n",
    "    index_sample = faiss.IndexFlatIP(dimension)\n",
    "    faiss.normalize_L2(sample_embeddings)\n",
    "    index_sample.add(sample_embeddings)\n",
    "    \n",
    "    k = min(200, len(sample_embeddings) - 1)\n",
    "    similarities, indices = index_sample.search(sample_embeddings, k)\n",
    "    \n",
    "    logger.info(\"Precomputing cross-dataset similarities...\")\n",
    "    cross_similarities = []\n",
    "    for i in range(len(sample_embeddings)):\n",
    "        neighbor_datasets = sample_datasets[indices[i, 1:]]\n",
    "        cross_mask = neighbor_datasets != sample_datasets[i]\n",
    "        cross_similarities.extend(similarities[i, 1:][cross_mask])\n",
    "    \n",
    "    cross_similarities = np.array(cross_similarities)\n",
    "    logger.info(f\"âœ“ Collected {len(cross_similarities):,} cross-dataset pairs\")\n",
    "    \n",
    "    all_percentiles = list(range(50, 100))\n",
    "    threshold_lookup = {p: np.percentile(cross_similarities, p) for p in all_percentiles}\n",
    "    \n",
    "    logger.info(f\"Threshold range: {threshold_lookup[50]:.4f} (P50) to {threshold_lookup[99]:.4f} (P99)\")\n",
    "    \n",
    "    logger.info(\"Precomputing edge structures...\")\n",
    "    all_edges_list = []\n",
    "    all_weights_list = []\n",
    "    \n",
    "    for i in range(len(sample_embeddings)):\n",
    "        dataset_i = sample_datasets[i]\n",
    "        neighbors = indices[i, 1:]\n",
    "        sims = similarities[i, 1:]\n",
    "        \n",
    "        valid_mask = neighbors > i\n",
    "        valid_neighbors = neighbors[valid_mask]\n",
    "        valid_sims = sims[valid_mask]\n",
    "        \n",
    "        if len(valid_neighbors) > 0:\n",
    "            neighbor_datasets = sample_datasets[valid_neighbors]\n",
    "            cross_mask = neighbor_datasets != dataset_i\n",
    "            \n",
    "            final_neighbors = valid_neighbors[cross_mask]\n",
    "            final_sims = valid_sims[cross_mask]\n",
    "            \n",
    "            for j, sim in zip(final_neighbors, final_sims):\n",
    "                all_edges_list.append([i, j])\n",
    "                all_weights_list.append(sim)\n",
    "    \n",
    "    all_edges = np.array(all_edges_list, dtype=np.int32)\n",
    "    all_weights = np.array(all_weights_list, dtype=np.float32)\n",
    "    \n",
    "    logger.info(f\"âœ“ Precomputed {len(all_edges):,} edges\")\n",
    "    \n",
    "    threshold_percentiles_coarse = [90, 93, 96, 98]\n",
    "    resolutions_coarse = np.logspace(-6, -1, 8)\n",
    "    \n",
    "    logger.info(f\"\\nCoarse grid: {len(threshold_percentiles_coarse)} thresholds Ã— {len(resolutions_coarse)} resolutions\")\n",
    "    \n",
    "    coarse_args = []\n",
    "    for threshold_pct in threshold_percentiles_coarse:\n",
    "        threshold = threshold_lookup.get(threshold_pct, np.percentile(cross_similarities, threshold_pct))\n",
    "        for resolution in resolutions_coarse:\n",
    "            coarse_args.append((\n",
    "                threshold_pct, threshold, resolution,\n",
    "                all_edges, all_weights, sample_datasets, len(sample_embeddings)\n",
    "            ))\n",
    "    \n",
    "    logger.info(f\"Running coarse sweep with {MAX_WORKERS} workers...\")\n",
    "    coarse_results = []\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(evaluate_single_combination, args): args for args in coarse_args}\n",
    "        with tqdm(total=len(futures), desc=\"Coarse sweep\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    coarse_results.append(result)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    logger.info(f\"âœ“ Coarse sweep: {len(coarse_results)} valid results\")\n",
    "    \n",
    "    if len(coarse_results) == 0:\n",
    "        logger.error(\"No valid results!\")\n",
    "        raise ValueError(\"Grid search failed\")\n",
    "    \n",
    "    coarse_df = pd.DataFrame(coarse_results)\n",
    "    best_coarse = coarse_df.loc[coarse_df['stability'].idxmax()]\n",
    "    \n",
    "    logger.info(f\"\\nBest coarse: P{best_coarse['threshold_percentile']}, res={best_coarse['resolution']:.2e}\")\n",
    "    logger.info(f\"  Stability: {best_coarse['stability']:.3f}\")\n",
    "    logger.info(f\"  Cross-dataset clusters: {best_coarse['n_cross_clusters']}\")\n",
    "    \n",
    "    thresh_fine = [max(50, int(best_coarse['threshold_percentile']) - 5),\n",
    "                   max(50, int(best_coarse['threshold_percentile']) - 2),\n",
    "                   int(best_coarse['threshold_percentile']),\n",
    "                   min(99, int(best_coarse['threshold_percentile']) + 2),\n",
    "                   min(99, int(best_coarse['threshold_percentile']) + 5)]\n",
    "    thresh_fine = sorted(list(set(thresh_fine)))\n",
    "    \n",
    "    log_res = np.log10(best_coarse['resolution'])\n",
    "    res_fine = np.logspace(log_res - 0.5, log_res + 0.5, 7)\n",
    "    \n",
    "    logger.info(f\"\\nFine grid: {len(thresh_fine)} thresholds Ã— {len(res_fine)} resolutions\")\n",
    "    \n",
    "    fine_args = []\n",
    "    for threshold_pct in thresh_fine:\n",
    "        threshold = threshold_lookup.get(threshold_pct, np.percentile(cross_similarities, threshold_pct))\n",
    "        for resolution in res_fine:\n",
    "            already_tested = any(\n",
    "                np.isclose(r['threshold_percentile'], threshold_pct) and \n",
    "                np.isclose(r['resolution'], resolution, rtol=0.1)\n",
    "                for r in coarse_results\n",
    "            )\n",
    "            if not already_tested:\n",
    "                fine_args.append((\n",
    "                    threshold_pct, threshold, resolution,\n",
    "                    all_edges, all_weights, sample_datasets, len(sample_embeddings)\n",
    "                ))\n",
    "    \n",
    "    logger.info(f\"Running fine sweep...\")\n",
    "    fine_results = []\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(evaluate_single_combination, args): args for args in fine_args}\n",
    "        with tqdm(total=len(futures), desc=\"Fine sweep\") as pbar:\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    fine_results.append(result)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    logger.info(f\"âœ“ Fine sweep: {len(fine_results)} valid results\")\n",
    "    \n",
    "    all_results = coarse_results + fine_results\n",
    "    sweep_df = pd.DataFrame(all_results)\n",
    "    sweep_df = sweep_df.sort_values('stability', ascending=False)\n",
    "    sweep_df.to_csv(RESULTS_DIR / 'poem_parameter_sweep.csv', index=False)\n",
    "    \n",
    "    best_params = sweep_df.iloc[0]\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"TOP 5 CONFIGURATIONS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    for idx, (_, row) in enumerate(sweep_df.head(5).iterrows(), 1):\n",
    "        logger.info(f\"\\n#{idx}. Threshold: P{row['threshold_percentile']} ({row['threshold_value']:.4f}), \"\n",
    "                   f\"Resolution: {row['resolution']:.2e}\")\n",
    "        logger.info(f\"     Stability: {row['stability']:.3f}\")\n",
    "        logger.info(f\"     Clusters: {row['n_clusters']}, Cross-dataset: {row['n_cross_clusters']} ({row['pct_cross_clusters']:.1f}%)\")\n",
    "        logger.info(f\"     Multi-dataset (3+): {row['n_multi_dataset_clusters']}\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"SELECTED PARAMETERS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Threshold: P{best_params['threshold_percentile']} = {best_params['threshold_value']:.4f}\")\n",
    "    logger.info(f\"Resolution: {best_params['resolution']:.6e}\")\n",
    "    logger.info(f\"Stability: {best_params['stability']:.3f}\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    create_visualization(sweep_df, best_params)\n",
    "    \n",
    "    return best_params['threshold_value'], best_params['resolution']\n",
    "\n",
    "def create_visualization(sweep_df, best_params):\n",
    "    logger.info(\"\\nCreating visualization...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Poem-Level Clustering: Parameter Sweep', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    pivot_data = sweep_df.pivot_table(\n",
    "        values='stability',\n",
    "        index='resolution',\n",
    "        columns='threshold_percentile',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    pivot_data.index = [f\"{res:.3e}\" for res in pivot_data.index]\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax)\n",
    "    ax.set_ylabel('Resolution', fontweight='bold')\n",
    "    ax.set_xlabel('Threshold Percentile', fontweight='bold')\n",
    "    ax.set_title('Stability Heatmap', fontweight='bold')\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.hist(sweep_df['stability'], bins=20, color='#0173B2', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(best_params['stability'], color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Stability', fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax.set_title('Stability Distribution', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    for thresh in sorted(sweep_df['threshold_percentile'].unique()):\n",
    "        data = sweep_df[sweep_df['threshold_percentile'] == thresh]\n",
    "        ax.plot(data['resolution'], data['stability'], 'o-', label=f'P{thresh}', markersize=4)\n",
    "    ax.set_xlabel('Resolution', fontweight='bold')\n",
    "    ax.set_ylabel('Stability', fontweight='bold')\n",
    "    ax.set_title('Stability vs Resolution', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(sweep_df['n_clusters'], sweep_df['n_cross_clusters'],\n",
    "              c=sweep_df['stability'], cmap='viridis', s=50, edgecolors='black')\n",
    "    ax.scatter(best_params['n_clusters'], best_params['n_cross_clusters'],\n",
    "              color='red', s=200, marker='*', edgecolors='black', linewidth=2)\n",
    "    ax.set_xlabel('Total Clusters', fontweight='bold')\n",
    "    ax.set_ylabel('Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_title('Cross-Dataset vs Total', fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(sweep_df.groupby('threshold_percentile')['n_multi_dataset_clusters'].max(), 'o-', linewidth=2)\n",
    "    ax.set_xlabel('Threshold Percentile', fontweight='bold')\n",
    "    ax.set_ylabel('Multi-Dataset Clusters (3+)', fontweight='bold')\n",
    "    ax.set_title('Multi-Dataset Clusters', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 2]\n",
    "    ax.scatter(sweep_df['pct_cross_clusters'], sweep_df['stability'],\n",
    "              c=sweep_df['threshold_value'], cmap='coolwarm', s=50, edgecolors='black')\n",
    "    ax.scatter(best_params['pct_cross_clusters'], best_params['stability'],\n",
    "              color='red', s=200, marker='*', edgecolors='black', linewidth=2)\n",
    "    ax.set_xlabel('% Cross-Dataset Clusters', fontweight='bold')\n",
    "    ax.set_ylabel('Stability', fontweight='bold')\n",
    "    ax.set_title('Stability vs Cross-Dataset %', fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'poem_parameter_sweep.png', dpi=300, bbox_inches='tight')\n",
    "    logger.info(\"âœ“ Visualization saved\")\n",
    "    plt.close()\n",
    "\n",
    "def cluster_all_poems(poem_embeddings, poem_datasets, threshold, resolution):\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"CLUSTERING ALL POEMS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Threshold: {threshold:.4f}, Resolution: {resolution:.6e}\")\n",
    "    \n",
    "    logger.info(\"Normalizing embeddings...\")\n",
    "    faiss.normalize_L2(poem_embeddings)\n",
    "    \n",
    "    logger.info(\"Building FAISS index...\")\n",
    "    dimension = poem_embeddings.shape[1]\n",
    "    nlist = min(int(4 * np.sqrt(len(poem_embeddings))), 8192)\n",
    "    nprobe = min(FAISS_NPROBE, nlist // 4)\n",
    "    \n",
    "    quantizer = faiss.IndexFlatIP(dimension)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "    \n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        logger.info(\"âœ“ Using GPU\")\n",
    "    except:\n",
    "        logger.info(\"âœ“ Using CPU\")\n",
    "    \n",
    "    if len(poem_embeddings) > 1000000:\n",
    "        train_size = min(500000, len(poem_embeddings))\n",
    "        train_idx = np.random.choice(len(poem_embeddings), train_size, replace=False)\n",
    "        index.train(poem_embeddings[train_idx])\n",
    "    else:\n",
    "        index.train(poem_embeddings)\n",
    "    \n",
    "    index.add(poem_embeddings)\n",
    "    index.nprobe = nprobe\n",
    "    \n",
    "    logger.info(f\"Searching for {N_NEIGHBORS} neighbors...\")\n",
    "    SEARCH_BATCH = 50000\n",
    "    all_sims = []\n",
    "    all_inds = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(poem_embeddings), SEARCH_BATCH), desc=\"Neighbor search\"):\n",
    "        batch_end = min(i + SEARCH_BATCH, len(poem_embeddings))\n",
    "        D, I = index.search(poem_embeddings[i:batch_end], N_NEIGHBORS)\n",
    "        all_sims.append(D)\n",
    "        all_inds.append(I)\n",
    "    \n",
    "    all_sims = np.vstack(all_sims)\n",
    "    all_inds = np.vstack(all_inds)\n",
    "    \n",
    "    logger.info(\"âœ“ Neighbor search complete\")\n",
    "    \n",
    "    edge_checkpoint = CHECKPOINT_DIR / 'poem_edges.npz'\n",
    "    \n",
    "    if edge_checkpoint.exists():\n",
    "        logger.info(\"Loading edges from checkpoint...\")\n",
    "        edge_data = np.load(edge_checkpoint)\n",
    "        all_edges = [(int(i), int(j)) for i, j in edge_data['edges']]\n",
    "        all_weights = edge_data['weights'].tolist()\n",
    "        logger.info(f\"âœ“ Loaded {len(all_edges):,} edges\")\n",
    "    else:\n",
    "        logger.info(\"Building edge list (cross-dataset only)...\")\n",
    "        all_edges = []\n",
    "        all_weights = []\n",
    "        \n",
    "        EDGE_BATCH = 20000\n",
    "        for batch_start in tqdm(range(0, len(poem_embeddings), EDGE_BATCH), desc=\"Building edges\"):\n",
    "            batch_end = min(batch_start + EDGE_BATCH, len(poem_embeddings))\n",
    "            \n",
    "            for local_idx in range(batch_end - batch_start):\n",
    "                node_idx = batch_start + local_idx\n",
    "                dataset_i = poem_datasets[node_idx]\n",
    "                \n",
    "                neighbors = all_inds[node_idx, 1:]\n",
    "                sims = all_sims[node_idx, 1:]\n",
    "                \n",
    "                valid_mask = (neighbors > node_idx) & (sims >= threshold)\n",
    "                valid_neighbors = neighbors[valid_mask]\n",
    "                valid_sims = sims[valid_mask]\n",
    "                \n",
    "                if len(valid_neighbors) > 0:\n",
    "                    neighbor_datasets = np.array([poem_datasets[n] for n in valid_neighbors])\n",
    "                    cross_mask = neighbor_datasets != dataset_i\n",
    "                    \n",
    "                    final_neighbors = valid_neighbors[cross_mask]\n",
    "                    final_sims = valid_sims[cross_mask]\n",
    "                    \n",
    "                    for neighbor, sim in zip(final_neighbors, final_sims):\n",
    "                        all_edges.append((node_idx, int(neighbor)))\n",
    "                        all_weights.append(float(sim))\n",
    "            \n",
    "            if batch_start > 0 and batch_start % (EDGE_BATCH * 10) == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        logger.info(f\"âœ“ Built {len(all_edges):,} cross-dataset edges\")\n",
    "        \n",
    "        logger.info(\"Saving edge checkpoint...\")\n",
    "        np.savez_compressed(edge_checkpoint,\n",
    "                          edges=np.array(all_edges, dtype=np.int32),\n",
    "                          weights=np.array(all_weights, dtype=np.float32))\n",
    "    \n",
    "    logger.info(\"Building graph...\")\n",
    "    g = ig.Graph(n=len(poem_embeddings), edges=all_edges, directed=False)\n",
    "    g.es['weight'] = all_weights\n",
    "    \n",
    "    logger.info(f\"Graph: {g.vcount():,} nodes, {g.ecount():,} edges\")\n",
    "    \n",
    "    logger.info(\"Running Leiden clustering...\")\n",
    "    partition = la.find_partition(\n",
    "        g,\n",
    "        la.CPMVertexPartition,\n",
    "        weights='weight',\n",
    "        resolution_parameter=resolution,\n",
    "        n_iterations=10,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    labels = np.array(partition.membership)\n",
    "    \n",
    "    cluster_datasets = defaultdict(set)\n",
    "    cluster_poems = defaultdict(list)\n",
    "    \n",
    "    for idx, cid in enumerate(labels):\n",
    "        cluster_datasets[cid].add(poem_datasets[idx])\n",
    "        cluster_poems[cid].append(idx)\n",
    "    \n",
    "    n_total = len(set(labels))\n",
    "    n_cross = sum(1 for ds in cluster_datasets.values() if len(ds) > 1)\n",
    "    n_multi = sum(1 for ds in cluster_datasets.values() if len(ds) >= 3)\n",
    "    \n",
    "    logger.info(f\"\\nFINAL RESULTS:\")\n",
    "    logger.info(f\"  Total clusters: {n_total:,}\")\n",
    "    logger.info(f\"  Cross-dataset clusters: {n_cross:,} ({n_cross/n_total*100:.1f}%)\")\n",
    "    logger.info(f\"  Multi-dataset (3+): {n_multi:,} ({n_multi/n_total*100:.1f}%)\")\n",
    "    \n",
    "    cluster_info = []\n",
    "    for cid, datasets in cluster_datasets.items():\n",
    "        if len(datasets) > 1:\n",
    "            poems = cluster_poems[cid]\n",
    "            counts = Counter(poem_datasets[i] for i in poems)\n",
    "            cluster_info.append({\n",
    "                'cluster_id': cid,\n",
    "                'size': len(poems),\n",
    "                'n_datasets': len(datasets),\n",
    "                'datasets': ', '.join(sorted(datasets)),\n",
    "                'dataset_counts': dict(counts)\n",
    "            })\n",
    "    \n",
    "    cluster_df = pd.DataFrame(cluster_info).sort_values('size', ascending=False)\n",
    "    \n",
    "    logger.info(f\"\\nTop 20 cross-dataset clusters:\")\n",
    "    if len(cluster_df) > 0:\n",
    "        print(cluster_df.head(20).to_string(index=False))\n",
    "    \n",
    "    return labels, cluster_datasets, cluster_df\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(\"Loading verse data and embeddings...\")\n",
    "    df = pd.read_csv('concatenated.csv')\n",
    "    \n",
    "    embeddings_file = CHECKPOINT_DIR.parent / 'full_semantic_clustering_checkpoints' / 'embeddings.npz'\n",
    "    if not embeddings_file.exists():\n",
    "        logger.error(f\"Verse embeddings not found at {embeddings_file}\")\n",
    "        logger.error(\"Please run verse-level embedding script first!\")\n",
    "        return\n",
    "    \n",
    "    verse_embeddings = np.load(embeddings_file)['embeddings']\n",
    "    logger.info(f\"âœ“ Loaded {len(verse_embeddings):,} verse embeddings\")\n",
    "    \n",
    "    poem_embeddings, poem_ids, poem_datasets, poem_sizes = aggregate_verse_embeddings_to_poems(\n",
    "        df, verse_embeddings\n",
    "    )\n",
    "    \n",
    "    del verse_embeddings\n",
    "    gc.collect()\n",
    "    \n",
    "    sample_indices = stratified_sample_poems(poem_datasets, poem_sizes, SAMPLE_SIZE)\n",
    "    sample_embeddings = poem_embeddings[sample_indices].copy()\n",
    "    sample_datasets = np.array([poem_datasets[i] for i in sample_indices])\n",
    "    \n",
    "    threshold, resolution = grid_search_parameters(sample_embeddings, sample_datasets)\n",
    "    \n",
    "    del sample_embeddings, sample_datasets\n",
    "    gc.collect()\n",
    "    \n",
    "    poem_labels, cluster_datasets, cluster_df = cluster_all_poems(\n",
    "        poem_embeddings, poem_datasets, threshold, resolution\n",
    "    )\n",
    "    \n",
    "    poem_id_to_label = {poem_id: label for poem_id, label in zip(poem_ids, poem_labels)}\n",
    "    \n",
    "    df['poem_composite_id'] = df['idoriginal_poem'].astype(str) + '___' + df['source_dataset'].astype(str)\n",
    "    df['poem_cluster_id'] = df['poem_composite_id'].map(poem_id_to_label)\n",
    "    \n",
    "    cross_cluster_ids = set(cid for cid, ds in cluster_datasets.items() if len(ds) > 1)\n",
    "    df['is_cross_dataset_poem_cluster'] = df['poem_cluster_id'].isin(cross_cluster_ids)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"SAVING RESULTS\")\n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    df.to_csv(RESULTS_DIR / 'poems_clustered_full.csv', index=False)\n",
    "    logger.info(f\"âœ“ Full results: {RESULTS_DIR / 'poems_clustered_full.csv'}\")\n",
    "    \n",
    "    df_cross = df[df['is_cross_dataset_poem_cluster']].copy()\n",
    "    df_cross.to_csv(RESULTS_DIR / 'poems_clustered_cross_dataset_only.csv', index=False)\n",
    "    logger.info(f\"âœ“ Cross-dataset only: {len(df_cross):,} verses\")\n",
    "    \n",
    "    cluster_df.to_csv(RESULTS_DIR / 'poem_clusters_summary.csv', index=False)\n",
    "    logger.info(f\"âœ“ Cluster summary saved\")\n",
    "    \n",
    "    summary = {\n",
    "        'n_verses': len(df),\n",
    "        'n_poems': len(poem_ids),\n",
    "        'n_datasets': len(set(poem_datasets)),\n",
    "        'threshold': threshold,\n",
    "        'resolution': resolution,\n",
    "        'n_total_clusters': len(set(poem_labels)),\n",
    "        'n_cross_clusters': len(cross_cluster_ids),\n",
    "        'n_multi_clusters': sum(1 for ds in cluster_datasets.values() if len(ds) >= 3),\n",
    "        'processing_time_minutes': (time.time() - start_time) / 60\n",
    "    }\n",
    "    \n",
    "    pd.DataFrame([summary]).to_csv(RESULTS_DIR / 'clustering_summary.csv', index=False)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"âœ… POEM-LEVEL CLUSTERING COMPLETE\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Time: {summary['processing_time_minutes']:.1f} minutes\")\n",
    "    logger.info(f\"Cross-dataset clusters: {summary['n_cross_clusters']:,}\")\n",
    "    logger.info(f\"Multi-dataset (3+) clusters: {summary['n_multi_clusters']:,}\")\n",
    "    logger.info(f\"Results saved to: {RESULTS_DIR}/\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f1a41-1aa0-40ed-9972-c051593db17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
